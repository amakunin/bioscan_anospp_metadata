{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "import itertools\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import ete3\n",
    "import geopy\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_VERSION = '0.1.0'\n",
    "ANOSPP_VERSION = '4.0'\n",
    "BIOSCAN_VERSION = '2.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)\n",
    "# logging.getLogger().setFormat('[%(levelname)s] %(message)s')\n",
    "\n",
    "def setup_logging(verbose=False):\n",
    "    try: \n",
    "        del logging.root.handlers[:]\n",
    "    except:\n",
    "        pass\n",
    "    if verbose:\n",
    "        logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n",
    "    else:\n",
    "        logging.basicConfig(level=logging.WARNING, format='[%(levelname)s] %(message)s')\n",
    "setup_logging(verbose=True)   \n",
    "logging.info('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anospp_fn = '../data/Anopheles_Metadata_Manifest_V4.0_20221220.xlsx'\n",
    "biosc_fn = '../data/BIOSCAN_Manifest_V2.0_20221017.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and install taxonomy\n",
    "ncbi = ete3.NCBITaxa()\n",
    "# run update if needed\n",
    "# ncbi.update_taxonomy_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(fn, sheet='TAB 2 Metadata Entry'):\n",
    "\n",
    "    logging.info('reading data from {!r} sheet {!r}'.format(fn, sheet))\n",
    "    \n",
    "    df = pd.read_excel(fn, dtype=str, index_col=0, keep_default_na=False,\n",
    "                       sheet_name=sheet)\n",
    "        \n",
    "    return df\n",
    "\n",
    "# df = get_data(anospp_fn, sheet='TAB 3 TEST Metadata Entry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_series(df):\n",
    "    \n",
    "    # series should be 1,2, ..., nsamples\n",
    "    logging.info('validating SERIES')\n",
    "    \n",
    "    if df.index.duplicated().any():\n",
    "        logging.error('duplicate SERIES: {}'.format(df.index[df.index.duplicated()].to_list()))\n",
    "    \n",
    "    # exclude non-numeric SERIES - these don't work with ranges\n",
    "    series_numeric = df.index.astype(str).str.isnumeric()\n",
    "    if not series_numeric.all():\n",
    "        logging.error(f'Found and excluded non-numeric SERIES: {df.index[~series_numeric].to_list()}')\n",
    "        df = df.loc[series_numeric]\n",
    "        \n",
    "    # check the remaining SERIES are continuous\n",
    "    expected_series = set([str(i) for i in range(1, df.shape[0] + 1)])\n",
    "    observed_series = set(df.index.astype(str))\n",
    "    if expected_series != observed_series:\n",
    "        logging.error(f'In SERIES, {sorted(list(expected_series - observed_series))} are missing, '\n",
    "                      f'{sorted(list(observed_series - expected_series))} are unexpected')\n",
    "        \n",
    "    return df\n",
    "        \n",
    "# df = validate_series(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_ranges(df):\n",
    "    # based on https://stackoverflow.com/questions/4628333/converting-a-list-of-integers-into-range-in-python\n",
    "    i = df.index.to_list()\n",
    "    ranges = []\n",
    "    for a, b in itertools.groupby(enumerate(i), lambda pair: pair[1] - pair[0]):\n",
    "        b = list(b)\n",
    "        if b[0][1] != b[-1][1]:\n",
    "            ranges.append(f'{b[0][1]}-{b[-1][1]}')\n",
    "        else:\n",
    "            ranges.append(f'{b[0][1]}')\n",
    "            \n",
    "    return ', '.join(ranges)\n",
    "\n",
    "# index_ranges(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_trailing_spaces(df):\n",
    "    for col in df.columns:\n",
    "        trailing_spaces = (df[col].str.startswith(' ') | df[col].str.endswith(' '))\n",
    "        if trailing_spaces.any():\n",
    "            logging.warning('trailing spaces found in column {!r}, SERIES {}. Removing for validation'.format(col,\n",
    "                index_ranges(df.loc[trailing_spaces])))\n",
    "            df[col] = df[col].str.strip()\n",
    "            \n",
    "    return df\n",
    "\n",
    "# df = remove_trailing_spaces(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nonbreaking_spaces(df):\n",
    "    for col in df.columns:\n",
    "        nonbreaking_spaces = df[col].str.contains(u\"\\u00A0\")\n",
    "        if nonbreaking_spaces.any():\n",
    "            logging.warning('non-breaking spaces found in column {!r}, SERIES {}. Removing for validation'.format(col,\n",
    "                index_ranges(df.loc[nonbreaking_spaces])))\n",
    "            df[col] = df[col].str.replace(u\"\\u00A0\", \" \")\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_columns(df, template_df):\n",
    "    \n",
    "    logging.info('checking manifest columns against template')\n",
    "    \n",
    "    data_cols = set(df.columns)\n",
    "    template_cols = set(template_df.columns)\n",
    "        \n",
    "    if data_cols - template_cols != set():\n",
    "        logging.warning('extra columns in filled manifest compared to template: {}'.format(data_cols - template_cols))\n",
    "    if template_cols - data_cols != set():\n",
    "        logging.error('template columns missing from filled manifest: {}'.format(template_cols - data_cols))\n",
    "\n",
    "# check_columns(df, template_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_dict(fn, validation_sheet='Data Validation - do not edit'):\n",
    "    \n",
    "    # pick up validation values from data validation sheet\n",
    "    logging.info('extracting value validation data from {!r}'.format(fn))\n",
    "    valid_df = pd.read_excel(fn, dtype=str, sheet_name=validation_sheet)\n",
    "    valid_dict = dict()\n",
    "    for col in valid_df.columns:\n",
    "        valid_dict[col] = valid_df[col].dropna().to_list()\n",
    "    \n",
    "    return valid_dict\n",
    "\n",
    "# valid_dict = get_valid_dict(anospp_fn, validation_sheet='TAB 5 Data Validation - do not ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_missing(series, na_values=[]):\n",
    "    \n",
    "    # valid missing data \n",
    "    if len(na_values) > 0:\n",
    "        no_data = (series.isin(na_values))\n",
    "        logging.info('excluding {} {!r} samples without data in {!r}'.format(no_data.sum(), na_values, series.name))\n",
    "        return series[~no_data]\n",
    "    return series\n",
    "    \n",
    "# exclude_missing(df['TIME_OF_COLLECTION'], na_values=['NOT_COLLECTED',''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_contributors(fn, contrib_sheet='TAB 1 Contributors'):\n",
    "    \n",
    "    logging.info(f'validating contributors in {fn}')\n",
    "        \n",
    "    df = pd.read_excel(fn, dtype=str, keep_default_na=False,\n",
    "                       sheet_name=contrib_sheet)\n",
    "    \n",
    "    df = remove_trailing_spaces(df)\n",
    "    \n",
    "    expected_columns = ['SURNAME','FIRST_NAME','PRIMARY_AFFILIATION','EMAIL ADDRESS','CONTRIBUTION','CONFIRMATION']\n",
    "    \n",
    "    if set(df.columns) != set(expected_columns):\n",
    "        ec = set(df.columns) - set(expected_columns)\n",
    "        mc = set(expected_columns) - set(df.columns)\n",
    "        logging.error(f'mismatch in contributor columns: extra {ec}, missing {mc}')\n",
    "    \n",
    "    for delim_char in (';','|'):\n",
    "        for col in df.columns:\n",
    "            if df[col].str.contains(delim_char, regex=False).any():\n",
    "                logging.error(f'contributor column {col} contains delimiter \"{delim_char}\"')\n",
    "    \n",
    "    df['FULL_NAME'] = df['FIRST_NAME'] + ' ' + df['SURNAME']\n",
    "    df['PARTNER_CODE'] = (df['SURNAME'].str.slice(0,2) + df['FIRST_NAME'].str.slice(0,2)).str.upper()\n",
    "    is_template_name = (df['FULL_NAME'] == 'Darwin Charles R.')\n",
    "    if is_template_name.any():\n",
    "        logging.warning('suspect template contributor was not removed')\n",
    "    \n",
    "    is_dup_name = df['FULL_NAME'].duplicated()\n",
    "    if is_dup_name.any():\n",
    "        logging.error('duplicated names {}'.format(\n",
    "                df.loc[is_dup_name, 'FULL_NAME'].to_list()))\n",
    "    \n",
    "    # TODO update template with underscore in email column\n",
    "    is_valid_email = df['EMAIL ADDRESS'].str.match('^[A-z0-9._%+-]+@[A-z0-9.-]+\\.[A-z]{2,}$')\n",
    "    if not is_valid_email.all():\n",
    "        logging.error('invalid email addresses {}'.format(\n",
    "                df.loc[~is_valid_email, 'EMAIL ADDRESS'].to_list()))\n",
    "    \n",
    "    is_confirmed = (df['CONFIRMATION'] == 'YES')\n",
    "    if not is_confirmed.any():\n",
    "        logging.error('confirmation lacking for any contributors')\n",
    "        \n",
    "    return df\n",
    "        \n",
    "# contrib_df = validate_contributors(anospp_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_plates_wells(df, contrib_df, plate_col='RACK_OR_PLATE_ID', well_col='TUBE_OR_WELL_ID', bioscan=False):\n",
    "    \n",
    "    # expect only complete 96-well plates\n",
    "    logging.info(f'validating {plate_col} and {well_col}')\n",
    "    \n",
    "    empty_rows = (df[plate_col] == '') | (df[well_col] == '')\n",
    "    if empty_rows.any():\n",
    "        logging.warning(f'Found and excluded {empty_rows.sum()} empty rows based on {plate_col} and {well_col}')\n",
    "        df = df.loc[~empty_rows]\n",
    "        \n",
    "    # plate names validation\n",
    "    plates = df[plate_col].drop_duplicates()\n",
    "    plate_name_template = ('^[A-Z]{4}-[0-9]{3}$' if bioscan else '^[A-Z]{4}_[0-9]{3}$')\n",
    "    wrong_plate_names = plates[~plates.str.match(plate_name_template, na='')]\n",
    "    if len(wrong_plate_names) > 0:\n",
    "        logging.error(f'plate names {wrong_plate_names.to_list()} do not match template \"{plate_name_template}\"')\n",
    "    # check plate name prefixes prefixes\n",
    "    plate_prefixes = plates.str.slice(0,4)\n",
    "    if bioscan:\n",
    "        # todo read partners table to feed GAL\n",
    "        partners_df = pd.read_csv('../data/bioscan_partners.tsv', sep='\\t', dtype=str)\n",
    "        unknown_prefixes = (~plate_prefixes.isin(partners_df['partner_code']))\n",
    "    else:\n",
    "        unknown_prefixes = (~plate_prefixes.isin(contrib_df['PARTNER_CODE']))\n",
    "    if unknown_prefixes.any():\n",
    "        logging.error(f'plate ID prefixes not recognised for {plates[unknown_prefixes].to_list()}')\n",
    "        \n",
    "    # add 96-well plate well IDs to validation\n",
    "    row_id = list('ABCDEFGH')\n",
    "    col_id = range(1,13)\n",
    "    expected_wells = [r + str(c) for (r,c) in itertools.product(row_id, col_id)]\n",
    "    \n",
    "    pdfs = []\n",
    "    \n",
    "    for plate, pdf in df.groupby(plate_col):\n",
    "        # If plate level only metadata is being entered\n",
    "        # put “PLATE_ONLY” in well \n",
    "        # and use only one row to capture the metadata for the whole plate\n",
    "        if bioscan and (pdf[well_col] == 'PLATE_ONLY').any():\n",
    "            logging.warning(f'found PLATE_ONLY plate {plate}, expanding to well-level')\n",
    "            if pdf.shape[0] > 1:\n",
    "                logging.error(f'too many rows in PLATE_ONLY plate {plate}, expected one')\n",
    "            # expand to 96 rows\n",
    "            pdf = pd.DataFrame(pdf.iloc[0] for i in range(len(expected_wells)))\n",
    "            pdf[well_col] = expected_wells\n",
    "            # H12 blank\n",
    "            for col in pdf.columns:\n",
    "                if col == 'ORGANISM_PART':\n",
    "                    pdf[col].iloc[-1]='NOT_APPLICABLE'\n",
    "                elif col not in ['SERIES','CATCH_LOT','RACK_OR_PLATE_ID',\n",
    "                                 'TUBE_OR_WELL_ID','ORGANISM_PART','PRESERVATIVE_SOLUTION']:\n",
    "                    pdf[col].iloc[-1]=''\n",
    "            #print(pdf.iloc[-3:,:3])\n",
    "        # check for well duplicates\n",
    "        dup_wells =  pdf[well_col].duplicated()\n",
    "        if dup_wells.any():\n",
    "            logging.error(f'duplicate {well_col} for plate {plate}: {pdf.loc[dup_wells, well_col].unique()}')\n",
    "        # check for non A1...H12 wells\n",
    "        observed_wells_set = set(pdf[well_col])\n",
    "        expected_wells_set = set(expected_wells)\n",
    "        if observed_wells_set != expected_wells_set:\n",
    "            msg = f'in {well_col} for plate {plate}, '\n",
    "            if len(expected_wells_set - observed_wells_set) > 0:\n",
    "                msg += f'wells {expected_wells_set - observed_wells_set} are missing'\n",
    "            if len(observed_wells_set - expected_wells_set) > 0:\n",
    "                if msg.endswith('missing'):\n",
    "                    msg += ', '\n",
    "                msg += f'wells {observed_wells_set - expected_wells_set} are excessive'\n",
    "            logging.error(msg)\n",
    "        pdfs.append(pdf)\n",
    "    \n",
    "    if bioscan and (df[well_col] == 'PLATE_ONLY').any():\n",
    "        df = pd.concat(pdfs).reset_index(drop=True)\n",
    "        df.index.name = 'SERIES'\n",
    "    logging.warning(f'found {df.shape[0]} samples across {df[plate_col].nunique()} plates')\n",
    "    \n",
    "    \n",
    "    \n",
    "    return df\n",
    "        \n",
    "# df = validate_plates_wells(df, contrib_df, 'RACK_OR_PLATE_ID', 'TUBE_OR_WELL_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO - which columns require NA, do not remove blanks to be able to get taxids for all\n",
    "def check_blanks(df):\n",
    "    \n",
    "    logging.info('Checking and excluding blank samples')    \n",
    "    \n",
    "    # blank criterion\n",
    "    is_blank = (df['ORGANISM_PART'] == 'NOT_APPLICABLE')\n",
    "    \n",
    "    blank_df = df[is_blank]\n",
    "    \n",
    "    non_blank_df = df[~is_blank]\n",
    "    \n",
    "    # last well of plate expected to be blank\n",
    "    non_blank_last_well_df = non_blank_df[non_blank_df['TUBE_OR_WELL_ID'] == 'H12']\n",
    "    if non_blank_last_well_df.shape[0] > 0:\n",
    "        logging.error(f'last well H12 is not blank at SERIES {index_ranges(non_blank_last_well_df)}: '\n",
    "                      f'in ORGANISM_PART, expected \"NOT_APPLICABLE\", '\n",
    "                      f'found {non_blank_last_well_df.ORGANISM_PART.to_list()}, '\n",
    "                      f'these samples will be included in further analysis',\n",
    "                        \n",
    "        )\n",
    "    \n",
    "    # raise to warning for sanity check\n",
    "    logging.warning(f'{is_blank.sum()} blanks located across {df.RACK_OR_PLATE_ID.nunique()} plates, '\n",
    "                    f'{non_blank_df.shape[0]} samples of {df.shape[0]} left for downstream analysis')\n",
    "    \n",
    "    \n",
    "    return is_blank\n",
    "\n",
    "# print(df.shape)\n",
    "# is_blank = check_blanks(df)\n",
    "# print(df[~is_blank].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_values(col, df, valid_dict, sep=None, na_values=[], level='e'):\n",
    "    \n",
    "    logging.info('validating values in column {!r}'.format(col))\n",
    "    \n",
    "    if col not in df.columns:\n",
    "        logging.error('{!r} column not found in manifest'.format(col))\n",
    "        return\n",
    "    if col not in valid_dict.keys():\n",
    "        logging.error('{!r} column not found in validation sheet'.format(col))\n",
    "        return\n",
    "    assert level in ('i','w','e'), '{!r} invalid logging level for validate_values'.format(level)\n",
    "    \n",
    "    series = df[col]\n",
    "    series = exclude_missing(series, na_values)\n",
    "    \n",
    "    col_values = set(series.unique())\n",
    "    # use separator to split values\n",
    "    if sep:\n",
    "        sep_col_values = list()\n",
    "        for v in col_values:\n",
    "            sep_col_values.extend([x.strip() for x in v.split(sep)])\n",
    "        col_values = set(sep_col_values)\n",
    "    \n",
    "    valid_values = set(valid_dict[col])\n",
    "    \n",
    "    invalid_values = col_values - valid_values\n",
    "    \n",
    "    if len(invalid_values) > 0:\n",
    "        if sep:\n",
    "            invalid_value_series = index_ranges(series[series.str.contains('|'.join(invalid_values), regex=True)])\n",
    "        else:\n",
    "            invalid_value_series = index_ranges(series[series.isin(invalid_values)])\n",
    "        msg = f'invalid values in {col}, SERIES {invalid_value_series}: {invalid_values}'\n",
    "        if level == 'i':\n",
    "            logging.info(msg)\n",
    "        elif level == 'w':\n",
    "            logging.warning(msg)\n",
    "        elif level == 'e':\n",
    "            logging.error(msg)\n",
    "#     else:\n",
    "#         logging.info('all values valid in {!r}'.format(col))\n",
    "            \n",
    "# validate_values('ORGANISM_PART', df, valid_dict, sep=\" | \", na_values=[], level='e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_date(col, df, na_values=[]):\n",
    "    \n",
    "    logging.info('validating date column {!r}'.format(col))\n",
    "\n",
    "    if col not in df.columns:\n",
    "        logging.error('{!r} column not found in manifest'.format(col))\n",
    "        return\n",
    "    series = df[col]\n",
    "    series = exclude_missing(series, na_values)\n",
    "    \n",
    "    # invalid date formats\n",
    "    # empty string converted to NaT\n",
    "    date_series = pd.to_datetime(series, format='%Y-%m-%d', errors='coerce')\n",
    "    if date_series.isna().any():\n",
    "        logging.error(f'invalid dates in {col}, SERIES {index_ranges(date_series[date_series.isna()])}: '\n",
    "                      f'{series[date_series.isna()].unique()}')\n",
    "    valid_date_series = date_series[~date_series.isna()]\n",
    "    \n",
    "    # dates in future\n",
    "    future_dates = (valid_date_series > datetime.datetime.today())\n",
    "    if future_dates.any():\n",
    "        logging.error(f'future dates in {col}, SERIES {index_ranges(valid_date_series[future_dates])}: '\n",
    "                      f'{valid_date_series[future_dates].to_list()}')\n",
    "        \n",
    "    # dates too old\n",
    "    old_dates = (valid_date_series < datetime.datetime.strptime('1900-01-01', '%Y-%m-%d'))\n",
    "    if old_dates.any():\n",
    "        logging.error(f'pre-1900 dates in {col}, SERIES {index_ranges(valid_date_series[old_dates])}: '\n",
    "                      f'{valid_date_series[old_dates].to_list()}')\n",
    "    \n",
    "    return valid_date_series\n",
    "\n",
    "# validate_date('DATE_OF_COLLECTION', df, na_values=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_time(col, df, na_values=[]):\n",
    "    \n",
    "    logging.info('validating time column {!r}'.format(col))\n",
    "    \n",
    "    if col not in df.columns:\n",
    "        logging.error('{!r} column not found in manifest'.format(col))\n",
    "        return\n",
    "    series = df[col]\n",
    "    series = exclude_missing(series, na_values)\n",
    "        \n",
    "    # invalid time formats\n",
    "    # NB empty string converted to NaT\n",
    "    time_series = pd.to_datetime(series, format='%H:%M:%S', errors='coerce')\n",
    "    if time_series.isna().any():\n",
    "        logging.error(\n",
    "            f'invalid times in {col}, SERIES {index_ranges(time_series[time_series.isna()])}: '\n",
    "            f'{series[time_series.isna()].unique()}'\n",
    "        )\n",
    "    valid_time_series = time_series[~time_series.isna()]\n",
    "    \n",
    "    return valid_time_series\n",
    "\n",
    "# validate_time('TIME_OF_COLLECTION', df, na_values=[''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_time_period(col, df, na_values=[]):\n",
    "    \n",
    "    logging.info('validating time period column {!r}'.format(col))\n",
    "    \n",
    "    if col not in df.columns:\n",
    "        logging.error('{!r} column not found in manifest'.format(col))\n",
    "        return\n",
    "    series = df[col]\n",
    "    series = exclude_missing(series, na_values)\n",
    "\n",
    "    # conversion with modifications for proper parsing \n",
    "    # by pd.Timedelta (does not accept missing data, e.g. 'PT1H')\n",
    "    # note - will not work for weeks and months\n",
    "    def convert_iso_duration(s):\n",
    "        if s == np.nan:\n",
    "            return np.nan\n",
    "        if not s.startswith('P') or 'T' not in s:\n",
    "            return np.nan\n",
    "        # add days\n",
    "        if s.startswith('PT'):\n",
    "            s = s.replace('PT','P0DT')\n",
    "        # add trailing minutes and seconds\n",
    "        if s.endswith('H'):\n",
    "            s += '0M0S'\n",
    "        elif s.endswith('M'):\n",
    "            s += '0S'\n",
    "        try:\n",
    "            return pd.Timedelta(s)\n",
    "        except:\n",
    "            return np.nan\n",
    "    time_period_series = series.apply(convert_iso_duration)\n",
    "    if time_period_series.isna().any():\n",
    "        logging.error(\n",
    "            f'invalid times in {col}, SERIES {index_ranges(time_period_series[time_period_series.isna()])}: '\n",
    "            f'{series[time_period_series.isna()].unique()}'\n",
    "        )\n",
    "    valid_time_period_series = time_period_series[~time_period_series.isna()]\n",
    "    return valid_time_period_series\n",
    "\n",
    "# df.loc[1,'DURATION_OF_COLLECTION'] = 'PVT1H'\n",
    "# validate_time_period('DURATION_OF_COLLECTION', df, na_values=['']);\n",
    "# df['DURATION_OF_COLLECTION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO include tests\n",
    "def validate_country_and_coordinates(df, fn, na_values=[], bioscan=False):\n",
    "    \n",
    "    logging.info('validating country with coordinates')\n",
    "    \n",
    "#     if bioscan:\n",
    "    country_col, lat_col, lon_col = 'COUNTRY_OF_COLLECTION', 'DECIMAL_LATITUDE', 'DECIMAL_LONGITUDE'\n",
    "#     else:\n",
    "#         country_col, lat_col, lon_col = 'COLLECTION_COUNTRY', 'DECIMAL_LATITUDE', 'DECIMAL_LONGITUDE'\n",
    "\n",
    "    try:\n",
    "        loc_df_complete = df[[country_col, lat_col, lon_col]].copy()\n",
    "    except:\n",
    "        logging.error('One of {!r} {!r} {!r} columns not found in manifest'.format(country_col, lat_col, lon_col))\n",
    "        return\n",
    "    loc_df_isna = (loc_df_complete.isin(na_values)).all(axis=1)\n",
    "    if loc_df_isna.any():\n",
    "        logging.info('removing {} {!r} samples with missing data from coordinate analysis'.format(\n",
    "                loc_df_isna.sum(), na_values))\n",
    "    loc_df_complete = loc_df_complete[~loc_df_isna].copy()\n",
    "    \n",
    "    # coordinates in geopy format\n",
    "    loc_df_complete['coord'] = loc_df_complete.apply(lambda x: '{}, {}'.format(\n",
    "            x[lat_col], x[lon_col]), axis=1)\n",
    "    \n",
    "    # get location data for coordinates\n",
    "    # use local copy of web query results for re-runs\n",
    "    # this \n",
    "    loc_fn = fn+'_loc.pkl'\n",
    "    if os.path.isfile(loc_fn):\n",
    "        locations = pickle.load(open(loc_fn, \"rb\"))\n",
    "    else:\n",
    "        # web map server - openstreetmaps\n",
    "        logging.info('querying coordinates')\n",
    "        locator = Nominatim(user_agent='myGeocoder')\n",
    "        rgeocode = RateLimiter(locator.reverse, min_delay_seconds=1)\n",
    "\n",
    "        locations = dict()\n",
    "        for c in loc_df_complete.coord.unique():\n",
    "            # pre-fill with unknown country\n",
    "            locations[c] = {'address':{'country':'UNKNOWN'}}\n",
    "            # check coordniate correctness\n",
    "            try:\n",
    "                lat, lon = c.split(', ')\n",
    "                lat, lon = float(lat), float(lon)\n",
    "            except:\n",
    "                unparsed_df = df[(df[lat_col] == str(lat)) & df[lon_col] == str(lon)]\n",
    "                logging.error(\n",
    "                    f'problem parsing coordinates {c} at SERIES {index_ranges(unparsed_df)}'\n",
    "                )\n",
    "                continue\n",
    "            if abs(lat) > 90:\n",
    "                logging.error(\n",
    "                    f'invalid latitude {lat} at SERIES {index_ranges(df[df[lat_col] == str(lat)])}'\n",
    "                    f', should be in [-90,90]')\n",
    "                continue\n",
    "            if abs(lon) > 180:\n",
    "                logging.error(\n",
    "                    f'invalid longitude {lon} at SERIES {index_ranges(df[df[lon_col] == str(lon)])}'\n",
    "                    f', should be in [-180,180]')\n",
    "                continue\n",
    "            # web query\n",
    "            location = rgeocode(c, language='en-gb')\n",
    "            # rgeocode returns empty location outside of counries and in some other situations\n",
    "            if location is not None:\n",
    "                locations[c] = location.raw\n",
    "\n",
    "        # save locations to file\n",
    "        pickle.dump(locations, open(loc_fn, \"wb\"))\n",
    "        \n",
    "    # parse country from partner input\n",
    "    loc_df_complete['partner_country'] = loc_df_complete[country_col].str.strip().str.upper()\n",
    "    \n",
    "    # extract countries from location data\n",
    "    loc_countries = dict()\n",
    "    for coord in locations.keys():\n",
    "        \n",
    "        lat, lon = coord.split(', ')\n",
    "        coord_series = index_ranges(df.query(f'({lat_col} == \"{lat}\") & ({lon_col} == \"{lon}\")'))\n",
    "                    \n",
    "        coord_country = locations[coord]['address']['country'].upper()\n",
    "        loc_countries[coord] = coord_country\n",
    "        \n",
    "        partner_countries = loc_df_complete.loc[loc_df_complete.coord == coord, 'partner_country']\n",
    "        if partner_countries.nunique() > 1:\n",
    "            logging.error(\n",
    "                f'multiple partner countries for coordinates {coord}, SERIES {coord_series}: '\n",
    "                f'{partner_countries.unique()}, skipping coordinate validation'\n",
    "            )\n",
    "            continue\n",
    "        if partner_countries.shape[0] == 0:\n",
    "            logging.error(f'no partner location found for coordinates {coord}, SERIES {coord_series}')\n",
    "            continue\n",
    "        partner_country = partner_countries.iloc[0]\n",
    "        if coord_country == 'UNKNOWN':\n",
    "            logging.warning(f'could not locate country for coordinates {coord}, '\n",
    "                            f'partner country {partner_country}, SERIES {coord_series}')\n",
    "        elif partner_country != coord_country:\n",
    "            logging.error(f'country mismatch for coordinates {coord}, partner country {partner_country}, '\n",
    "                          'coordinate country {coord_country}, SERIES {coord_series}')\n",
    "    \n",
    "    # countries based on coordinates\n",
    "    loc_df_complete['coord_country'] = loc_df_complete['coord'].replace(loc_countries)\n",
    "    country_mismatch = (loc_df_complete.coord_country != loc_df_complete.partner_country)\n",
    "\n",
    "#     if country_mismatch.any():\n",
    "#         logging.error('coordinates do not match country for SERIES: {}'.format(\n",
    "#                 country_mismatch[country_mismatch].index.to_list()))\n",
    "    \n",
    "    # location data can be re-used, e.g. as an additional field\n",
    "    return loc_df_complete\n",
    "# df.loc[2,'DECIMAL_LATITUDE'] = '65'\n",
    "# loc_test = validate_country_and_coordinates(df, anospp_fn)\n",
    "# loc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO hierarchy tests in BIOSCAN\n",
    "def validate_taxonomy(df, ncbi, na_values = [], anospp=False, add_taxids=False):\n",
    "\n",
    "    logging.info('validating taxonomy against NCBI')\n",
    "    \n",
    "    if anospp:\n",
    "        df['PREDICTED_ORDER_OR_GROUP'] = 'Diptera'\n",
    "        df['PREDICTED_FAMILY'] = 'Culicidae'\n",
    "        df['PREDICTED_GENUS'] = 'Anopheles'\n",
    "        \n",
    "        harbach_spp = []\n",
    "        with open('../data/harbach_spp_201910.txt') as f:\n",
    "            for line in f:\n",
    "                harbach_spp.append('Anopheles ' + line.strip())\n",
    "        harbach_spp = set(harbach_spp)\n",
    "        \n",
    "    tax_levels = {\n",
    "        'PREDICTED_ORDER_OR_GROUP':'order',\n",
    "        'PREDICTED_FAMILY':'family',\n",
    "        'PREDICTED_GENUS':'genus',\n",
    "        'PREDICTED_SCIENTIFIC_NAME':'species'\n",
    "    }\n",
    "    \n",
    "    hierarchies = df[tax_levels.keys()].drop_duplicates().copy()\n",
    "    \n",
    "    hierarchies.columns = list(tax_levels.values())\n",
    "        \n",
    "    tax_info = dict()\n",
    "    \n",
    "    for tax_col, tax_level in tax_levels.items():\n",
    "        \n",
    "        logging.info(f'validating {tax_col} against NCBI')\n",
    "        \n",
    "        if tax_col not in df.columns:\n",
    "                logging.error(f'{tax_col} column not found in manifest')\n",
    "                continue\n",
    "            \n",
    "        tax_names = list(hierarchies[tax_level].unique())\n",
    "        \n",
    "        for na_value in na_values:\n",
    "            try:\n",
    "                tax_names.remove(na_value)\n",
    "            except:\n",
    "                pass \n",
    "            \n",
    "        for i, tax_name in enumerate(tax_names):\n",
    "            if len(tax_name) == 0:\n",
    "                continue\n",
    "            corr_tax_name = tax_name[0].upper() + tax_name[1:].lower()\n",
    "            if corr_tax_name != tax_name and tax_name != 'blank sample':\n",
    "                s = index_ranges(df.query(f'{tax_col} == {tax_name}'))\n",
    "                logging.error(f'{tax_level}, SERIES {s}'\n",
    "                              f': unexpected case for \"{tax_name}\", '\n",
    "                              f'changing to \"{corr_tax_name}\" for validation')\n",
    "            tax_names[i] = corr_tax_name\n",
    "        \n",
    "        tax_info[tax_level] = ncbi.get_name_translator(tax_names) \n",
    "        \n",
    "        unmatched_names = set(tax_names) - set(tax_info[tax_level].keys())\n",
    "        if len(unmatched_names) > 0:\n",
    "            if tax_level == 'species' and anospp:\n",
    "                for sp in unmatched_names:\n",
    "                    s = index_ranges(df.query(f'{tax_col} == \"{sp}\"'))\n",
    "                    if sp in harbach_spp:\n",
    "                        logging.warning(f'{tax_level}, SERIES {s}:'\n",
    "                                        f' \"{sp}\" found in Harbach list, but not in NCBI Taxonomy')\n",
    "                    else:\n",
    "                        logging.error(f'{tax_level}, SERIES {s}'\n",
    "                                      f': \"{sp}\" not found in both Harbach list and NCBI Taxonomy')\n",
    "            else:\n",
    "                logging.error(f'{tax_level}: {unmatched_names} not found in NCBI Taxonomy')\n",
    "        \n",
    "        expected_rank = tax_level\n",
    "        \n",
    "        for tname, tids in tax_info[tax_level].items():\n",
    "            \n",
    "            ranks = ncbi.get_rank(tids)\n",
    "            \n",
    "            upd_tid = tids[0]\n",
    "            \n",
    "            if len(tids) == 1:\n",
    "                if ranks[upd_tid] != expected_rank: \n",
    "                    # TODO warning->info for ORDER\n",
    "                    logging.warning(f'{tax_level}: found unexpected rank for {tname} (taxid {upd_tid}): {ranks[upd_tid]}')\n",
    "            if len(tids) > 1:            \n",
    "                for tid, r in ranks.items():\n",
    "                    if r == expected_rank and len(tids) > 1:\n",
    "                        logging.info(f'{tax_level}: using only first matching rank for {tname} (taxid {tid}): {r}')\n",
    "                        upd_tid = tid\n",
    "                        break\n",
    "                else:\n",
    "                    logging.warning(f'{tax_level}: could not find matching rank for {tname}, '\n",
    "                                    f'using (taxid {upd_tid}): {ranks[upd_tid]}')\n",
    "                    \n",
    "            tax_info[tax_level][tname] = upd_tid\n",
    "        \n",
    "        #logging.info(f'{tax_level} {tax_info[tax_level]}')\n",
    "                    \n",
    "    # check consistency of taxonomy\n",
    "    for _, r in hierarchies.iterrows():\n",
    "        \n",
    "        if r.order in na_values:\n",
    "            continue\n",
    "        try:\n",
    "            order_id = tax_info['order'][r.order]\n",
    "        except KeyError:\n",
    "            logging.info(f'cannot validate PREDICTED_ORDER_OR_GROUP for \"{r.order}\", skipping taxonomy consistency check')\n",
    "            continue\n",
    "            \n",
    "        if r.genus in na_values:\n",
    "            continue\n",
    "        try:\n",
    "            family_id = tax_info['family'][r.family]\n",
    "            \n",
    "            family_lineage = ncbi.get_lineage(family_id)\n",
    "            \n",
    "            s = index_ranges(df.query(f'PREDICTED_FAMILY == \"{r.family}\"'))\n",
    "            \n",
    "            if order_id not in family_lineage:\n",
    "                logging.error(f'SERIES {s}: '\n",
    "                              f'family {r.family} (taxid {family_id}) does not belong to {r.order} (taxid {order_id})')\n",
    "        except KeyError:\n",
    "            logging.info(f'cannot validate PREDICTED_FAMILY for \"{r.family}\", skipping taxonomy consistency check')\n",
    "            continue\n",
    "            \n",
    "        if r.genus in na_values:\n",
    "            continue\n",
    "        try:\n",
    "            genus_id = tax_info['genus'][r.genus]\n",
    "            \n",
    "            genus_lineage = ncbi.get_lineage(genus_id)\n",
    "            \n",
    "            s = index_ranges(df.query(f'PREDICTED_GENUS == \"{r.genus}\"'))\n",
    "            \n",
    "            if order_id not in genus_lineage:\n",
    "                logging.error(\n",
    "                    f'SERIES {s}: '\n",
    "                    f'genus {r.genus} (taxid {genus_id}) does not belong to {r.order} (taxid {order_id})')\n",
    "            if family_id not in genus_lineage:\n",
    "                logging.error(\n",
    "                    f'SERIES {s}: '\n",
    "                    f'genus {r.genus} (taxid {genus_id}) does not belong to {r.family} (taxid {family_id})')\n",
    "        except KeyError:\n",
    "            logging.info(f'cannot validate PREDICTED_GENUS for \"{r.genus}\", skipping taxonomy consistency check')\n",
    "            continue\n",
    "            \n",
    "        if r.species in na_values:\n",
    "            continue\n",
    "        try:\n",
    "            species_id = tax_info['species'][r.species]\n",
    "            \n",
    "            species_lineage = ncbi.get_lineage(species_id)\n",
    "            \n",
    "            s = index_ranges(df.query(f'PREDICTED_SCIENTIFIC_NAME == \"{r.species}\"'))\n",
    "            \n",
    "            if order_id not in species_lineage:\n",
    "                logging.error(\n",
    "                    f'SERIES {s}: '\n",
    "                    f'species {r.species} (taxid {species_id}) does not belong to {r.order} (taxid {order_id})')\n",
    "            if family_id not in species_lineage:\n",
    "                logging.error(\n",
    "                    f'SERIES {s}: '\n",
    "                    f'species {r.species} (taxid {species_id}) does not belong to {r.family} (taxid {family_id})')\n",
    "            if genus_id not in species_lineage:\n",
    "                logging.error(\n",
    "                    f'SERIES {s}: '\n",
    "                    f'species {r.species} (taxid {species_id}) does not belong to {r.family} (taxid {genus_id})')\n",
    "        except KeyError:\n",
    "            logging.info(f'cannot validate PREDICTED_SCIENTIFIC_NAME for \"{r.species}\", skipping taxonomy consistency check')\n",
    "            continue\n",
    "    \n",
    "    if add_taxids:\n",
    "        for tc in tax_levels.keys():\n",
    "            df[f'{tc}_TAXID'] = df[tc].replace(tax_info[tax_levels[tc]])\n",
    "            \n",
    "    return df\n",
    "        \n",
    "                \n",
    "# validate_taxonomy(df, ncbi, anospp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_specimen_id_risk(df):\n",
    "    \n",
    "    logging.info(f'validating SPECIMEN_IDENTITY_RISK')\n",
    "    \n",
    "    if 'SPECIMEN_IDENTITY_RISK' not in df.columns:\n",
    "        logging.error(f'SPECIMEN_IDENTITY_RISK column not found in manifest')\n",
    "        return\n",
    "    \n",
    "    # missing species name, but no idenitity risk\n",
    "    invalid_risk = ((df.PREDICTED_SCIENTIFIC_NAME == '') & (df.SPECIMEN_IDENTITY_RISK == 'N'))\n",
    "    \n",
    "    if invalid_risk.any():\n",
    "        logging.error(f'SPECIMEN_IDENTITY_RISK should be Y in SERIES {index_ranges(df.loc[invalid_risk])}')\n",
    "\n",
    "# validate_specimen_id_risk(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_float(col, df, na_values=[]):\n",
    "    \n",
    "    logging.info(f'validating numeric format in {col}')\n",
    "    \n",
    "    if col not in df.columns:\n",
    "        logging.error(f'{col} column not found in manifest')\n",
    "        return\n",
    "    series = df[col]\n",
    "    series = exclude_missing(series, na_values)\n",
    "    \n",
    "    for val in series.unique():\n",
    "        try:\n",
    "            float(val)\n",
    "        except:\n",
    "            s = index_ranges(df.query(f'{col} == \"{val}\"'))\n",
    "            logging.error(\n",
    "                f'SERIES {s}: '\n",
    "                f'found non-numeric value in {col}: \"{val}\"')\n",
    "            \n",
    "            \n",
    "# validate_float('ELEVATION', df, na_values=[''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_freetext(col, df, na_values=['']):\n",
    "    \n",
    "    logging.info(f'validating freetext chars in {col}')\n",
    "    \n",
    "    if col not in df.columns:\n",
    "        logging.error(f'{col} column not found in manifest')\n",
    "        return\n",
    "    series = df[col]\n",
    "    series = exclude_missing(series, na_values)\n",
    "    \n",
    "    regex = '^[A-z0-9.,\\-_ ]+$'\n",
    "    \n",
    "    is_valid_freetext = series.str.match(regex)\n",
    "    if not is_valid_freetext.all():\n",
    "        logging.info('found non-standard characters in column {}, SERIES {}. Regex: \"{}\"'.format(\n",
    "        col, index_ranges(series.loc[~is_valid_freetext]), regex))\n",
    "\n",
    "        \n",
    "# validate_freetext('IDENTIFIED_HOW', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bd = validate_date('DATE_OF_COLLECTION', df)\n",
    "# ad = validate_date('DATE_OF_PRESERVATION', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_dates(before, after):\n",
    "        \n",
    "    logging.info(f'checking that {before.name} are earlier than {after.name}')\n",
    "\n",
    "    ctdf = pd.concat([before, after], axis=1)\n",
    "    date_conflict = ctdf[before.name] > ctdf[after.name]\n",
    "    \n",
    "    if date_conflict.any():\n",
    "        logging.error(f'{before.name} values are later than {after.name} for SERIES'\n",
    "                      f' {index_ranges(ctdf[date_conflict])}')\n",
    "\n",
    "        \n",
    "# compare_dates(bd, ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sts_cols(df, contrib_df, gal, bioscan=True):\n",
    "    \n",
    "    is_blank = (df['ORGANISM_PART'] == 'NOT_APPLICABLE')\n",
    "    \n",
    "    df['SPECIMEN_ID'] = df['RACK_OR_PLATE_ID'] + '_' + df['TUBE_OR_WELL_ID']\n",
    "    if df['SPECIMEN_ID'].duplicated().any():\n",
    "        logging.error('duplicate SPECIMEN_ID: {}'.format(df.loc[df.SPECIMEN_ID.duplicated(), 'SPECIMEN_ID']))\n",
    "    df['SCIENTIFIC_NAME'] = 'unidentified'\n",
    "    df.loc[is_blank, 'SCIENTIFIC_NAME'] = 'blank sample'\n",
    "    df['TAXON_ID'] = '32644'\n",
    "    df.loc[is_blank, 'TAXON_ID'] = '2582415'\n",
    "    df['GAL'] = gal\n",
    "    df['SYMBIONT'] = 'TARGET'\n",
    "    df['REGULATORY_COMPLIANCE'] = 'Y'\n",
    "    df['HAZARD_GROUP'] = 'HG1'\n",
    "    # add contributors - delimiters checked in validate_contributors\n",
    "    contrib_series = contrib_df['FULL_NAME'] + ';' + \\\n",
    "        contrib_df['PRIMARY_AFFILIATION'] + ';' + \\\n",
    "        contrib_df['EMAIL ADDRESS'] + ';' + \\\n",
    "        contrib_df['CONTRIBUTION']\n",
    "    df['CONTRIBUTORS'] = '|'.join(list(contrib_series))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# add_sts_cols(df, contrib_df, gal='Sanger Institute');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_sts_manifest(df, input_fn, validation_version):\n",
    "    \n",
    "    output_fn = input_fn.rstrip('.xlsx') + '_' + validation_version + '_for_sts.xlsx'\n",
    "    \n",
    "    logging.info(f'writing STS manifest to {output_fn}')\n",
    "    \n",
    "    df.to_excel(output_fn, sheet_name='Metadata Entry')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
