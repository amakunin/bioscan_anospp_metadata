{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "import itertools\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import ete3\n",
    "import geopy\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] test\n"
     ]
    }
   ],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)\n",
    "# logging.getLogger().setFormat('[%(levelname)s] %(message)s')\n",
    "\n",
    "def setup_logging(verbose=False):\n",
    "    try: \n",
    "        del logging.root.handlers[:]\n",
    "    except:\n",
    "        pass\n",
    "    if verbose:\n",
    "        logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n",
    "    else:\n",
    "        logging.basicConfig(level=logging.WARNING, format='[%(levelname)s] %(message)s')\n",
    "setup_logging(verbose=True)   \n",
    "logging.info('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'data/NE BIOSCAN_Manifest_V1.0_Yarner_2021.xlsx'\n",
    "template_fn = '../data/BIOSCAN_Manifest_V1.0_20211207.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and install taxonomy\n",
    "ncbi = ete3.NCBITaxa()\n",
    "# only run update if needed\n",
    "# ncbi.update_taxonomy_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] reading data from 'data/NE BIOSCAN_Manifest_V1.0_Yarner_2021.xlsx'\n",
      "[WARNING] trailing spaces found in column 'WHAT_3_WORDS', rows [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184]. Removing for validation\n",
      "[WARNING] trailing spaces found in column 'ORDER', rows [88, 343, 344, 345, 650, 756, 757, 758, 759, 760, 833, 861, 905, 906, 907, 908, 936]. Removing for validation\n"
     ]
    }
   ],
   "source": [
    "def get_data(fn):\n",
    "\n",
    "    logging.info('reading data from {!r}'.format(fn))\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_excel(fn, dtype=str, index_col=0, keep_default_na=False,\n",
    "                           sheet_name='TAB1 Specimen Metadata Entry')\n",
    "    except:\n",
    "        df = pd.read_excel(fn, dtype=str, index_col=0, keep_default_na=False,\n",
    "                           sheet_name='Specimen Metadata Entry')\n",
    "    \n",
    "    if df.index.duplicated().any():\n",
    "        logging.error('duplicate SERIES: {}'.format(df.index[df.index.duplicated()].to_list()))\n",
    "        \n",
    "    # trailing spaces\n",
    "    for col in df.columns:\n",
    "        trailing_spaces = (df[col].str.startswith(' ') | df[col].str.endswith(' '))\n",
    "        if trailing_spaces.any():\n",
    "            logging.warning('trailing spaces found in column {!r}, rows {}. Removing for validation'.format(col,\n",
    "                df.loc[trailing_spaces].index.to_list()))\n",
    "            df[col] = df[col].str.strip()\n",
    "        \n",
    "    return df\n",
    "df = get_data(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] reading data from '../data/BIOSCAN_Manifest_V1.0_20211207.xlsx'\n"
     ]
    }
   ],
   "source": [
    "template_df = get_data(template_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] checking manifest columns against template\n"
     ]
    }
   ],
   "source": [
    "def check_columns(df, template_df):\n",
    "    \n",
    "    logging.info('checking manifest columns against template')\n",
    "    \n",
    "    data_cols = set(df.columns)\n",
    "    template_cols = set(template_df.columns)\n",
    "        \n",
    "    if data_cols - template_cols != set():\n",
    "        logging.warning('extra columns in filled manifest compared to template: {}'.format(data_cols - template_cols))\n",
    "    if template_cols - data_cols != set():\n",
    "        logging.error('template columns missing from filled manifest: {}'.format(template_cols - data_cols))\n",
    "check_columns(df, template_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Checking and excluding blank samples\n",
      "[ERROR] last well H12 is not blank at SERIES [96]: in SCIENTIFIC_NAME, expected \"blank sample\", found ['NOT_APPLICABLE']\n",
      "[INFO] found 10 blank samples based on SCIENTIFIC_NAME\n"
     ]
    }
   ],
   "source": [
    "## TODO - which columns require NA, do not remove blanks to be able to get taxids for all\n",
    "def check_blanks(df):\n",
    "    \n",
    "    logging.info('Checking and excluding blank samples')\n",
    "    \n",
    "    # last well of plate expected to be blank\n",
    "    last_well = df[df['TUBE_OR_WELL_ID'] == 'H12']\n",
    "    last_well_blanks = (last_well['SCIENTIFIC_NAME'] == 'blank sample')\n",
    "    if not last_well_blanks.all():\n",
    "        logging.error('last well H12 is not blank at SERIES {}: in SCIENTIFIC_NAME, '\n",
    "                      'expected \"blank sample\", found {}'.format(\n",
    "                        last_well[~last_well_blanks].index.to_list(),\n",
    "                        last_well[~last_well_blanks].SCIENTIFIC_NAME.to_list()\n",
    "        ))\n",
    "    \n",
    "    is_blank = (df['SCIENTIFIC_NAME'] == 'blank sample')\n",
    "    blank_df = df[is_blank]\n",
    "    \n",
    "    logging.info('found {} blank samples based on SCIENTIFIC_NAME'.format(blank_df.shape[0]))\n",
    "    \n",
    "    # check organism part\n",
    "    organism_part_pass = (blank_df['ORGANISM_PART'] == 'BLANK_SAMPLE')\n",
    "    if not organism_part_pass.all():\n",
    "        logging.error('for blanks, ORGANISM_PART expected to be BLANK_SAMPLE, found {}'.format(\n",
    "                set(blank_df.loc[~organism_part_pass, 'ORGANISM_PART'])))\n",
    "    \n",
    "    # check that NOT_APPLICABLE is filled in all applicable \"orange\" columns\n",
    "    blanks_na = blank_df[[\n",
    "        'CATCH_LOT','BOTTLE_DIRECTION','HAZARD_GROUP',\n",
    "        'REGULATORY_COMPLIANCE','DATE_OF_COLLECTION','COLLECTION_LOCATION',\n",
    "        'DECIMAL_LATITUDE','DECIMAL_LONGITUDE','WHAT_3_WORDS' # ORDER FAMILY GENUS\n",
    "    ]]\n",
    "    na_filled = (blanks_na == 'NOT_APPLICABLE').all(axis=0)\n",
    "    if not na_filled.all():\n",
    "        logging.warning('for blanks, NOT_APPLICABLE expected, but not found in columns {}'.format(\n",
    "                            na_filled[~na_filled].index.to_list()))\n",
    "    # exclude blanks from downstream analysis    \n",
    "    # logging.info('{} samples of {} left for downstream analysis'.format(df_flt.shape[0], df.shape[0]))\n",
    "    \n",
    "    return is_blank\n",
    "        \n",
    "blanks_mask = check_blanks(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] extracting value validation data from '../data/BIOSCAN_Manifest_V1.0_20211207.xlsx'\n"
     ]
    }
   ],
   "source": [
    "def get_valid_dict(fn):\n",
    "    \n",
    "    # pick up validation values from data validation sheet\n",
    "    logging.info('extracting value validation data from {!r}'.format(fn))\n",
    "    valid_df = pd.read_excel(fn, dtype=str, sheet_name='Data Validation - do not edit')\n",
    "    valid_dict = dict()\n",
    "    for col in valid_df.columns:\n",
    "        valid_dict[col] = valid_df[col].dropna().to_list()\n",
    "    \n",
    "    return valid_dict\n",
    "valid_dict = get_valid_dict(template_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] excluding 290 ['NOT_COLLECTED', ''] samples without data in 'TIME_OF_COLLECTION'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SERIES\n",
       "example-small               11:22:00\n",
       "example-large               11:22:00\n",
       "example-handcaught          13:00:00\n",
       "1                           11:00:00\n",
       "2                           11:00:00\n",
       "                           ...      \n",
       "1002                        10:30:00\n",
       "1003                        10:30:00\n",
       "1004                        10:30:00\n",
       "1005                        10:30:00\n",
       "1056                  NOT_APPLICABLE\n",
       "Name: TIME_OF_COLLECTION, Length: 1009, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def exclude_missing(series, na_values=None):\n",
    "    \n",
    "    # valid missing data \n",
    "    no_data = (series.isin(na_values))\n",
    "    if no_data.sum() > 0:\n",
    "        logging.info('excluding {} {!r} samples without data in {!r}'.format(no_data.sum(), na_values, series.name))\n",
    "    return series[~no_data]\n",
    "    \n",
    "exclude_missing(df['TIME_OF_COLLECTION'], na_values=['NOT_COLLECTED',''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1299, 38)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] validating SERIES\n",
      "[ERROR] In SERIES, {'1299', '1297', '1298'} are missing, {'example-large', 'example-small', 'example-handcaught'} are unexpected\n"
     ]
    }
   ],
   "source": [
    "def validate_series(df):\n",
    "    \n",
    "    # series should be 1,2, ..., nsamples\n",
    "    logging.info('validating SERIES')\n",
    "    \n",
    "    expected_series = set([str(i) for i in range(1, df.shape[0] + 1)])\n",
    "    \n",
    "    observed_series = set(df.index.astype(str))\n",
    "    \n",
    "    if expected_series != observed_series:\n",
    "        logging.error(f'In SERIES, {expected_series - observed_series} are missing, '\n",
    "                      f'{observed_series - expected_series} are unexpected')\n",
    "    \n",
    "    \n",
    "validate_series(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SERIES\n",
       "example-small         False\n",
       "example-large          True\n",
       "example-handcaught     True\n",
       "1                     False\n",
       "2                      True\n",
       "                      ...  \n",
       "1292                   True\n",
       "1293                   True\n",
       "1294                   True\n",
       "1295                   True\n",
       "1296                   True\n",
       "Name: RACK_OR_PLATE_ID, Length: 1299, dtype: bool"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.RACK_OR_PLATE_ID.duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] validating RACK_OR_PLATE_ID and TUBE_OR_WELL_ID\n",
      "[INFO] found 1299 samples across 13 plates\n",
      "[ERROR] duplicate TUBE_OR_WELL_ID for plate : ['']\n",
      "[ERROR] in TUBE_OR_WELL_ID for plate , wells {'E4', 'B5', 'G8', 'E3', 'H2', 'E6', 'B7', 'H11', 'E11', 'A5', 'F3', 'D6', 'G10', 'F11', 'B4', 'C3', 'G4', 'A12', 'B12', 'F4', 'D12', 'C11', 'G7', 'H6', 'H3', 'B8', 'H5', 'C6', 'F7', 'F5', 'H1', 'G9', 'E1', 'C1', 'G5', 'A1', 'E9', 'B6', 'A8', 'B1', 'F8', 'H8', 'H10', 'H7', 'F9', 'G3', 'D10', 'A4', 'F12', 'C7', 'A7', 'C8', 'G2', 'C12', 'C2', 'A10', 'D5', 'D1', 'E7', 'D3', 'H9', 'B10', 'A6', 'D7', 'F1', 'F6', 'C4', 'D11', 'G6', 'E10', 'D8', 'G11', 'G12', 'E12', 'B2', 'B9', 'A9', 'D2', 'H12', 'F2', 'C10', 'D4', 'A2', 'A11', 'E5', 'G1', 'H4', 'C9', 'F10', 'A3', 'E2', 'C5', 'E8', 'D9', 'B3', 'B11'} are missing, wells {''} are excessive\n",
      "[ERROR] in TUBE_OR_WELL_ID for plate SHAP-001, wells {'E4', 'B5', 'G8', 'E3', 'H2', 'E6', 'B7', 'H11', 'E11', 'A5', 'F3', 'D6', 'G10', 'F11', 'B4', 'C3', 'G4', 'A12', 'B12', 'F4', 'D12', 'C11', 'G7', 'H6', 'H3', 'B8', 'H5', 'C6', 'F7', 'F5', 'H1', 'G9', 'E1', 'G5', 'E9', 'B6', 'A8', 'F8', 'H8', 'H10', 'H7', 'F9', 'G3', 'D10', 'A4', 'F12', 'C7', 'A7', 'C8', 'G2', 'C12', 'C2', 'A10', 'D5', 'D1', 'E7', 'D3', 'H9', 'B10', 'A6', 'D7', 'F1', 'F6', 'C4', 'D11', 'G6', 'E10', 'D8', 'G11', 'G12', 'E12', 'B2', 'B9', 'A9', 'D2', 'H12', 'F2', 'C10', 'D4', 'A2', 'A11', 'E5', 'G1', 'H4', 'C9', 'F10', 'A3', 'E2', 'C5', 'E8', 'D9', 'B3', 'B11'} are missing, wells set() are excessive\n"
     ]
    }
   ],
   "source": [
    "def validate_plates_wells(df, plate_col, well_col):\n",
    "    \n",
    "    # expect only complete 96-well plates\n",
    "    logging.info(f'validating {plate_col} and {well_col}')\n",
    "    \n",
    "    logging.info(f'found {df.shape[0]} samples across {df[plate_col].nunique()} plates')\n",
    "    \n",
    "    # add 96-well plate well IDs to validation\n",
    "    row_id = list('ABCDEFGH')\n",
    "    col_id = range(1,13)\n",
    "    expected_wells = set([r + str(c) for (r,c) in itertools.product(row_id, col_id)])\n",
    "    \n",
    "    for plate, pdf in df.groupby(plate_col):\n",
    "        dup_wells =  pdf[well_col].duplicated()\n",
    "        if dup_wells.any():\n",
    "            logging.error(f'duplicate {well_col} for plate {plate}: {pdf.loc[dup_wells, well_col].unique()}')\n",
    "        observed_wells = set(pdf[well_col])\n",
    "        if observed_wells != expected_wells:\n",
    "            logging.error(f'in {well_col} for plate {plate}, wells {expected_wells - observed_wells} '\n",
    "                          f'are missing, wells {observed_wells - expected_wells} are excessive')\n",
    "        \n",
    "validate_plates_wells(df, 'RACK_OR_PLATE_ID', 'TUBE_OR_WELL_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] validating values in column 'ORGANISM_PART'\n",
      "[ERROR] invalid values in 'ORGANISM_PART': {''}\n"
     ]
    }
   ],
   "source": [
    "def validate_values(col, df, valid_dict, sep=None, na_values=None, level='e'):\n",
    "    \n",
    "    logging.info('validating values in column {!r}'.format(col))\n",
    "    \n",
    "    if col not in df.columns:\n",
    "        logging.error('{!r} column not found in manifest'.format(col))\n",
    "        return\n",
    "    if col not in valid_dict.keys():\n",
    "        logging.error('{!r} column not found in validation sheet'.format(col))\n",
    "        return\n",
    "    assert level in ('i','w','e'), '{!r} invalid logging level for validate_values'.format(level)\n",
    "    \n",
    "    series = df[col]\n",
    "    if na_values:\n",
    "        series = exclude_missing(series, na_values)\n",
    "    \n",
    "    col_values = set(series.unique())\n",
    "    # use separator to split values\n",
    "    if sep:\n",
    "        sep_col_values = list()\n",
    "        for v in col_values:\n",
    "            sep_col_values.extend([x.strip() for x in v.split(sep)])\n",
    "        col_values = set(sep_col_values)\n",
    "    valid_values = set(valid_dict[col])\n",
    "    invalid_values = col_values - valid_values\n",
    "    if len(invalid_values) > 0:\n",
    "        msg = 'invalid values in {!r}: {}'.format(col, invalid_values)\n",
    "        if level == 'i':\n",
    "            logging.info(msg)\n",
    "        elif level == 'w':\n",
    "            logging.warning(msg)\n",
    "        elif level == 'e':\n",
    "            logging.error(msg)\n",
    "#     else:\n",
    "#         logging.info('all values valid in {!r}'.format(col))\n",
    "            \n",
    "validate_values('ORGANISM_PART', df, valid_dict, sep=\" | \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] validating date column 'DATE_OF_COLLECTION'\n",
      "[INFO] excluding 13 ['NOT_COLLECTED', 'NOT_APPLICABLE'] samples without data in 'DATE_OF_COLLECTION'\n",
      "[ERROR] invalid dates in 'DATE_OF_COLLECTION': ['']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SERIES\n",
       "example-small        2021-01-12\n",
       "example-large        2021-01-12\n",
       "example-handcaught   2021-05-14\n",
       "2                    2021-06-26\n",
       "3                    2021-06-26\n",
       "                        ...    \n",
       "1001                 2021-12-05\n",
       "1002                 2021-12-05\n",
       "1003                 2021-12-05\n",
       "1004                 2021-12-05\n",
       "1005                 2021-12-05\n",
       "Name: DATE_OF_COLLECTION, Length: 996, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def validate_date(col, df, na_values=['NOT_COLLECTED','NOT_APPLICABLE']):\n",
    "    \n",
    "    logging.info('validating date column {!r}'.format(col))\n",
    "\n",
    "    if col not in df.columns:\n",
    "        logging.error('{!r} column not found in manifest'.format(col))\n",
    "        return\n",
    "    series = df[col]\n",
    "    series = exclude_missing(series, na_values)\n",
    "    \n",
    "    # invalid date formats\n",
    "    # empty string converted to NaT\n",
    "    date_series = pd.to_datetime(series, format='%Y-%m-%d', errors='coerce')\n",
    "    if date_series.isna().any():\n",
    "        logging.error('invalid dates in {!r}: {}'.format(col, \n",
    "                                                         series[date_series.isna()].unique()))\n",
    "    valid_date_series = date_series[~date_series.isna()]\n",
    "    \n",
    "    # dates in future\n",
    "    future_dates = (valid_date_series > datetime.datetime.today())\n",
    "    if future_dates.any():\n",
    "        logging.error('future dates in {!r}: {}'.format(col,\n",
    "            valid_date_series[future_dates].to_list()))\n",
    "        \n",
    "    # dates too old\n",
    "    old_dates = (valid_date_series < datetime.datetime.strptime('1900-01-01', '%Y-%m-%d'))\n",
    "    if old_dates.any():\n",
    "        logging.error(\"pre-1900 dates in {!r}: {}\".format(col,\n",
    "            valid_date_series[old_dates].to_list())) \n",
    "    \n",
    "    return valid_date_series\n",
    "df.loc[1,'DATE_OF_COLLECTION'] = 'NOT_COLLECTED'\n",
    "validate_date('DATE_OF_COLLECTION', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] validating time column 'TIME_OF_COLLECTION'\n",
      "[INFO] excluding 12 ['NOT_COLLECTED', 'NOT_APPLICABLE'] samples without data in 'TIME_OF_COLLECTION'\n",
      "[ERROR] invalid times in 'TIME_OF_COLLECTION': ['']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SERIES\n",
       "example-small        1900-01-01 11:22:00\n",
       "example-large        1900-01-01 11:22:00\n",
       "example-handcaught   1900-01-01 13:00:00\n",
       "1                    1900-01-01 11:00:00\n",
       "2                    1900-01-01 11:00:00\n",
       "                             ...        \n",
       "1001                 1900-01-01 10:30:00\n",
       "1002                 1900-01-01 10:30:00\n",
       "1003                 1900-01-01 10:30:00\n",
       "1004                 1900-01-01 10:30:00\n",
       "1005                 1900-01-01 10:30:00\n",
       "Name: TIME_OF_COLLECTION, Length: 997, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def validate_time(col, df, na_values=['NOT_COLLECTED','NOT_APPLICABLE']):\n",
    "    \n",
    "    logging.info('validating time column {!r}'.format(col))\n",
    "    \n",
    "    if col not in df.columns:\n",
    "        logging.error('{!r} column not found in manifest'.format(col))\n",
    "        return\n",
    "    series = df[col]\n",
    "    series = exclude_missing(series, na_values)\n",
    "        \n",
    "    # invalid time formats\n",
    "    # NB empty string converted to NaT\n",
    "    time_series = pd.to_datetime(series, format='%H:%M:%S', errors='coerce')\n",
    "    if time_series.isna().any():\n",
    "        logging.error('invalid times in {!r}: {}'.format(col, \n",
    "                                                         series[time_series.isna()].unique()))\n",
    "    valid_time_series = time_series[~time_series.isna()]\n",
    "    \n",
    "    return valid_time_series\n",
    "# df.loc[1,'TIME_OF_COLLECTION'] = '23'\n",
    "validate_time('TIME_OF_COLLECTION', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] validating time period column 'DURATION_OF_COLLECTION'\n",
      "[INFO] excluding 13 ['NOT_COLLECTED', 'NOT_APPLICABLE'] samples without data in 'DURATION_OF_COLLECTION'\n",
      "[ERROR] invalid times in 'DURATION_OF_COLLECTION': ['']\n"
     ]
    }
   ],
   "source": [
    "def validate_time_period(col, df, na_values=['NOT_COLLECTED','NOT_APPLICABLE']):\n",
    "    \n",
    "    logging.info('validating time period column {!r}'.format(col))\n",
    "    \n",
    "    if col not in df.columns:\n",
    "        logging.error('{!r} column not found in manifest'.format(col))\n",
    "        return\n",
    "    series = df[col]\n",
    "    series = exclude_missing(series, na_values)\n",
    "\n",
    "    # conversion with modifications for proper parsing \n",
    "    # by pd.Timedelta (does not accept missing data, e.g. 'PT1H')\n",
    "    # note - will not work for weeks and months\n",
    "    def convert_iso_duration(s):\n",
    "        if s == np.nan:\n",
    "            return np.nan\n",
    "        if not s.startswith('P') or 'T' not in s:\n",
    "            return np.nan\n",
    "        # add days\n",
    "        if s.startswith('PT'):\n",
    "            s = s.replace('PT','P0DT')\n",
    "        # add trailing minutes and seconds\n",
    "        if s.endswith('H'):\n",
    "            s += '0M0S'\n",
    "        elif s.endswith('M'):\n",
    "            s += '0S'\n",
    "        try:\n",
    "            return pd.Timedelta(s)\n",
    "        except:\n",
    "            return np.nan\n",
    "    time_period_series = series.apply(convert_iso_duration)\n",
    "    if time_period_series.isna().any():\n",
    "        logging.error('invalid times in {!r}: {}'.format(col, \n",
    "            series[time_period_series.isna()].unique()))\n",
    "    valid_time_period_series = time_period_series[~time_period_series.isna()]\n",
    "    return valid_time_period_series\n",
    "\n",
    "# df.loc[1,'DURATION_OF_COLLECTION'] = 'PVT1H'\n",
    "validate_time_period('DURATION_OF_COLLECTION', df);\n",
    "# df['DURATION_OF_COLLECTION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] validating country with coordinates\n",
      "[ERROR] multiple partner countries for coordinates 'NOT_APPLICABLE, NOT_APPLICABLE': ['UNITED KINGDOM' 'NOT_APPLICABLE']skipping coordinate validation\n",
      "[ERROR] multiple partner countries for coordinates '50.598618, -3.7209498': ['UNITED KINGDOM' '']skipping coordinate validation\n",
      "[WARNING] could not locate country for coordinates ', ', partner country ''\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COLLECTION_LOCATION</th>\n",
       "      <th>DECIMAL_LATITUDE</th>\n",
       "      <th>DECIMAL_LONGITUDE</th>\n",
       "      <th>coord</th>\n",
       "      <th>partner_country</th>\n",
       "      <th>coord_country</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SERIES</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>example-small</th>\n",
       "      <td>UNITED KINGDOM | ENGLAND | SAFFRON WALDEN | VI...</td>\n",
       "      <td>52.0236</td>\n",
       "      <td>0.2389</td>\n",
       "      <td>52.0236, 0.2389</td>\n",
       "      <td>UNITED KINGDOM</td>\n",
       "      <td>UNITED KINGDOM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>example-large</th>\n",
       "      <td>UNITED KINGDOM | ENGLAND | SAFFRON WALDEN | VI...</td>\n",
       "      <td>52.0236</td>\n",
       "      <td>0.2389</td>\n",
       "      <td>52.0236, 0.2389</td>\n",
       "      <td>UNITED KINGDOM</td>\n",
       "      <td>UNITED KINGDOM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>example-handcaught</th>\n",
       "      <td>UNITED KINGDOM | ENGLAND | BICESTER | CAVERSFI...</td>\n",
       "      <td>51.917197</td>\n",
       "      <td>-1.148376</td>\n",
       "      <td>51.917197, -1.148376</td>\n",
       "      <td>UNITED KINGDOM</td>\n",
       "      <td>UNITED KINGDOM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UNITED KINGDOM | ENGLAND | DEVON | EAST DARTMO...</td>\n",
       "      <td>50.592446</td>\n",
       "      <td>-3.727224</td>\n",
       "      <td>50.592446, -3.727224</td>\n",
       "      <td>UNITED KINGDOM</td>\n",
       "      <td>UNITED KINGDOM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UNITED KINGDOM | ENGLAND | DEVON | EAST DARTMO...</td>\n",
       "      <td>50.592446</td>\n",
       "      <td>-3.727224</td>\n",
       "      <td>50.592446, -3.727224</td>\n",
       "      <td>UNITED KINGDOM</td>\n",
       "      <td>UNITED KINGDOM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1292</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>,</td>\n",
       "      <td></td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1293</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>,</td>\n",
       "      <td></td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>,</td>\n",
       "      <td></td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1295</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>,</td>\n",
       "      <td></td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>,</td>\n",
       "      <td></td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1299 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  COLLECTION_LOCATION  \\\n",
       "SERIES                                                                  \n",
       "example-small       UNITED KINGDOM | ENGLAND | SAFFRON WALDEN | VI...   \n",
       "example-large       UNITED KINGDOM | ENGLAND | SAFFRON WALDEN | VI...   \n",
       "example-handcaught  UNITED KINGDOM | ENGLAND | BICESTER | CAVERSFI...   \n",
       "1                   UNITED KINGDOM | ENGLAND | DEVON | EAST DARTMO...   \n",
       "2                   UNITED KINGDOM | ENGLAND | DEVON | EAST DARTMO...   \n",
       "...                                                               ...   \n",
       "1292                                                                    \n",
       "1293                                                                    \n",
       "1294                                                                    \n",
       "1295                                                                    \n",
       "1296                                                                    \n",
       "\n",
       "                   DECIMAL_LATITUDE DECIMAL_LONGITUDE                 coord  \\\n",
       "SERIES                                                                        \n",
       "example-small               52.0236            0.2389       52.0236, 0.2389   \n",
       "example-large               52.0236            0.2389       52.0236, 0.2389   \n",
       "example-handcaught        51.917197         -1.148376  51.917197, -1.148376   \n",
       "1                         50.592446         -3.727224  50.592446, -3.727224   \n",
       "2                         50.592446         -3.727224  50.592446, -3.727224   \n",
       "...                             ...               ...                   ...   \n",
       "1292                                                                     ,    \n",
       "1293                                                                     ,    \n",
       "1294                                                                     ,    \n",
       "1295                                                                     ,    \n",
       "1296                                                                     ,    \n",
       "\n",
       "                   partner_country   coord_country  \n",
       "SERIES                                              \n",
       "example-small       UNITED KINGDOM  UNITED KINGDOM  \n",
       "example-large       UNITED KINGDOM  UNITED KINGDOM  \n",
       "example-handcaught  UNITED KINGDOM  UNITED KINGDOM  \n",
       "1                   UNITED KINGDOM  UNITED KINGDOM  \n",
       "2                   UNITED KINGDOM  UNITED KINGDOM  \n",
       "...                            ...             ...  \n",
       "1292                                       UNKNOWN  \n",
       "1293                                       UNKNOWN  \n",
       "1294                                       UNKNOWN  \n",
       "1295                                       UNKNOWN  \n",
       "1296                                       UNKNOWN  \n",
       "\n",
       "[1299 rows x 6 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to be replaced/supported by w3w check\n",
    "def check_location(df, fn):\n",
    "    \n",
    "    logging.info('validating country with coordinates')\n",
    "    \n",
    "    loc_col, lat_col, lon_col = 'COLLECTION_LOCATION', 'DECIMAL_LATITUDE', 'DECIMAL_LONGITUDE'\n",
    "\n",
    "    try:\n",
    "        loc_df_complete = df[[loc_col, lat_col, lon_col]].copy()\n",
    "    except:\n",
    "        logging.error('One of {!r} {!r} {!r} columns not found in manifest'.format(loc_col, lat_col, lon_col))\n",
    "        return\n",
    "#     loc_df_isna = (loc_df.isin(na_values)).all(axis=1)\n",
    "#     if loc_df_isna.any():\n",
    "#         logging.info('removing {} {!r} samples with missing data from coordinate analysis'.format(\n",
    "#                 loc_df_isna.sum(), na_values))\n",
    "#     loc_df_complete = loc_df[~loc_df_isna].copy()\n",
    "    \n",
    "    # coordinates in geopy format\n",
    "    loc_df_complete['coord'] = loc_df_complete.apply(lambda x: '{}, {}'.format(\n",
    "            x[lat_col], x[lon_col]), axis=1)\n",
    "    \n",
    "    # get location data for coordinates\n",
    "    # use local copy of web query results for re-runs\n",
    "    # this \n",
    "    loc_fn = fn+'_loc.pkl'\n",
    "    if os.path.isfile(loc_fn):\n",
    "        locations = pickle.load(open(loc_fn, \"rb\"))\n",
    "    else:\n",
    "        # web map server - openstreetmaps\n",
    "        logging.info('querying coordinates')\n",
    "        locator = Nominatim(user_agent='myGeocoder')\n",
    "        rgeocode = RateLimiter(locator.reverse, min_delay_seconds=1)\n",
    "\n",
    "        locations = dict()\n",
    "        for c in loc_df_complete.coord.unique():\n",
    "            # pre-fill with unknown country\n",
    "            locations[c] = {'address':{'country':'UNKNOWN'}}\n",
    "            # check coordniate correctness\n",
    "            try:\n",
    "                lat, lon = c.split(', ')\n",
    "                lat, lon = float(lat), float(lon)\n",
    "            except:\n",
    "                logging.error('problem parsing coordinates {!r}'.format(c))\n",
    "                continue\n",
    "            if abs(lat) > 90:\n",
    "                logging.error('invalid latitude {}, should be in [-90,90]'.format(lat))\n",
    "                continue\n",
    "            if abs(lon) > 180:\n",
    "                logging.error('invalid longitude {}, should be in [-180,180]'.format(lon))\n",
    "                continue\n",
    "            # web query\n",
    "            location = rgeocode(c, language='en-gb')\n",
    "            # rgeocode returns empty location outside of counries and in some other situations\n",
    "            if location is not None:\n",
    "                locations[c] = location.raw\n",
    "\n",
    "        # save locations to file\n",
    "        pickle.dump(locations, open(loc_fn, \"wb\"))\n",
    "        \n",
    "    # parse country from partner input\n",
    "    loc_df_complete['partner_country'] = loc_df_complete[loc_col].apply(lambda x: x.split('|')[0].strip().upper())\n",
    "    \n",
    "    # extract countries from location data\n",
    "    loc_countries = dict()\n",
    "    for coord in locations.keys():\n",
    "        coord_country = locations[coord]['address']['country'].upper()\n",
    "        loc_countries[coord] = coord_country\n",
    "        \n",
    "        partner_countries = loc_df_complete.loc[loc_df_complete.coord == coord, 'partner_country']\n",
    "        if partner_countries.nunique() > 1:\n",
    "            logging.error('multiple partner countries for coordinates {!r}: {}'\n",
    "                          'skipping coordinate validation'.format(\n",
    "                                coord, partner_countries.unique()))\n",
    "            continue\n",
    "        if partner_countries.shape[0] == 0:\n",
    "            logging.error('no partner location found for coordinates {!r}'.format(coord))\n",
    "            continue\n",
    "        partner_country = partner_countries.iloc[0]\n",
    "        if coord_country == 'UNKNOWN':\n",
    "            logging.warning('could not locate country for coordinates {!r}, partner country {!r}'.format(\n",
    "                    coord, partner_country))\n",
    "        elif partner_country != coord_country:\n",
    "            logging.error('country mismatch for coordinates {!r}, partner country {!r}, '\n",
    "                          'coordinate country {!r}'.format(coord, partner_country, coord_country))\n",
    "    \n",
    "    # countries based on coordinates\n",
    "    loc_df_complete['coord_country'] = loc_df_complete['coord'].replace(loc_countries)\n",
    "    country_mismatch = (loc_df_complete.coord_country != loc_df_complete.partner_country)\n",
    "\n",
    "#     if country_mismatch.any():\n",
    "#         logging.error('coordinates do not match country for SERIES: {}'.format(\n",
    "#                 country_mismatch[country_mismatch].index.to_list()))\n",
    "    \n",
    "    # location data can be re-used, e.g. as an additional field\n",
    "    return loc_df_complete\n",
    "# df.loc[2,'DECIMAL_LATITUDE'] = '65'\n",
    "loc_test = check_location(df, fn)\n",
    "loc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] validating taxonomy against NCBI\n",
      "[INFO] validating ORDER against NCBI\n",
      "[ERROR] ORDER: unexpected case for \"diptera\", changing to \"Diptera\" for validation\n",
      "[ERROR] ORDER: unexpected case for \"Diptera and Arachnidae\", changing to \"Diptera and arachnidae\" for validation\n",
      "[ERROR] ORDER: {'', 'Tricoptera', 'Acari (subclass)', 'Diptera and arachnidae', 'Symphyleona'} not found in NCBI Taxonomy\n",
      "[WARNING] ORDER: found unexpected rank for Collembola (taxid 30001): class\n",
      "[INFO] ORDER: using only first matching rank for Plecoptera (taxid 50622): order\n",
      "[WARNING] ORDER: found unexpected rank for Protura (taxid 29999): class\n",
      "[INFO] ORDER {'Coleoptera': 7041, 'Collembola': 30001, 'Dermaptera': 27434, 'Diptera': 7147, 'Ephemeroptera': 30073, 'Hemiptera': 7524, 'Hymenoptera': 7399, 'Lepidoptera': 7088, 'Megaloptera': 50553, 'Opiliones': 43271, 'Plecoptera': 50622, 'Protura': 29999, 'Psocoptera': 30259}\n",
      "[INFO] validating FAMILY against NCBI\n",
      "[ERROR] FAMILY: unexpected case for \"Unkown and Acari\", changing to \"Unkown and acari\" for validation\n",
      "[ERROR] FAMILY: {'', 'Aphidiodea', 'Unkown and acari'} not found in NCBI Taxonomy\n",
      "[WARNING] FAMILY: found unexpected rank for Aphidoidea (taxid 33385): superfamily\n",
      "[INFO] FAMILY {'Aphidoidea': 33385, 'Apidae': 7458, 'Baetidae': 172515, 'Carabidae': 41073, 'Chrysomelidae': 27439, 'Cicadellidae': 30102, 'Dolichopodidae': 92558, 'Elateridae': 30009, 'Formicidae': 36668, 'Limnephilidae': 50645, 'Psychodidae': 7197, 'Ptiliidae': 75545, 'Syrphidae': 34680, 'Tephritidae': 7211}\n",
      "[INFO] validating GENUS against NCBI\n",
      "[ERROR] GENUS: {''} not found in NCBI Taxonomy\n",
      "[INFO] GENUS: using only first matching rank for Bombus (taxid 28641): genus\n",
      "[INFO] GENUS {'Baetis': 189838, 'Bombus': 28641, 'Episyrphus': 286458, 'Formica': 72766, 'Syrphus': 224255, 'Urophora': 28620}\n",
      "[INFO] validating SCIENTIFIC_NAME against NCBI\n",
      "[ERROR] SCIENTIFIC_NAME: unexpected case for \"blank sample\", changing to \"Blank sample\" for validation\n",
      "[ERROR] SCIENTIFIC_NAME: {''} not found in NCBI Taxonomy\n",
      "[INFO] SCIENTIFIC_NAME {'Baetis rhodani': 189839, 'Blank sample': 2582415, 'Episyrphus balteatus': 286459, 'Formica rufa': 258706, 'Syrphus ribesii': 1124549, 'Urophora cardui': 503482}\n",
      "[WARNING] cannot validate ORDER for \"Acari (subclass)\", skipping taxonomy consistency check\n",
      "[WARNING] cannot validate ORDER for \"diptera\", skipping taxonomy consistency check\n",
      "[WARNING] cannot validate ORDER for \"Diptera and Arachnidae\", skipping taxonomy consistency check\n",
      "[ERROR] Family Syrphidae (taxid 34680) does not belong to Hymenoptera (taxid 7399)\n",
      "[ERROR] Genus Syrphus (taxid 224255) does not belong to Hymenoptera (taxid 7399)\n",
      "[ERROR] Species Syrphus ribesii (taxid 1124549) does not belong to Hymenoptera (taxid 7399)\n",
      "[WARNING] cannot validate ORDER for \"Tricoptera\", skipping taxonomy consistency check\n",
      "[WARNING] cannot validate FAMILY for \"Aphidiodea\", skipping taxonomy consistency check\n",
      "[WARNING] cannot validate ORDER for \"Tricoptera\", skipping taxonomy consistency check\n",
      "[WARNING] cannot validate ORDER for \"Symphyleona\", skipping taxonomy consistency check\n",
      "[WARNING] cannot validate ORDER for \"\", skipping taxonomy consistency check\n"
     ]
    }
   ],
   "source": [
    "def validate_ncbi_taxonomy(df, ncbi, na_values = ['NOT_COLLECTED','NOT_APPLICABLE']):\n",
    "    \n",
    "    logging.info('validating taxonomy against NCBI')\n",
    "    \n",
    "    tax_columns = [\n",
    "        'ORDER',\n",
    "        'FAMILY',\n",
    "        'GENUS',\n",
    "        'SCIENTIFIC_NAME'\n",
    "    ]        \n",
    "    \n",
    "    hierarchies = df[tax_columns].drop_duplicates().copy()\n",
    "    \n",
    "    tax_info = dict()\n",
    "    \n",
    "    for tax_level in tax_columns:\n",
    "        \n",
    "        logging.info(f'validating {tax_level} against NCBI')\n",
    "        \n",
    "        if tax_level not in df.columns:\n",
    "                logging.error(f'{tax_level} column not found in manifest')\n",
    "                continue\n",
    "            \n",
    "        tax_names = list(hierarchies[tax_level].unique())\n",
    "        \n",
    "        for na_value in na_values:\n",
    "            try:\n",
    "                tax_names.remove(na_value)\n",
    "            except:\n",
    "                pass \n",
    "            \n",
    "        for i, tax_name in enumerate(tax_names):\n",
    "            if len(tax_name) == 0:\n",
    "                continue\n",
    "            corr_tax_name = tax_name[0].upper() + tax_name[1:].lower()\n",
    "            if corr_tax_name != tax_name:\n",
    "                logging.error(f'{tax_level}: unexpected case for \"{tax_name}\", '\n",
    "                              f'changing to \"{corr_tax_name}\" for validation')\n",
    "            tax_names[i] = corr_tax_name\n",
    "        \n",
    "        tax_info[tax_level] = ncbi.get_name_translator(tax_names) \n",
    "        \n",
    "        unmatched_names = set(tax_names) - set(tax_info[tax_level].keys())\n",
    "        logging.error(f'{tax_level}: {unmatched_names} not found in NCBI Taxonomy')\n",
    "        \n",
    "        expected_rank = 'species' if (tax_level == 'SCIENTIFIC_NAME') else tax_level.lower()\n",
    "        \n",
    "        for tname, tids in tax_info[tax_level].items():\n",
    "            \n",
    "            ranks = ncbi.get_rank(tids)\n",
    "            \n",
    "            upd_tid = tids[0]\n",
    "            \n",
    "            if len(tids) == 1:\n",
    "                if ranks[upd_tid] != expected_rank: \n",
    "                    logging.warning(f'{tax_level}: found unexpected rank for {tname} (taxid {upd_tid}): {ranks[upd_tid]}')\n",
    "            if len(tids) > 1:            \n",
    "                for tid, r in ranks.items():\n",
    "                    if r == expected_rank and len(tids) > 1:\n",
    "                        logging.info(f'{tax_level}: using only first matching rank for {tname} (taxid {tid}): {r}')\n",
    "                        upd_tid = tid\n",
    "                        break\n",
    "                else:\n",
    "                    logging.warning(f'{tax_level}: could not find matching rank for {tname}, '\n",
    "                                    f'using (taxid {upd_tid}): {ranks[upd_tid]}')\n",
    "                    \n",
    "            tax_info[tax_level][tname] = upd_tid\n",
    "        \n",
    "        logging.info(f'{tax_level} {tax_info[tax_level]}')\n",
    "                    \n",
    "    # check correctness of taxonomy\n",
    "    for _, r in hierarchies.iterrows():\n",
    "        \n",
    "        if r.ORDER in na_values:\n",
    "            continue\n",
    "        try:\n",
    "            order_id = tax_info['ORDER'][r.ORDER]\n",
    "        except KeyError:\n",
    "            logging.warning(f'cannot validate ORDER for \"{r.ORDER}\", skipping taxonomy consistency check')\n",
    "            continue\n",
    "            \n",
    "        if r.FAMILY in na_values:\n",
    "            continue\n",
    "        try:\n",
    "            family_id = tax_info['FAMILY'][r.FAMILY]\n",
    "            \n",
    "            family_lineage = ncbi.get_lineage(family_id)\n",
    "            \n",
    "            if order_id not in family_lineage:\n",
    "                logging.error(f'Family {r.FAMILY} (taxid {family_id}) does not belong to {r.ORDER} (taxid {order_id})')\n",
    "        except KeyError:\n",
    "            logging.warning(f'cannot validate FAMILY for \"{r.FAMILY}\", skipping taxonomy consistency check')\n",
    "            continue\n",
    "            \n",
    "        if r.GENUS in na_values:\n",
    "            continue\n",
    "        try:\n",
    "            genus_id = tax_info['GENUS'][r.GENUS]\n",
    "            \n",
    "            genus_lineage = ncbi.get_lineage(genus_id)\n",
    "            \n",
    "            if order_id not in genus_lineage:\n",
    "                logging.error(f'Genus {r.GENUS} (taxid {genus_id}) does not belong to {r.ORDER} (taxid {order_id})')\n",
    "            if family_id not in genus_lineage:\n",
    "                logging.error(f'Genus {r.GENUS} (taxid {genus_id}) does not belong to {r.FAMILY} (taxid {family_id})')\n",
    "        except KeyError:\n",
    "            logging.warning(f'cannot validate GENUS for \"{r.GENUS}\", skipping taxonomy consistency check')\n",
    "            continue\n",
    "            \n",
    "        if r.SCIENTIFIC_NAME in na_values:\n",
    "            continue\n",
    "        try:\n",
    "            species_id = tax_info['SCIENTIFIC_NAME'][r.SCIENTIFIC_NAME]\n",
    "            \n",
    "            species_lineage = ncbi.get_lineage(species_id)\n",
    "            \n",
    "            if order_id not in species_lineage:\n",
    "                logging.error(f'Species {r.SCIENTIFIC_NAME} (taxid {species_id}) does not belong to {r.ORDER} (taxid {order_id})')\n",
    "            if family_id not in species_lineage:\n",
    "                logging.error(f'Species {r.SCIENTIFIC_NAME} (taxid {species_id}) does not belong to {r.FAMILY} (taxid {family_id})')\n",
    "            if genus_id not in species_lineage:\n",
    "                logging.error(f'Species {r.SCIENTIFIC_NAME} (taxid {species_id}) does not belong to {r.GENUS} (taxid {genus_id})')\n",
    "        except KeyError:\n",
    "            logging.warning(f'cannot validate SCIENTIFIC_NAME for \"{r.SCIENTIFIC_NAME}\", skipping taxonomy consistency check')\n",
    "            continue\n",
    "            \n",
    "    return df\n",
    "        \n",
    "                \n",
    "validate_ncbi_taxonomy(df, ncbi);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] validating int format in TIME_ELAPSED_FROM_COLLECTION_TO_PLATING\n",
      "[INFO] excluding 12 ['NOT_COLLECTED', 'NOT_APPLICABLE'] samples without data in 'TIME_ELAPSED_FROM_COLLECTION_TO_PLATING'\n",
      "[ERROR] found non-int value in TIME_ELAPSED_FROM_COLLECTION_TO_PLATING: \"\"\n"
     ]
    }
   ],
   "source": [
    "def validate_int(col, df, na_values=['NOT_COLLECTED','NOT_APPLICABLE']):\n",
    "    \n",
    "    logging.info(f'validating int format in {col}')\n",
    "    \n",
    "    if col not in df.columns:\n",
    "        logging.error(f'{col} column not found in manifest')\n",
    "        return\n",
    "    series = df[col]\n",
    "    series = exclude_missing(series, na_values)\n",
    "    \n",
    "    for val in series.unique():\n",
    "        try:\n",
    "            int(val)\n",
    "        except:\n",
    "            logging.error(f'found non-int value in {col}: \"{val}\"')\n",
    "validate_int('TIME_ELAPSED_FROM_COLLECTION_TO_PLATING', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] validating date column 'DATE_OF_COLLECTION'\n",
      "[INFO] excluding 13 ['NOT_COLLECTED', 'NOT_APPLICABLE'] samples without data in 'DATE_OF_COLLECTION'\n",
      "[ERROR] invalid dates in 'DATE_OF_COLLECTION': ['']\n",
      "[INFO] validating date column 'DATE_OF_PRESERVATION'\n",
      "[INFO] excluding 2 ['NOT_COLLECTED', 'NOT_APPLICABLE'] samples without data in 'DATE_OF_PRESERVATION'\n",
      "[ERROR] invalid dates in 'DATE_OF_PRESERVATION': ['']\n"
     ]
    }
   ],
   "source": [
    "bd = validate_date('DATE_OF_COLLECTION', df)\n",
    "ad = validate_date('DATE_OF_PRESERVATION', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctdf = pd.concat([bd, ad], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "example-small         False\n",
       "example-large         False\n",
       "example-handcaught    False\n",
       "2                     False\n",
       "3                     False\n",
       "                      ...  \n",
       "1001                  False\n",
       "1002                  False\n",
       "1003                  False\n",
       "1004                  False\n",
       "1005                  False\n",
       "Length: 996, dtype: bool"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctdf.iloc[:, 0] > ctdf.iloc[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_dates(before, after):\n",
    "    \n",
    "    ctdf = pd.concat([before, after], axis=1)\n",
    "    date_conflict = ctdf[before.name] > ctdf[after.name]\n",
    "    \n",
    "#     logging.info(date_conflict)\n",
    "    if date_conflict.any():\n",
    "        logging.error(f'{before.name} values are later than {after.name} for SERIES'\n",
    "                      f' {ctdf[date_conflict].index.to_list()}')\n",
    "    \n",
    "compare_dates(bd, ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] # started validate_partner_manifest_v.1.0\n",
      "[INFO] reading data from 'data/NE BIOSCAN_Manifest_V1.0_Yarner_2021.xlsx'\n",
      "[WARNING] trailing spaces found in column 'WHAT_3_WORDS', rows [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184]. Removing for validation\n",
      "[WARNING] trailing spaces found in column 'ORDER', rows [88, 343, 344, 345, 650, 756, 757, 758, 759, 760, 833, 861, 905, 906, 907, 908, 936]. Removing for validation\n",
      "[INFO] reading data from '../data/BIOSCAN_Manifest_V1.0_20211207.xlsx'\n",
      "[INFO] checking manifest columns against template\n",
      "[INFO] extracting value validation data from '../data/BIOSCAN_Manifest_V1.0_20211207.xlsx'\n",
      "[INFO] Checking and excluding blank samples\n",
      "[ERROR] last well H12 is not blank at SERIES [96]: in SCIENTIFIC_NAME, expected \"blank sample\", found ['NOT_APPLICABLE']\n",
      "[INFO] found 10 blank samples based on SCIENTIFIC_NAME\n",
      "[INFO] validating SERIES\n",
      "[ERROR] In SERIES, {'1299', '1297', '1298'} are missing, {'example-large', 'example-small', 'example-handcaught'} are unexpected\n",
      "[INFO] validating RACK_OR_PLATE_ID and TUBE_OR_WELL_ID\n",
      "[INFO] found 1299 samples across 13 plates\n",
      "[ERROR] duplicate TUBE_OR_WELL_ID for plate : ['']\n",
      "[ERROR] in TUBE_OR_WELL_ID for plate , wells {'E4', 'B5', 'G8', 'E3', 'H2', 'E6', 'B7', 'H11', 'E11', 'A5', 'F3', 'D6', 'G10', 'F11', 'B4', 'C3', 'G4', 'A12', 'B12', 'F4', 'D12', 'C11', 'G7', 'H6', 'H3', 'B8', 'H5', 'C6', 'F7', 'F5', 'H1', 'G9', 'E1', 'C1', 'G5', 'A1', 'E9', 'B6', 'A8', 'B1', 'F8', 'H8', 'H10', 'H7', 'F9', 'G3', 'D10', 'A4', 'F12', 'C7', 'A7', 'C8', 'G2', 'C12', 'C2', 'A10', 'D5', 'D1', 'E7', 'D3', 'H9', 'B10', 'A6', 'D7', 'F1', 'F6', 'C4', 'D11', 'G6', 'E10', 'D8', 'G11', 'G12', 'E12', 'B2', 'B9', 'A9', 'D2', 'H12', 'F2', 'C10', 'D4', 'A2', 'A11', 'E5', 'G1', 'H4', 'C9', 'F10', 'A3', 'E2', 'C5', 'E8', 'D9', 'B3', 'B11'} are missing, wells {''} are excessive\n",
      "[ERROR] in TUBE_OR_WELL_ID for plate SHAP-001, wells {'E4', 'B5', 'G8', 'E3', 'H2', 'E6', 'B7', 'H11', 'E11', 'A5', 'F3', 'D6', 'G10', 'F11', 'B4', 'C3', 'G4', 'A12', 'B12', 'F4', 'D12', 'C11', 'G7', 'H6', 'H3', 'B8', 'H5', 'C6', 'F7', 'F5', 'H1', 'G9', 'E1', 'G5', 'E9', 'B6', 'A8', 'F8', 'H8', 'H10', 'H7', 'F9', 'G3', 'D10', 'A4', 'F12', 'C7', 'A7', 'C8', 'G2', 'C12', 'C2', 'A10', 'D5', 'D1', 'E7', 'D3', 'H9', 'B10', 'A6', 'D7', 'F1', 'F6', 'C4', 'D11', 'G6', 'E10', 'D8', 'G11', 'G12', 'E12', 'B2', 'B9', 'A9', 'D2', 'H12', 'F2', 'C10', 'D4', 'A2', 'A11', 'E5', 'G1', 'H4', 'C9', 'F10', 'A3', 'E2', 'C5', 'E8', 'D9', 'B3', 'B11'} are missing, wells set() are excessive\n",
      "[INFO] validating values in column 'PRESERVATIVE_SOLUTION'\n",
      "[ERROR] invalid values in 'PRESERVATIVE_SOLUTION': {''}\n",
      "[INFO] validating values in column 'TUBE_OR_WELL_ID'\n",
      "[ERROR] 'TUBE_OR_WELL_ID' column not found in validation sheet\n",
      "[INFO] validating values in column 'BOTTLE_DIRECTION'\n",
      "[ERROR] invalid values in 'BOTTLE_DIRECTION': {''}\n",
      "[INFO] validating values in column 'ORGANISM_PART'\n",
      "[ERROR] invalid values in 'ORGANISM_PART': {''}\n",
      "[INFO] validating values in column 'HAZARD_GROUP'\n",
      "[ERROR] invalid values in 'HAZARD_GROUP': {'', 'NOT_APPLICABLE'}\n",
      "[INFO] validating values in column 'REGULATORY_COMPLIANCE'\n",
      "[ERROR] invalid values in 'REGULATORY_COMPLIANCE': {'', 'y'}\n",
      "[INFO] validating date column 'DATE_OF_COLLECTION'\n",
      "[INFO] excluding 12 ['NOT_APPLICABLE'] samples without data in 'DATE_OF_COLLECTION'\n",
      "[ERROR] invalid dates in 'DATE_OF_COLLECTION': ['']\n",
      "[INFO] validating country with coordinates\n",
      "[ERROR] multiple partner countries for coordinates 'NOT_APPLICABLE, NOT_APPLICABLE': ['UNITED KINGDOM' 'NOT_APPLICABLE']skipping coordinate validation\n",
      "[ERROR] multiple partner countries for coordinates '50.598618, -3.7209498': ['UNITED KINGDOM' '']skipping coordinate validation\n",
      "[WARNING] could not locate country for coordinates ', ', partner country ''\n",
      "[INFO] validating taxonomy against NCBI\n",
      "[INFO] validating ORDER against NCBI\n",
      "[ERROR] ORDER: unexpected case for \"diptera\", changing to \"Diptera\" for validation\n",
      "[ERROR] ORDER: unexpected case for \"Diptera and Arachnidae\", changing to \"Diptera and arachnidae\" for validation\n",
      "[ERROR] ORDER: {'', 'Tricoptera', 'Acari (subclass)', 'Diptera and arachnidae', 'Symphyleona'} not found in NCBI Taxonomy\n",
      "[WARNING] ORDER: found unexpected rank for Collembola (taxid 30001): class\n",
      "[INFO] ORDER: using only first matching rank for Plecoptera (taxid 50622): order\n",
      "[WARNING] ORDER: found unexpected rank for Protura (taxid 29999): class\n",
      "[INFO] ORDER {'Coleoptera': 7041, 'Collembola': 30001, 'Dermaptera': 27434, 'Diptera': 7147, 'Ephemeroptera': 30073, 'Hemiptera': 7524, 'Hymenoptera': 7399, 'Lepidoptera': 7088, 'Megaloptera': 50553, 'Opiliones': 43271, 'Plecoptera': 50622, 'Protura': 29999, 'Psocoptera': 30259}\n",
      "[INFO] validating FAMILY against NCBI\n",
      "[ERROR] FAMILY: unexpected case for \"Unkown and Acari\", changing to \"Unkown and acari\" for validation\n",
      "[ERROR] FAMILY: {'', 'Aphidiodea', 'Unkown and acari'} not found in NCBI Taxonomy\n",
      "[WARNING] FAMILY: found unexpected rank for Aphidoidea (taxid 33385): superfamily\n",
      "[INFO] FAMILY {'Aphidoidea': 33385, 'Apidae': 7458, 'Baetidae': 172515, 'Carabidae': 41073, 'Chrysomelidae': 27439, 'Cicadellidae': 30102, 'Dolichopodidae': 92558, 'Elateridae': 30009, 'Formicidae': 36668, 'Limnephilidae': 50645, 'Psychodidae': 7197, 'Ptiliidae': 75545, 'Syrphidae': 34680, 'Tephritidae': 7211}\n",
      "[INFO] validating GENUS against NCBI\n",
      "[ERROR] GENUS: {''} not found in NCBI Taxonomy\n",
      "[INFO] GENUS: using only first matching rank for Bombus (taxid 28641): genus\n",
      "[INFO] GENUS {'Baetis': 189838, 'Bombus': 28641, 'Episyrphus': 286458, 'Formica': 72766, 'Syrphus': 224255, 'Urophora': 28620}\n",
      "[INFO] validating SCIENTIFIC_NAME against NCBI\n",
      "[ERROR] SCIENTIFIC_NAME: unexpected case for \"blank sample\", changing to \"Blank sample\" for validation\n",
      "[ERROR] SCIENTIFIC_NAME: {''} not found in NCBI Taxonomy\n",
      "[INFO] SCIENTIFIC_NAME {'Baetis rhodani': 189839, 'Blank sample': 2582415, 'Episyrphus balteatus': 286459, 'Formica rufa': 258706, 'Syrphus ribesii': 1124549, 'Urophora cardui': 503482}\n",
      "[WARNING] cannot validate ORDER for \"Acari (subclass)\", skipping taxonomy consistency check\n",
      "[WARNING] cannot validate ORDER for \"diptera\", skipping taxonomy consistency check\n",
      "[WARNING] cannot validate ORDER for \"Diptera and Arachnidae\", skipping taxonomy consistency check\n",
      "[ERROR] Family Syrphidae (taxid 34680) does not belong to Hymenoptera (taxid 7399)\n",
      "[ERROR] Genus Syrphus (taxid 224255) does not belong to Hymenoptera (taxid 7399)\n",
      "[ERROR] Species Syrphus ribesii (taxid 1124549) does not belong to Hymenoptera (taxid 7399)\n",
      "[WARNING] cannot validate ORDER for \"Tricoptera\", skipping taxonomy consistency check\n",
      "[WARNING] cannot validate FAMILY for \"Aphidiodea\", skipping taxonomy consistency check\n",
      "[WARNING] cannot validate ORDER for \"Tricoptera\", skipping taxonomy consistency check\n",
      "[WARNING] cannot validate ORDER for \"Symphyleona\", skipping taxonomy consistency check\n",
      "[WARNING] cannot validate ORDER for \"\", skipping taxonomy consistency check\n",
      "[INFO] validating values in column 'SEX'\n",
      "[ERROR] invalid values in 'SEX': {''}\n",
      "[INFO] validating time column 'TIME_OF_COLLECTION'\n",
      "[INFO] excluding 12 ['NOT_COLLECTED', 'NOT_APPLICABLE'] samples without data in 'TIME_OF_COLLECTION'\n",
      "[ERROR] invalid times in 'TIME_OF_COLLECTION': ['']\n",
      "[INFO] validating time period column 'DURATION_OF_COLLECTION'\n",
      "[INFO] excluding 13 ['NOT_COLLECTED', 'NOT_APPLICABLE'] samples without data in 'DURATION_OF_COLLECTION'\n",
      "[ERROR] invalid times in 'DURATION_OF_COLLECTION': ['']\n",
      "[INFO] validating values in column 'COLLECTION_METHOD'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] invalid values in 'COLLECTION_METHOD': {''}\n",
      "[INFO] validating int format in TIME_ELAPSED_FROM_COLLECTION_TO_PLATING\n",
      "[INFO] excluding 12 ['NOT_COLLECTED', 'NOT_APPLICABLE'] samples without data in 'TIME_ELAPSED_FROM_COLLECTION_TO_PLATING'\n",
      "[ERROR] found non-int value in TIME_ELAPSED_FROM_COLLECTION_TO_PLATING: \"\"\n",
      "[INFO] validating date column 'DATE_OF_PRESERVATION'\n",
      "[INFO] excluding 2 ['NOT_COLLECTED', 'NOT_APPLICABLE'] samples without data in 'DATE_OF_PRESERVATION'\n",
      "[ERROR] invalid dates in 'DATE_OF_PRESERVATION': ['']\n",
      "[INFO] validating int format in ELEVATION\n",
      "[ERROR] found non-int value in ELEVATION: \"\"\n",
      "[INFO] # ended validate_partner_manifest_v.1.0\n"
     ]
    }
   ],
   "source": [
    "def validate(fn, template_fn, verbose=False, version='1.0'):\n",
    "    '''\n",
    "    Validation follows the order of columns order in data entry sheet\n",
    "    '''\n",
    "\n",
    "    setup_logging(verbose=verbose)\n",
    "\n",
    "    logging.info('# started validate_partner_manifest_v.{}'.format(version))\n",
    "\n",
    "    # read data\n",
    "    df = get_data(fn)\n",
    "    \n",
    "    # read taxonomy\n",
    "    ncbi = ete3.NCBITaxa()\n",
    "    \n",
    "    # prepare for validation\n",
    "    template_df = get_data(template_fn)\n",
    "    check_columns(df, template_df)\n",
    "    valid_dict = get_valid_dict(template_fn)\n",
    "\n",
    "    # check blanks\n",
    "    blanks_mask = check_blanks(df)\n",
    "    \n",
    "    #orange cols\n",
    "    validate_series(df)\n",
    "    validate_plates_wells(df, 'RACK_OR_PLATE_ID', 'TUBE_OR_WELL_ID')\n",
    "    validate_values('PRESERVATIVE_SOLUTION', df, valid_dict)\n",
    "    validate_values('TUBE_OR_WELL_ID', df, valid_dict)\n",
    "    # CATCH_LOT not checked TODO do not allow missing\n",
    "    validate_values('BOTTLE_DIRECTION', df, valid_dict)\n",
    "    validate_values('ORGANISM_PART', df, valid_dict, sep='|')\n",
    "    validate_values('HAZARD_GROUP', df, valid_dict)\n",
    "    validate_values('REGULATORY_COMPLIANCE', df, valid_dict)\n",
    "    date_coll = validate_date('DATE_OF_COLLECTION', df, na_values=['NOT_APPLICABLE'])\n",
    "    check_location(df, fn)\n",
    "    \n",
    "    # purple cols\n",
    "    # taxonomy validation adds a few columns\n",
    "    df = validate_ncbi_taxonomy(df, ncbi, na_values = ['NOT_COLLECTED', 'NOT_APPLICABLE'])\n",
    "    validate_values('SEX', df, valid_dict)\n",
    "    # HABITAT not checked\n",
    "    validate_time('TIME_OF_COLLECTION', df)\n",
    "    validate_time_period('DURATION_OF_COLLECTION', df)\n",
    "    validate_values('COLLECTION_METHOD', df, valid_dict)\n",
    "    # DESCRIPTION_OF_COLLECTION_METHOD not checked\n",
    "    validate_int('TIME_ELAPSED_FROM_COLLECTION_TO_PLATING', df)\n",
    "    # PHOTOGRAPH_* columns not checked\n",
    "    # VOUCHER_ID not checked\n",
    "    # PRESERVATION_APPROACH not checked - should match DATE_OF_PRESERVATION\n",
    "    date_pres = validate_date('DATE_OF_PRESERVATION', df) # allow for empty values unlike DATE_OF_COLLECTION\n",
    "    compare_dates(before=date_coll, after=date_pres)\n",
    "    # COLLECTOR_SAMPLE_ID not checked\n",
    "    validate_int('ELEVATION', df)\n",
    "    # OTHER_INFORMATION\tMISC_METADATA\tIDENTIFIED_BY\tIDENTIFIER_AFFILIATION\tIDENTIFIED_HOW not checked\n",
    "        \n",
    "    logging.info('# ended validate_partner_manifest_v.{}'.format(version))\n",
    "\n",
    "    return df\n",
    "\n",
    "# fn = '../../results/partner_manifests/IRD-Neandersquito_T222Amplicon_Manifest_V2.0.xlsx'\n",
    "df = validate(fn, template_fn, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'data/Mike Ashworth NE 2021-06-24 BIOSCAN_Manifest_V1.0.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] for blanks, NOT_APPLICABLE expected, but not found in columns ['CATCH_LOT', 'BOTTLE_DIRECTION', 'WHAT_3_WORDS']\n",
      "[ERROR] duplicate TUBE_OR_WELL_ID for plate : ['']\n",
      "[ERROR] in TUBE_OR_WELL_ID for plate , wells {'E4', 'B5', 'G8', 'E3', 'H2', 'E6', 'B7', 'H11', 'E11', 'A5', 'F3', 'D6', 'G10', 'F11', 'B4', 'C3', 'G4', 'A12', 'B12', 'F4', 'D12', 'C11', 'G7', 'H6', 'H3', 'B8', 'H5', 'C6', 'F7', 'F5', 'H1', 'G9', 'E1', 'C1', 'G5', 'A1', 'E9', 'B6', 'A8', 'B1', 'F8', 'H8', 'H10', 'H7', 'F9', 'G3', 'D10', 'A4', 'F12', 'C7', 'A7', 'C8', 'G2', 'C12', 'C2', 'A10', 'D5', 'D1', 'E7', 'D3', 'H9', 'B10', 'A6', 'D7', 'F1', 'F6', 'C4', 'D11', 'G6', 'E10', 'D8', 'G11', 'G12', 'E12', 'B2', 'B9', 'A9', 'D2', 'H12', 'F2', 'C10', 'D4', 'A2', 'A11', 'E5', 'G1', 'H4', 'C9', 'F10', 'A3', 'E2', 'C5', 'E8', 'D9', 'B3', 'B11'} are missing, wells {''} are excessive\n",
      "[ERROR] invalid values in 'PRESERVATIVE_SOLUTION': {''}\n",
      "[ERROR] 'TUBE_OR_WELL_ID' column not found in validation sheet\n",
      "[ERROR] invalid values in 'BOTTLE_DIRECTION': {''}\n",
      "[ERROR] invalid values in 'ORGANISM_PART': {''}\n",
      "[ERROR] invalid values in 'HAZARD_GROUP': {'', 'NOT_APPLICABLE'}\n",
      "[ERROR] invalid values in 'REGULATORY_COMPLIANCE': {''}\n",
      "[ERROR] invalid dates in 'DATE_OF_COLLECTION': ['']\n",
      "[WARNING] could not locate country for coordinates ', ', partner country ''\n",
      "[ERROR] ORDER: {''} not found in NCBI Taxonomy\n",
      "[ERROR] FAMILY: {''} not found in NCBI Taxonomy\n",
      "[ERROR] GENUS: {''} not found in NCBI Taxonomy\n",
      "[ERROR] SCIENTIFIC_NAME: unexpected case for \"blank sample\", changing to \"Blank sample\" for validation\n",
      "[ERROR] SCIENTIFIC_NAME: {''} not found in NCBI Taxonomy\n",
      "[WARNING] cannot validate ORDER for \"\", skipping taxonomy consistency check\n",
      "[ERROR] invalid values in 'SEX': {''}\n",
      "[ERROR] invalid times in 'TIME_OF_COLLECTION': ['']\n",
      "[ERROR] invalid times in 'DURATION_OF_COLLECTION': ['']\n",
      "[ERROR] invalid values in 'COLLECTION_METHOD': {''}\n",
      "[ERROR] found non-int value in TIME_ELAPSED_FROM_COLLECTION_TO_PLATING: \"\"\n",
      "[ERROR] invalid dates in 'DATE_OF_PRESERVATION': ['']\n",
      "[ERROR] found non-int value in ELEVATION: \"\"\n"
     ]
    }
   ],
   "source": [
    "# TODO debug last well H12 is not blank at SERIES [96]; Diptera not in NCBI; unexpected rank - specify expected\n",
    "df = validate(fn, template_fn, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = pickle.load(open('data/NE BIOSCAN_Manifest_V1.0_Yarner_2021.xlsx_loc.pkl', \"rb\"))\n",
    "# locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
