{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "import itertools\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import ete3\n",
    "import geopy\n",
    "import re\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_VERSION = '0.2.0'\n",
    "ANOSPP_VERSION = '4.0'\n",
    "BIOSCAN_VERSION = '3.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.DEBUG)\n",
    "# logging.getLogger().setFormat('[%(levelname)s] %(message)s')\n",
    "\n",
    "def setup_logging(verbose=False):\n",
    "    try: \n",
    "        del logging.root.handlers[:]\n",
    "    except:\n",
    "        pass\n",
    "    if verbose:\n",
    "        logging.basicConfig(level=logging.DEBUG, format='[%(levelname)s] %(message)s')\n",
    "    else:\n",
    "        logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n",
    "setup_logging(verbose=True)   \n",
    "logging.debug('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anospp_fn = '../data/Anopheles_Metadata_Manifest_V4.0_20221220.xlsx'\n",
    "biosc_fn = '../data/BIOSCAN_Manifest_V2.0_20230727.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and install taxonomy\n",
    "ncbi = ete3.NCBITaxa()\n",
    "# run update if needed\n",
    "# ncbi.update_taxonomy_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(fn, sheet='TAB 2 Metadata Entry'):\n",
    "\n",
    "    logging.debug(f'reading data from \"{fn}\" sheet \"{sheet}\"')\n",
    "    \n",
    "    df = pd.read_excel(fn, dtype=str, index_col=0, keep_default_na=False,\n",
    "                       sheet_name=sheet)\n",
    "        \n",
    "    return df\n",
    "\n",
    "# df = get_data(anospp_fn, sheet='TAB 3 TEST Metadata Entry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_date_formats(df):\n",
    "    # time can be appended to date by conversion to string\n",
    "    # fixing date formats appearing due to string conversion on reading\n",
    "    logging.debug('fixing date formats by removing appended times')\n",
    "    \n",
    "    for col in ['DATE_OF_COLLECTION', 'DATE_OF_PLATING', 'DATE_OF_PRESERVATION']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].str.replace(r' 00:00:00$','')\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_bioscan_version(df):\n",
    "    \n",
    "    logging.debug(f'inferring bioscan version from data format')\n",
    "    \n",
    "    version_evidence = {}\n",
    "    \n",
    "    for col in ('CATCH_SOLUTION', 'AMOUNT_OF_CATCH_PLATED'):\n",
    "        if col in df.columns:\n",
    "            version_evidence[col] = 'v3'\n",
    "        else:\n",
    "            version_evidence[col] = 'v2'\n",
    "            \n",
    "    v3_plate_only = (('PLATE_ONLY_1_BLANK' in df['TUBE_OR_WELL_ID']) \n",
    "                     or ('PLATE_ONLY_2_BLANKS' in df['TUBE_OR_WELL_ID']))\n",
    "    if 'PLATE_ONLY' in df['TUBE_OR_WELL_ID']:\n",
    "        if v3_plate_only:\n",
    "            logging.error('found both bioscan v2 and v3 style PLATE_ONLY entries in manifest')\n",
    "            raise ValueError('fix bioscan manifest version before proceeding')\n",
    "        else:\n",
    "            version_evidence['PLATE_ONLY'] = 'v2'\n",
    "    elif v3_plate_only:\n",
    "        version_evidence['PLATE_ONLY'] = 'v3'\n",
    "        \n",
    "    if len(set(version_evidence.values())) > 1:\n",
    "        msg = 'found conflicting evidence on bioscan version: '\n",
    "        for k, v in version_evidence.items():\n",
    "            msg += f'{k} suggests {v}, '\n",
    "        logging.error(msg[:-2])\n",
    "        raise ValueError('fix bioscan manifest version before proceeding')\n",
    "        \n",
    "    # returns only if all evidence is concordant\n",
    "    v = version_evidence['CATCH_SOLUTION']\n",
    "    logging.info(f'bioscan manifest {v} inferred')\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_series(df):\n",
    "    \n",
    "    # series should be 1,2, ..., nsamples\n",
    "    logging.debug('validating SERIES column')\n",
    "    \n",
    "    if df.index.duplicated().any():\n",
    "        logging.error(f'duplicate SERIES column values: {df.index[df.index.duplicated()].to_list()}, '\n",
    "                      f'references to these SERIES can be wrong')\n",
    "    \n",
    "    # exclude non-numeric SERIES - these don't work with ranges\n",
    "    series_numeric = df.index.astype(str).str.isnumeric()\n",
    "    if not series_numeric.all():\n",
    "        logging.error(f'found rows with non-numeric SERIES column values: '\n",
    "                      f'{df.index[~series_numeric].to_list()} - '\n",
    "                      f'these will be excluded from validation and output')\n",
    "        df = df.loc[series_numeric]\n",
    "        \n",
    "    # check the remaining SERIES are continuous\n",
    "    expected_series = set([str(i) for i in range(1, df.shape[0] + 1)])\n",
    "    observed_series = set(df.index.astype(str))\n",
    "    if expected_series != observed_series:\n",
    "        logging.error(f'SERIES column values {sorted(list(expected_series - observed_series))} are missing, '\n",
    "                      f'{sorted(list(observed_series - expected_series))} are unexpected')\n",
    "        \n",
    "    return df\n",
    "        \n",
    "# df = validate_series(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_ranges(df):\n",
    "    \n",
    "    # based on https://stackoverflow.com/questions/4628333/converting-a-list-of-integers-into-range-in-python\n",
    "    i = df.index.astype(int).to_list()\n",
    "    ranges = []\n",
    "    for a, b in itertools.groupby(enumerate(i), lambda pair: pair[1] - pair[0]):\n",
    "        b = list(b)\n",
    "        if b[0][1] != b[-1][1]:\n",
    "            ranges.append(f'{b[0][1]}-{b[-1][1]}')\n",
    "        else:\n",
    "            ranges.append(f'{b[0][1]}')\n",
    "            \n",
    "    return ', '.join(ranges)\n",
    "\n",
    "# index_ranges(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_trailing_spaces(df):\n",
    "    for col in df.columns:\n",
    "        trailing_spaces = (df[col].str.startswith(' ') | df[col].str.endswith(' '))\n",
    "        if trailing_spaces.any():\n",
    "            logging.info(f'trailing spaces found in {col} column, '\n",
    "                         f'SERIES {index_ranges(df.loc[trailing_spaces])} - removing for validation and output')\n",
    "            df[col] = df[col].str.strip()\n",
    "            \n",
    "    return df\n",
    "\n",
    "# df = remove_trailing_spaces(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nonbreaking_spaces(df):\n",
    "    for col in df.columns:\n",
    "        nonbreaking_spaces = df[col].str.contains(u\"\\u00A0\")\n",
    "        if nonbreaking_spaces.any():\n",
    "            logging.info(f'non-breaking spaces found in {col} column, '\n",
    "                         f'SERIES {index_ranges(df.loc[nonbreaking_spaces])} - removing for validation and output')\n",
    "            df[col] = df[col].str.replace(u\"\\u00A0\", \" \")\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_columns(df, template_df):\n",
    "    \n",
    "    logging.debug('checking manifest columns against template')\n",
    "    \n",
    "    data_cols = set(df.columns)\n",
    "    template_cols = set(template_df.columns)\n",
    "        \n",
    "    if data_cols - template_cols != set():\n",
    "        logging.info(f'extra columns in filled manifest compared to template: {data_cols - template_cols}')\n",
    "    if template_cols - data_cols != set():\n",
    "        logging.error(f'template columns missing from filled manifest: {template_cols - data_cols}')\n",
    "\n",
    "# check_columns(df, template_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_dict(fn, validation_sheet='Data Validation - do not edit'):\n",
    "    \n",
    "    # pick up validation values from data validation sheet\n",
    "    logging.debug(f'extracting value validation data from {fn} sheet \"{validation_sheet}\"')\n",
    "    valid_df = pd.read_excel(fn, dtype=str, sheet_name=validation_sheet)\n",
    "    valid_dict = dict()\n",
    "    for col in valid_df.columns:\n",
    "        valid_dict[col] = valid_df[col].dropna().to_list()\n",
    "    \n",
    "    return valid_dict\n",
    "\n",
    "# valid_dict = get_valid_dict(anospp_fn, validation_sheet='TAB 5 Data Validation - do not ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_missing(series, na_values=[]):\n",
    "    \n",
    "    # valid missing data \n",
    "    if len(na_values) > 0:\n",
    "        no_data = (series.isin(na_values))\n",
    "        logging.debug(f'excluding {no_data.sum()} samples with missing values ({na_values}) '\n",
    "                      f'in column {series.name}')\n",
    "        return series[~no_data]\n",
    "    return series\n",
    "    \n",
    "# exclude_missing(df['TIME_OF_COLLECTION'], na_values=['NOT_COLLECTED',''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_contributors(fn, contrib_sheet='TAB 1 Contributors'):\n",
    "    \n",
    "    logging.debug(f'validating contributors in {fn}')\n",
    "        \n",
    "    df = pd.read_excel(fn, dtype=str, keep_default_na=False,\n",
    "                       sheet_name=contrib_sheet)\n",
    "    \n",
    "    df = remove_trailing_spaces(df)\n",
    "    \n",
    "    expected_columns = ['SURNAME','FIRST_NAME','PRIMARY_AFFILIATION','EMAIL ADDRESS','CONTRIBUTION','CONFIRMATION']\n",
    "    \n",
    "    if set(df.columns) != set(expected_columns):\n",
    "        ec = set(df.columns) - set(expected_columns)\n",
    "        mc = set(expected_columns) - set(df.columns)\n",
    "        logging.error(f'mismatch in contributor columns: extra {ec}, missing {mc}')\n",
    "    \n",
    "    for delim_char in (';','|'):\n",
    "        for col in df.columns:\n",
    "            if df[col].str.contains(delim_char, regex=False).any():\n",
    "                logging.error(f'contributor column {col} contains delimiter \"{delim_char}\"')\n",
    "    \n",
    "    df['FULL_NAME'] = df['FIRST_NAME'] + ' ' + df['SURNAME']\n",
    "    df['PARTNER_CODE'] = (df['SURNAME'].str.slice(0,2) + df['FIRST_NAME'].str.slice(0,2)).str.upper()\n",
    "    is_template_name = (df['FULL_NAME'] == 'Charles R. Darwin')\n",
    "    if is_template_name.any():\n",
    "        logging.warning('suspect template contributor was not removed')\n",
    "    \n",
    "    is_dup_name = df['FULL_NAME'].duplicated()\n",
    "    if is_dup_name.any():\n",
    "        logging.error('duplicated contributor names {}'.format(\n",
    "                df.loc[is_dup_name, 'FULL_NAME'].to_list()))\n",
    "    \n",
    "    # TODO update template with underscore in email column\n",
    "    is_valid_email = df['EMAIL ADDRESS'].str.match('^[A-z0-9._%+-]+@[A-z0-9.-]+\\.[A-z]{2,}$')\n",
    "    if not is_valid_email.all():\n",
    "        logging.error('invalid contributor email addresses {}'.format(\n",
    "                df.loc[~is_valid_email, 'EMAIL ADDRESS'].to_list()))\n",
    "    \n",
    "    is_confirmed = (df['CONFIRMATION'] == 'YES')\n",
    "    if not is_confirmed.any():\n",
    "        logging.error('confirmation lacking for any of contributors')\n",
    "        \n",
    "    return df\n",
    "        \n",
    "# contrib_df = validate_contributors(anospp_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_regex(col, df, na_values=[]):\n",
    "    \n",
    "    logging.debug(f'validating data format in {col} column')\n",
    "    \n",
    "    if col not in df.columns:\n",
    "        logging.error(f'{col} column not found in manifest')\n",
    "        return\n",
    "    series = df[col]\n",
    "    series = exclude_missing(series, na_values)\n",
    "    \n",
    "    # dates between 1900 and 2100\n",
    "    date_regex = (r'^(19\\d\\d|20\\d\\d)-(0[1-9]|1[0-2])-(0[1-9]|[12]\\d|3[01])$|'\n",
    "                  r'^(19\\d\\d|20\\d\\d)-(0[1-9]|1[0-2])$|'\n",
    "                  r'^(19\\d\\d|20\\d\\d)$')\n",
    "    \n",
    "    numeric_regex = r'^[-+]?\\d*\\.?\\d+$'\n",
    "    \n",
    "    regexs = {\n",
    "        'CATCH_LOT': (r'^C\\d{3}[A-Z]$', \n",
    "                      'like C123A'),\n",
    "        'DATE_OF_COLLECTION': (date_regex, 'in YYYY-MM-DD format'),\n",
    "        'DECIMAL_LATITUDE': (r'^[-+]?([0-8]\\d|\\d)(\\.\\d+)?$', \n",
    "                             'between -90 and 90'),\n",
    "        'DECIMAL_LONGITUDE': (r'^[-+]?(1[0-7]\\d|\\d\\d|\\d)(\\.\\d+)?$', \n",
    "                              'between -180 and 180'),\n",
    "        'WHAT_3_WORDS': (r'^///[a-z]+\\.[a-z]+\\.[a-z]+$', \n",
    "                         'like ///one.two.three'),\n",
    "        'TIME_OF_COLLECTION': (r'^(?:[01]\\d|2[0-3]):[0-5]\\d$|^(?:[01]\\d|2[0-3]):[0-5]\\d:[0-5]\\d$', \n",
    "                              'in HH:MM format'),\n",
    "        'DURATION_OF_COLLECTION': (r'^P(?:\\d+Y)?(?:\\d+M)?(?:\\d+D)?(?:T(?:\\d+H)?(?:\\d+M)?(?:\\d+S)?)?$', \n",
    "                                   'in P[n]Y[n]M[n]DT[n]H[n]M[n]S format'),\n",
    "        'DATE_OF_PLATING': (date_regex, 'in YYYY-MM-DD format'),\n",
    "        'DATE_OF_PRESERVATION': (date_regex, 'in YYYY-MM-DD format'),\n",
    "        'ELEVATION': (numeric_regex, 'only a number (in metres)'),\n",
    "        'DNA_EXTRACT_VOLUME_PROVIDED': (numeric_regex, 'a number (in microlitres)'),\n",
    "        'DNA_EXTRACT_CONCENTRATION': (numeric_regex, 'a number (in nanograms per microlitre)')\n",
    "    }\n",
    "    \n",
    "    is_valid_regex = series.str.match(regexs[col][0])\n",
    "    if not is_valid_regex.all():\n",
    "        offending_values = list(series[~is_valid_regex].unique())\n",
    "        s = index_ranges(series[~is_valid_regex])\n",
    "        msg = (f'{col} format incorrect for SERIES {s}: found {offending_values} - '\n",
    "              f'expected to be {regexs[col][1]}')\n",
    "        if col == 'CATCH_LOT':\n",
    "            logging.warning(msg)\n",
    "        else:\n",
    "            logging.error(msg)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_plates_wells(df, contrib_df, plate_col='RACK_OR_PLATE_ID', well_col='TUBE_OR_WELL_ID', bioscan=False):\n",
    "    \n",
    "    bioscan_partners_fn = '../data/bioscan_partners.tsv'\n",
    "    \n",
    "    # expect only complete 96-well plates\n",
    "    logging.debug(f'validating {plate_col} and {well_col} columns')\n",
    "    \n",
    "    empty_rows = (df[plate_col] == '') | (df[well_col] == '')\n",
    "    if empty_rows.any():\n",
    "        logging.info(f'found and excluded {empty_rows.sum()} empty rows based on {plate_col} and {well_col}')\n",
    "        df = df.loc[~empty_rows]\n",
    "        \n",
    "    # plate names validation\n",
    "    plates = df[plate_col].drop_duplicates()\n",
    "    plate_name_template = ('^[A-Z]{4}-[0-9]{3}$|^[A-Z]{4}_[0-9]{3}$' if bioscan else '^[A-Z]{4}_[0-9]{3}$')\n",
    "    wrong_plate_names = plates[~plates.str.match(plate_name_template, na='')]\n",
    "    if len(wrong_plate_names) > 0:\n",
    "        logging.error(f'plate names {wrong_plate_names.to_list()} do not match template ABCD_1234')\n",
    "    # check plate name prefixes against partner codes\n",
    "    # and assign GAL\n",
    "    plate_prefixes = plates.str.slice(0,4)\n",
    "    if bioscan:\n",
    "        partners_df = pd.read_csv(bioscan_partners_fn, sep='\\t', dtype=str)\n",
    "        assert partners_df['partner_code'].is_unique, f'duplicated partner codes found in {bioscan_partners_fn}'\n",
    "        unknown_prefixes = (~plate_prefixes.isin(partners_df['partner_code']))\n",
    "        \n",
    "        possible_partner_codes = plate_prefixes.value_counts().index.to_list()\n",
    "        if len(possible_partner_codes) > 1:\n",
    "            logging.error(f'only one plate ID prefix expected, found multiple: {possible_partner_codes}')\n",
    "        selected_partner_code = possible_partner_codes[0]\n",
    "        selected_partner_df = partners_df.query(f'partner_code == \"{selected_partner_code}\"')\n",
    "        if selected_partner_df.shape[0] == 0:\n",
    "            logging.error(f'partner code {selected_partner_code} not found in {bioscan_partners_fn}, '\n",
    "                          f'using \"Sanger Institute\" as default partner')\n",
    "            gal = \"Sanger Institute\"\n",
    "        else:\n",
    "            gal = selected_partner_df['gal'].iloc[0]\n",
    "        logging.info(f'selected GAL \"{gal}\" for partner code \"{selected_partner_code}\"')\n",
    "    else:\n",
    "        # anospp\n",
    "        unknown_prefixes = (~plate_prefixes.isin(contrib_df['PARTNER_CODE']))\n",
    "        gal = \"Sanger Institute\"\n",
    "    if unknown_prefixes.any():\n",
    "        logging.error(f'plate ID prefixes not recognised for {plates[unknown_prefixes].to_list()}')\n",
    "        \n",
    "    # add 96-well plate well IDs to validation\n",
    "    row_id = list('ABCDEFGH')\n",
    "    col_id = range(1,13)\n",
    "    expected_wells = [r + str(c) for (r,c) in itertools.product(row_id, col_id)]\n",
    "        \n",
    "    for plate, pdf in df.groupby(plate_col):\n",
    "        \n",
    "        if bioscan and (pdf[well_col] == 'PLATE_ONLY').any():\n",
    "            logging.debug(f'skipping well validation for PLATE_ONLY plate {plate}')\n",
    "            continue\n",
    "        \n",
    "        # check for well duplicates\n",
    "        dup_wells =  pdf[well_col].duplicated()\n",
    "        if dup_wells.any():\n",
    "            logging.error(f'duplicate {well_col} for plate {plate}: {pdf.loc[dup_wells, well_col].unique()}')\n",
    "        # check for non A1...H12 wells\n",
    "        observed_wells_set = set(pdf[well_col])\n",
    "        expected_wells_set = set(expected_wells)\n",
    "        if observed_wells_set != expected_wells_set:\n",
    "            msg = f'in {well_col} for plate {plate}, '\n",
    "            if len(expected_wells_set - observed_wells_set) > 0:\n",
    "                msg += f'wells {expected_wells_set - observed_wells_set} are missing'\n",
    "            if len(observed_wells_set - expected_wells_set) > 0:\n",
    "                if msg.endswith('missing'):\n",
    "                    msg += ', '\n",
    "                msg += f'wells {observed_wells_set - expected_wells_set} are excessive'\n",
    "            logging.error(msg)\n",
    "    \n",
    "    logging.info(f'{df.shape[0]} samples found across {df[plate_col].nunique()} plates')\n",
    "    \n",
    "\n",
    "    return df, gal\n",
    "        \n",
    "# df = validate_plates_wells(df, contrib_df, 'RACK_OR_PLATE_ID', 'TUBE_OR_WELL_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_plate_only(df, plate_col='RACK_OR_PLATE_ID', well_col='TUBE_OR_WELL_ID'):\n",
    "    \n",
    "    # add 96-well plate well IDs for expansion\n",
    "    row_id = list('ABCDEFGH')\n",
    "    col_id = range(1,13)\n",
    "    expected_wells = [r + str(c) for (c,r) in itertools.product(col_id, row_id)]\n",
    "    \n",
    "    blank_wells = {\n",
    "        'PLATE_ONLY':['H12'], # v2\n",
    "        'PLATE_ONLY_1_BLANK':['H12'], # v3\n",
    "        'PLATE_ONLY_2_BLANKS':['G12','H12'], # v3\n",
    "    }\n",
    "    \n",
    "    if df[well_col].isin(blank_wells.keys()).any():\n",
    "        pdfs = []\n",
    "\n",
    "        for plate, pdf in df.groupby(plate_col):\n",
    "            # If plate level only metadata is being entered\n",
    "            # put “PLATE_ONLY” in well \n",
    "            # and use only one row to capture the metadata for the whole plate\n",
    "            if pdf[well_col].isin(blank_wells.keys()).any():\n",
    "                \n",
    "                if pdf.shape[0] > 1:\n",
    "                    logging.error(f'expected one row in PLATE_ONLY plate {plate}, found {pdf.shape[0]} - '\n",
    "                                  f'extracting metadata from first row only')\n",
    "                plate_only_val = pdf[well_col].iloc[0]\n",
    "                logging.info(f'found {plate_only_val} plate {plate}, expanding to 96 well rows, '\n",
    "                             f'keeping {\", \".join(blank_wells[plate_only_val])} blank')\n",
    "                # expand to 96 rows\n",
    "                pdf = pd.DataFrame(pdf.iloc[0] for i in range(len(expected_wells)))\n",
    "                pdf[well_col] = expected_wells\n",
    "                \n",
    "                for blank_well in blank_wells[plate_only_val]:\n",
    "                    for col in pdf.columns:\n",
    "                        if col == 'ORGANISM_PART':\n",
    "                            pdf.loc[pdf[well_col] == blank_well, col] = 'NOT_APPLICABLE'\n",
    "                        elif col not in ['SERIES','CATCH_LOT','RACK_OR_PLATE_ID',\n",
    "                                         'TUBE_OR_WELL_ID','ORGANISM_PART','PRESERVATIVE_SOLUTION']:\n",
    "                            pdf.loc[pdf[well_col] == blank_well, col] = ''\n",
    "            pdfs.append(pdf)\n",
    "            \n",
    "        df = pd.concat(pdfs).reset_index(drop=True)\n",
    "        df.index += 1\n",
    "        df.index.name = 'SERIES'\n",
    "        \n",
    "    else:\n",
    "        logging.debug('no PLATE_ONLY rows found')\n",
    "    \n",
    "    return df\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_blanks(df):\n",
    "    \n",
    "    logging.debug('checking and excluding blank samples based on ORGANISM_PART being NOT_APPLICABLE')    \n",
    "    \n",
    "    # blank criterion\n",
    "    is_blank = (df['ORGANISM_PART'] == 'NOT_APPLICABLE')\n",
    "    \n",
    "    blank_df = df[is_blank]\n",
    "    \n",
    "    non_blank_df = df[~is_blank]\n",
    "    \n",
    "    # last well of plate expected to be blank\n",
    "    non_blank_last_well_df = non_blank_df.query('TUBE_OR_WELL_ID == \"H12\"')\n",
    "    if non_blank_last_well_df.shape[0] > 0:\n",
    "        logging.error(f'last well H12 is not blank at SERIES {index_ranges(non_blank_last_well_df)}: '\n",
    "                      f'in ORGANISM_PART, expected \"NOT_APPLICABLE\", '\n",
    "                      f'found {non_blank_last_well_df.ORGANISM_PART.to_list()}, '\n",
    "                      f'these samples will be included in further analysis',\n",
    "                        \n",
    "        )\n",
    "        \n",
    "    for col in df.columns:\n",
    "        if col not in ['SERIES','CATCH_LOT','RACK_OR_PLATE_ID',\n",
    "                       'TUBE_OR_WELL_ID','ORGANISM_PART','PRESERVATIVE_SOLUTION',\n",
    "                       'OTHER_INFORMATION','MISC_METADATA']:\n",
    "            excessive_blank_info_df = blank_df[blank_df[col] != '']\n",
    "            if excessive_blank_info_df.shape[0] > 0:\n",
    "                logging.error(f'{col} column values non-empty for blank samples '\n",
    "                              f'at SERIES {index_ranges(excessive_blank_info_df)} - these should be deleted')\n",
    "            \n",
    "    \n",
    "    logging.info(f'{is_blank.sum()} blanks located across {df.RACK_OR_PLATE_ID.nunique()} plates, '\n",
    "                 f'{non_blank_df.shape[0]} samples of {df.shape[0]} left for downstream analysis')\n",
    "    \n",
    "    \n",
    "    return is_blank\n",
    "\n",
    "# print(df.shape)\n",
    "# is_blank = check_blanks(df)\n",
    "# print(df[~is_blank].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_values(col, df, valid_dict, sep=None, na_values=[], level='e'):\n",
    "    \n",
    "    logging.debug(f'validating for allowed values in {col} column')\n",
    "    \n",
    "    if col not in df.columns:\n",
    "        logging.error(f'{col} column not found in manifest')\n",
    "        return\n",
    "    if col not in valid_dict.keys():\n",
    "        logging.error(f'{col} column not found in validation sheet')\n",
    "        return\n",
    "    assert level in ('i','w','e'), '{!r} invalid logging level for validate_values'.format(level)\n",
    "    \n",
    "    series = df[col]\n",
    "    series = exclude_missing(series, na_values)\n",
    "    \n",
    "    col_values = set(series.unique())\n",
    "    # use separator to split values\n",
    "    if sep:\n",
    "        sep_col_values = list()\n",
    "        for v in col_values:\n",
    "            sep_col_values.extend([x.strip() for x in v.split(sep)])\n",
    "        col_values = set(sep_col_values)\n",
    "    \n",
    "    valid_values = set(valid_dict[col])\n",
    "    \n",
    "    invalid_values = col_values - valid_values\n",
    "    \n",
    "    if len(invalid_values) > 0:\n",
    "        if sep:\n",
    "            invalid_value_series = index_ranges(series[series.str.contains('|'.join(invalid_values), regex=True)])\n",
    "        else:\n",
    "            invalid_value_series = index_ranges(series[series.isin(invalid_values)])\n",
    "        msg = f'invalid values in {col} column, SERIES {invalid_value_series}: {invalid_values}'\n",
    "        if level == 'i':\n",
    "            logging.info(msg)\n",
    "        elif level == 'w':\n",
    "            logging.warning(msg)\n",
    "        elif level == 'e':\n",
    "            logging.error(msg)\n",
    "#     else:\n",
    "#         logging.info('all values valid in {!r}'.format(col))\n",
    "            \n",
    "# validate_values('ORGANISM_PART', df, valid_dict, sep=\" | \", na_values=[], level='e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_dates_text(before_col, after_col, df):\n",
    "    \n",
    "    logging.debug(f'checking that {before_col} is earlier than {after_col}')\n",
    "    \n",
    "    for col in (before_col, after_col):\n",
    "        if col not in df.columns:\n",
    "            logging.error(f'{col} column not found in manifest')\n",
    "            return\n",
    "    # invalid date formats or empty string converted to NaT\n",
    "    before_series = pd.to_datetime(df[before_col], format='%Y-%m-%d', errors='coerce').copy()\n",
    "    after_series = pd.to_datetime(df[after_col], format='%Y-%m-%d', errors='coerce').copy()\n",
    "    \n",
    "    date_conflict = before_series > after_series\n",
    "    \n",
    "    s = index_ranges(df[date_conflict])\n",
    "    \n",
    "    if date_conflict.any():\n",
    "        logging.error(f'{before_col} column values are later than {after_col} column values in SERIES {s}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_country_and_coordinates(df, fn, na_values=[], bioscan=False):\n",
    "    \n",
    "    logging.debug('validating COUNTRY_OF_COLLECTION against DECIMAL_LATITUDE and DECIMAL_LONGITUDE')\n",
    "    \n",
    "    country_col, lat_col, lon_col = 'COUNTRY_OF_COLLECTION', 'DECIMAL_LATITUDE', 'DECIMAL_LONGITUDE'\n",
    "\n",
    "    try:\n",
    "        loc_df_complete = df[[country_col, lat_col, lon_col]].copy()\n",
    "    except:\n",
    "        logging.error(f'some of {country_col} {lat_col} {lon_col} columns not found in manifest')\n",
    "        return\n",
    "    loc_df_isna = (loc_df_complete.isin(na_values)).all(axis=1)\n",
    "    if loc_df_isna.any():\n",
    "        logging.info(f'removing {loc_df_isna.sum()} missing data ({na_values}) samples from coordinate analysis')\n",
    "    loc_df_complete = loc_df_complete[~loc_df_isna].copy()\n",
    "    \n",
    "    # coordinates in geopy format\n",
    "    loc_df_complete['coord'] = loc_df_complete.apply(lambda x: '{}, {}'.format(\n",
    "            x[lat_col], x[lon_col]), axis=1)\n",
    "    \n",
    "    # get location data for coordinates\n",
    "    # use local copy of web query results for re-runs\n",
    "    # this \n",
    "    loc_fn = fn+'_loc.pkl'\n",
    "    if os.path.isfile(loc_fn):\n",
    "        locations = pickle.load(open(loc_fn, \"rb\"))\n",
    "    else:\n",
    "        # web map server - openstreetmaps\n",
    "        logging.debug('querying coordinates')\n",
    "        locator = Nominatim(user_agent='myGeocoder')\n",
    "        rgeocode = RateLimiter(locator.reverse, min_delay_seconds=1)\n",
    "\n",
    "        locations = dict()\n",
    "        for c in loc_df_complete.coord.unique():\n",
    "            # pre-fill with unknown country\n",
    "            locations[c] = {'address':{'country':'UNKNOWN'}}\n",
    "            # check coordniate correctness\n",
    "            try:\n",
    "                lat, lon = c.split(', ')\n",
    "                lat, lon = float(lat), float(lon)\n",
    "            except:\n",
    "                unparsed_df = df[(df[lat_col] == str(lat)) & df[lon_col] == str(lon)]\n",
    "                logging.error(\n",
    "                    f'problem parsing coordinates {c} at SERIES {index_ranges(unparsed_df)}'\n",
    "                )\n",
    "                continue\n",
    "            if abs(lat) > 90:\n",
    "                logging.error(\n",
    "                    f'invalid DECIMAL_LATITUDE {lat} at SERIES {index_ranges(df[df[lat_col] == str(lat)])}'\n",
    "                    f', should be in [-90,90]')\n",
    "                continue\n",
    "            if abs(lon) > 180:\n",
    "                logging.error(\n",
    "                    f'invalid DECIMAL_LONGITUDE {lon} at SERIES {index_ranges(df[df[lon_col] == str(lon)])}'\n",
    "                    f', should be in [-180,180]')\n",
    "                continue\n",
    "            # web query\n",
    "            location = rgeocode(c, language='en-gb')\n",
    "            # rgeocode returns empty location outside of counries and in some other situations\n",
    "            if location is not None:\n",
    "                locations[c] = location.raw\n",
    "\n",
    "        # save locations to file\n",
    "        pickle.dump(locations, open(loc_fn, \"wb\"))\n",
    "        \n",
    "    # parse country from partner input\n",
    "    loc_df_complete['partner_country'] = loc_df_complete[country_col].str.strip().str.upper()\n",
    "    \n",
    "    # extract countries from location data\n",
    "    loc_countries = dict()\n",
    "    for coord in locations.keys():\n",
    "        \n",
    "        lat, lon = coord.split(', ')\n",
    "        coord_series = index_ranges(df.query(f'({lat_col} == \"{lat}\") & ({lon_col} == \"{lon}\")'))\n",
    "                    \n",
    "        coord_country = locations[coord]['address']['country'].upper()\n",
    "        loc_countries[coord] = coord_country\n",
    "        \n",
    "        partner_countries = loc_df_complete.loc[loc_df_complete.coord == coord, 'partner_country']\n",
    "        if partner_countries.nunique() > 1:\n",
    "            logging.error(\n",
    "                f'multiple COUNTRY_OF_COLLECTION records found for coordinates {coord}, SERIES {coord_series}: '\n",
    "                f'{partner_countries.unique()}, skipping coordinate validation'\n",
    "            )\n",
    "            continue\n",
    "        if partner_countries.shape[0] == 0:\n",
    "            logging.error(f'no COUNTRY_OF_COLLECTION records found for coordinates {coord}, SERIES {coord_series}')\n",
    "            continue\n",
    "        partner_country = partner_countries.iloc[0]\n",
    "        if coord_country == 'UNKNOWN':\n",
    "            logging.warning(f'could not locate country for coordinates {coord}, '\n",
    "                            f'COUNTRY_OF_COLLECTION {partner_country}, SERIES {coord_series}')\n",
    "        elif partner_country != coord_country:\n",
    "            logging.error(f'country mismatch for coordinates {coord}, SERIES {coord_series}: '\n",
    "                          f'COUNTRY_OF_COLLECTION indicated as {partner_country}, '\n",
    "                          f'while coordinates point to {coord_country}')\n",
    "    \n",
    "    # countries based on coordinates\n",
    "    loc_df_complete['coord_country'] = loc_df_complete['coord'].replace(loc_countries)\n",
    "    country_mismatch = (loc_df_complete.coord_country != loc_df_complete.partner_country)\n",
    "\n",
    "#     if country_mismatch.any():\n",
    "#         logging.error('coordinates do not match country for SERIES: {}'.format(\n",
    "#                 country_mismatch[country_mismatch].index.to_list()))\n",
    "    \n",
    "    # location data can be re-used, e.g. as an additional field\n",
    "    return loc_df_complete\n",
    "# df.loc[2,'DECIMAL_LATITUDE'] = '65'\n",
    "# loc_test = validate_country_and_coordinates(df, anospp_fn)\n",
    "# loc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_taxonomy(df, ncbi, na_values = [], anospp=False, add_taxids=False):\n",
    "\n",
    "    logging.debug('validating taxonomy against NCBI')\n",
    "    \n",
    "    if anospp:\n",
    "        df['PREDICTED_ORDER_OR_GROUP'] = 'Diptera'\n",
    "        df['PREDICTED_FAMILY'] = 'Culicidae'\n",
    "        df['PREDICTED_GENUS'] = 'Anopheles'\n",
    "        \n",
    "        harbach_spp = []\n",
    "        with open('../data/harbach_spp_201910.txt') as f:\n",
    "            for line in f:\n",
    "                harbach_spp.append('Anopheles ' + line.strip())\n",
    "        harbach_spp = set(harbach_spp)\n",
    "        \n",
    "    tax_levels = {\n",
    "        'PREDICTED_ORDER_OR_GROUP':'order',\n",
    "        'PREDICTED_FAMILY':'family',\n",
    "        'PREDICTED_GENUS':'genus',\n",
    "        'PREDICTED_SCIENTIFIC_NAME':'species'\n",
    "    }\n",
    "    \n",
    "    expected_ranks = {\n",
    "            'order':('class','subclass','order'),\n",
    "            'family':('suborder','superfamily','family','subfamily','tribe','subtribe'),\n",
    "            'genus':('genus','subgenus'),\n",
    "            'species':('species')\n",
    "        }\n",
    "    \n",
    "    hierarchies = df[tax_levels.keys()].drop_duplicates().copy()\n",
    "    \n",
    "    hierarchies.columns = list(tax_levels.values())\n",
    "        \n",
    "    tax_info = dict()\n",
    "    \n",
    "    for tax_col, tax_level in tax_levels.items():\n",
    "        \n",
    "        logging.debug(f'validating {tax_col} column against NCBI')\n",
    "        \n",
    "        if tax_col not in df.columns:\n",
    "                logging.error(f'{tax_col} column not found in manifest')\n",
    "                continue\n",
    "            \n",
    "        tax_names = list(hierarchies[tax_level].unique())\n",
    "        \n",
    "        for na_value in na_values:\n",
    "            try:\n",
    "                tax_names.remove(na_value)\n",
    "            except:\n",
    "                pass \n",
    "            \n",
    "        for i, tax_name in enumerate(tax_names):\n",
    "            if len(tax_name) == 0:\n",
    "                continue\n",
    "            corr_tax_name = tax_name[0].upper() + tax_name[1:].lower()\n",
    "            if corr_tax_name != tax_name and tax_name != 'blank sample':\n",
    "                s = index_ranges(df.query(f'{tax_col} == \"{tax_name}\"'))\n",
    "                logging.info(f'{tax_col} column, SERIES {s}'\n",
    "                             f': unexpected case for \"{tax_name}\", '\n",
    "                             f'changing to \"{corr_tax_name}\" for validation and output')\n",
    "            tax_names[i] = corr_tax_name\n",
    "        \n",
    "        tax_info[tax_level] = ncbi.get_name_translator(tax_names) \n",
    "        \n",
    "        unmatched_names = set(tax_names) - set(tax_info[tax_level].keys())\n",
    "        if len(unmatched_names) > 0:\n",
    "            for tname in unmatched_names:\n",
    "                s = index_ranges(df[df[tax_col].str.match(f'^{re.escape(tname)}$', case=False)])\n",
    "                if tax_level == 'species' and anospp:\n",
    "                    if tname in harbach_spp:\n",
    "                        logging.warning(f'{tax_col} column, SERIES {s}:'\n",
    "                                        f' \"{tname}\" found in Harbach list, but not in NCBI Taxonomy')\n",
    "                    else:\n",
    "                        logging.error(f'{tax_col} column, SERIES {s}'\n",
    "                                      f': \"{tname}\" not found in both Harbach list and NCBI Taxonomy')\n",
    "                else:\n",
    "                    logging.error(f'{tax_col} column, SERIES {s}: \"{tname}\" not found in NCBI Taxonomy')\n",
    "        \n",
    "        \n",
    "        \n",
    "        for tname, tids in tax_info[tax_level].items():\n",
    "            \n",
    "            ranks = ncbi.get_rank(tids)\n",
    "            \n",
    "            upd_tid = tids[0]\n",
    "            \n",
    "            s = index_ranges(df[df[tax_col].str.match(f'^{re.escape(tname)}$', case=False)])\n",
    "                        \n",
    "            if len(tids) == 1:\n",
    "                if ranks[upd_tid] not in expected_ranks[tax_level]: \n",
    "                    logging.error(f'{tax_col} column, SERIES {s}: found unexpected rank for \"{tname}\" '\n",
    "                                      f'(taxid {upd_tid}): \"{ranks[upd_tid]}\"')\n",
    "                    \n",
    "            if len(tids) > 1:            \n",
    "                for tid, rank in ranks.items():\n",
    "                    if rank in expected_ranks[tax_level] and len(tids) > 1:\n",
    "                        logging.debug(f'{tax_col} column, SERIES {s}: using only first matching rank '\n",
    "                                      f'for \"{tname}\" (taxid {tid}): \"{rank}\"')\n",
    "                        upd_tid = tid\n",
    "                        break\n",
    "                else:\n",
    "                    logging.error(f'{tax_col} column, SERIES {s}: could not find matching rank '\n",
    "                                  f'for \"{tname}\" - using taxid {upd_tid}: \"{ranks[upd_tid]}\"')\n",
    "                    \n",
    "            tax_info[tax_level][tname] = upd_tid\n",
    "        \n",
    "        #logging.info(f'{tax_level} {tax_info[tax_level]}')\n",
    "                    \n",
    "    # check consistency of taxonomy\n",
    "    for _, r in hierarchies.iterrows():\n",
    "        \n",
    "        if r.order in na_values:\n",
    "            continue\n",
    "        try:\n",
    "            order_id = tax_info['order'][r.order]\n",
    "        except KeyError:\n",
    "            logging.debug(f'PREDICTED_ORDER_OR_GROUP value \"{r.order}\" not in NCBI Taxonomy, '\n",
    "                          f'skipping taxonomy consistency check')\n",
    "            continue\n",
    "            \n",
    "        if r.genus in na_values:\n",
    "            continue\n",
    "        try:\n",
    "            family_id = tax_info['family'][r.family]\n",
    "            \n",
    "            family_lineage = ncbi.get_lineage(family_id)\n",
    "            \n",
    "            s = index_ranges(df[df['PREDICTED_FAMILY'].str.match(f'^{re.escape(r.family)}$', case=False)])\n",
    "            \n",
    "            if order_id not in family_lineage:\n",
    "                logging.error(f'PREDICTED_FAMILY column, SERIES {s}: family \"{r.family}\" (taxid {family_id}) '\n",
    "                              f'does not belong to order \"{r.order}\" (taxid {order_id})')\n",
    "        except KeyError:\n",
    "            logging.debug(f'PREDICTED_FAMILY value \"{r.family}\" not in NCBI Taxonomy, '\n",
    "                          f'skipping taxonomy consistency check')\n",
    "            continue\n",
    "            \n",
    "        if r.genus in na_values:\n",
    "            continue\n",
    "        try:\n",
    "            genus_id = tax_info['genus'][r.genus]\n",
    "            \n",
    "            genus_lineage = ncbi.get_lineage(genus_id)\n",
    "            \n",
    "            s = index_ranges(df[df['PREDICTED_GENUS'].str.match(f'^{re.escape(r.genus)}$', case=False)])\n",
    "            \n",
    "            if order_id not in genus_lineage:\n",
    "                logging.error(\n",
    "                    f'PREDICTED_GENUS column, SERIES {s}: '\n",
    "                    f'genus \"{r.genus}\" (taxid {genus_id}) does not belong to \"{r.order}\" (taxid {order_id})')\n",
    "            if family_id not in genus_lineage:\n",
    "                logging.error(\n",
    "                    f'PREDICTED_GENUS column, SERIES {s}: '\n",
    "                    f'genus \"{r.genus}\" (taxid {genus_id}) does not belong to \"{r.family}\" (taxid {family_id})')\n",
    "        except KeyError:\n",
    "            logging.debug(f'PREDICTED_GENUS value \"{r.genus}\" not in NCBI Taxonomy, '\n",
    "                          f'skipping taxonomy consistency check')\n",
    "            continue\n",
    "            \n",
    "        if r.species in na_values:\n",
    "            continue\n",
    "        try:\n",
    "            species_id = tax_info['species'][r.species]\n",
    "            \n",
    "            species_lineage = ncbi.get_lineage(species_id)\n",
    "            \n",
    "            s = index_ranges(df[df['PREDICTED_SCIENTIFIC_NAME'].str.match(f'^{re.escape(r.species)}$', case=False)])\n",
    "            \n",
    "            if order_id not in species_lineage:\n",
    "                logging.error(\n",
    "                    f'PREDICTED_SCIENTIFIC_NAME column, SERIES {s}: '\n",
    "                    f'species \"{r.species}\" (taxid {species_id}) does not belong to \"{r.order}\" (taxid {order_id})')\n",
    "            if family_id not in species_lineage:\n",
    "                logging.error(\n",
    "                    f'PREDICTED_SCIENTIFIC_NAME column, SERIES {s}: '\n",
    "                    f'species \"{r.species}\" (taxid {species_id}) does not belong to \"{r.family}\" (taxid {family_id})')\n",
    "            if genus_id not in species_lineage:\n",
    "                logging.error(\n",
    "                    f'PREDICTED_SCIENTIFIC_NAME column, SERIES {s}: '\n",
    "                    f'species \"{r.species}\" (taxid {species_id}) does not belong to \"{r.genus}\" (taxid {genus_id})')\n",
    "            elif r.species.split(' ')[0] != r.genus:\n",
    "                logging.error(\n",
    "                    f'PREDICTED_SCIENTIFIC_NAME column, SERIES {s}: '\n",
    "                    f'species \"{r.species}\" (taxid {species_id}) states a different genus name than '\n",
    "                    f'\"{r.genus}\" (taxid {genus_id})')\n",
    "        except KeyError:\n",
    "            logging.info(f'PREDICTED_SCIENTIFIC_NAME value \"{r.species}\" not in NCBI Taxonomy, '\n",
    "                         f'skipping taxonomy consistency check')\n",
    "            continue\n",
    "    \n",
    "    if add_taxids:\n",
    "        for tc in tax_levels.keys():\n",
    "            df[f'{tc}_TAXID'] = df[tc].replace(tax_info[tax_levels[tc]])\n",
    "    \n",
    "    # only fix taxonomy case after validation to get naming \n",
    "    for tax_col in tax_levels.keys():\n",
    "        df[tax_col] = df[tax_col].str.capitalize()\n",
    "            \n",
    "    return df\n",
    "        \n",
    "                \n",
    "# validate_taxonomy(df, ncbi, anospp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_specimen_id_risk(df):\n",
    "    \n",
    "    logging.debug(f'validating SPECIMEN_IDENTITY_RISK column')\n",
    "    \n",
    "    if 'SPECIMEN_IDENTITY_RISK' not in df.columns:\n",
    "        logging.error(f'SPECIMEN_IDENTITY_RISK column not found in manifest')\n",
    "        return\n",
    "    \n",
    "    # missing species name, but no idenitity risk\n",
    "    invalid_risk = ((df.PREDICTED_SCIENTIFIC_NAME == '') & (df.SPECIMEN_IDENTITY_RISK == 'N'))\n",
    "    \n",
    "    if invalid_risk.any():\n",
    "        logging.error(f'SERIES {index_ranges(df.loc[invalid_risk])}: with no PREDICTED_SCIENTIFIC_NAME, '\n",
    "                      f'SPECIMEN_IDENTITY_RISK values should be blank or \"Y\", not \"N\"')\n",
    "\n",
    "# validate_specimen_id_risk(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_freetext(col, df, na_values=['']):\n",
    "    \n",
    "    logging.debug(f'validating freetext field characters in {col} column')\n",
    "    \n",
    "    if col not in df.columns:\n",
    "        logging.error(f'{col} column not found in manifest')\n",
    "        return\n",
    "    series = df[col]\n",
    "    series = exclude_missing(series, na_values)\n",
    "    \n",
    "    regex = '^[A-z0-9.,\\-_ ]+$'\n",
    "    \n",
    "    is_valid_freetext = series.str.match(regex)\n",
    "    if not is_valid_freetext.all():\n",
    "        s = index_ranges(series.loc[~is_valid_freetext])\n",
    "        logging.debug(f'{col} column, SERIES {s}: found non-standard characters - regex: \"{regex}\"')\n",
    "\n",
    "        \n",
    "# validate_freetext('IDENTIFIED_HOW', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sts_cols(df, contrib_df, gal, bioscan=True, v='v2'):\n",
    "    \n",
    "    logging.debug('adding STS columns to manifest')\n",
    "    \n",
    "    is_blank = (df['ORGANISM_PART'] == 'NOT_APPLICABLE')\n",
    "    \n",
    "    df['SPECIMEN_ID'] = df['RACK_OR_PLATE_ID'] + '_' + df['TUBE_OR_WELL_ID']\n",
    "    dup_specimen_id = df['SPECIMEN_ID'].duplicated()\n",
    "    if dup_specimen_id.any():\n",
    "        logging.error(f'duplicate SPECIMEN_ID generated: {df.SPECIMEN_ID[dup_specimen_id].unique()}')\n",
    "    df['SCIENTIFIC_NAME'] = 'unidentified'\n",
    "    df.loc[is_blank, 'SCIENTIFIC_NAME'] = 'blank sample'\n",
    "    df['TAXON_ID'] = '32644'\n",
    "    df.loc[is_blank, 'TAXON_ID'] = '2582415'\n",
    "    df['GAL'] = gal\n",
    "    df['SYMBIONT'] = 'TARGET'\n",
    "    df['REGULATORY_COMPLIANCE'] = 'Y'\n",
    "    df['HAZARD_GROUP'] = 'HG1'\n",
    "    if bioscan and v == 'v2':\n",
    "        logging.info('auto-filling CATCH_SOLUTION and AMOUNT_OF_CATCH_PLATED columns for bioscan manifest v2')\n",
    "        df['CATCH_SOLUTION'] = '100%_ETHANOL'\n",
    "        df['AMOUNT_OF_CATCH_PLATED'] = 'ALL_SPECIMENS_PLATED'\n",
    "        logging.info('dropping IDENTIFIER_AFFILIATION column for bioscan manifest v2')\n",
    "        if (df['IDENTIFIER_AFFILIATION'] != '').any():\n",
    "            logging.warning('IDENTIFIER_AFFILIATION has some data filled in - '\n",
    "                            'note this column will be removed from output')\n",
    "        logging.info('dropping IDENTIFIER_AFFILIATION column for bioscan manifest v2')\n",
    "        df = df.drop(columns=['IDENTIFIER_AFFILIATION'])\n",
    "    # add contributors - delimiters checked in validate_contributors\n",
    "    contrib_series = contrib_df['FULL_NAME'] + ';' + \\\n",
    "        contrib_df['PRIMARY_AFFILIATION'] + ';' + \\\n",
    "        contrib_df['EMAIL ADDRESS'] + ';' + \\\n",
    "        contrib_df['CONTRIBUTION']\n",
    "    df['CONTRIBUTORS'] = '|'.join(list(contrib_series))\n",
    "    \n",
    "    logging.info('dropping MISC_METADATA column')\n",
    "    if (df['MISC_METADATA'] != '').any():\n",
    "        logging.warning('MISC_METADATA has some data filled in - note this column will be removed from output')\n",
    "    df = df.drop(columns=['MISC_METADATA'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# add_sts_cols(df, contrib_df, gal='Sanger Institute');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_sts_manifest(df, input_fn, validation_version):\n",
    "    \n",
    "    output_fn = input_fn.rstrip('.xlsx') + '_' + validation_version + '_for_sts.xlsx'\n",
    "        \n",
    "    logging.info(f'writing STS manifest to \"{output_fn}\"')\n",
    "    \n",
    "    df.to_excel(output_fn, sheet_name='Metadata Entry')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
