{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import geopy\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] test\n"
     ]
    }
   ],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)\n",
    "# logging.getLogger().setFormat('[%(levelname)s] %(message)s')\n",
    "\n",
    "\n",
    "def setup_logging(verbose=False):\n",
    "    try: \n",
    "        del logging.root.handlers[:]\n",
    "    except:\n",
    "        pass\n",
    "    if verbose:\n",
    "        logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n",
    "    else:\n",
    "        logging.basicConfig(level=logging.WARNING, format='[%(levelname)s] %(message)s')\n",
    "setup_logging(verbose=True)   \n",
    "logging.info('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = '../../results/partner_manifests/IRD-Neandersquito_T222Amplicon_Manifest_V2.0.xlsx'\n",
    "template_fn = '../../analysis/0_partner/data/T222Amplicon_Manifest_V2.0.xlsx'\n",
    "spp_fn = '../../analysis/0_partner/data/harbach_spp_201910.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] reading data from '../../results/partner_manifests/IRD-Neandersquito_T222Amplicon_Manifest_V2.0.xlsx'\n"
     ]
    }
   ],
   "source": [
    "def get_data(fn):\n",
    "\n",
    "    logging.info('reading data from {!r}'.format(fn))\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_excel(fn, dtype=str, index_col=0, keep_default_na=False,\n",
    "                           sheet_name='TAB1 Specimen Metadata Entry')\n",
    "    except:\n",
    "        df = pd.read_excel(fn, dtype=str, index_col=0, keep_default_na=False,\n",
    "                           sheet_name='Specimen Metadata Entry')\n",
    "    \n",
    "    if df.index.duplicated().any():\n",
    "        logging.error('duplicate SERIES: {}'.format(df.index[df.index.duplicated()].to_list()))\n",
    "        \n",
    "    # trailing spaces\n",
    "    for col in df.columns:\n",
    "        trailing_spaces = (df[col].str.startswith(' ') | df[col].str.endswith(' '))\n",
    "        if trailing_spaces.any():\n",
    "            logging.warning('trailing spaces found in {!r}, removing for validation'.format(col,\n",
    "                df.loc[trailing_spaces, col].to_list()))\n",
    "            df[col] = df[col].str.strip()\n",
    "        \n",
    "    return df\n",
    "df = get_data(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] reading data from '../../analysis/0_partner/data/T222Amplicon_Manifest_V2.0.xlsx'\n"
     ]
    }
   ],
   "source": [
    "template_df = get_data(template_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] checking manifest columns against template\n"
     ]
    }
   ],
   "source": [
    "def check_columns(df, template_df):\n",
    "    \n",
    "    logging.info('checking manifest columns against template')\n",
    "    \n",
    "    data_cols = set(df.columns)\n",
    "    template_cols = set(template_df.columns)\n",
    "        \n",
    "    if data_cols - template_cols != set():\n",
    "        logging.warning('extra columns in filled manifest compared to template: {}'.format(data_cols - template_cols))\n",
    "    if template_cols - data_cols != set():\n",
    "        logging.error('template columns missing from filled manifest: {}'.format(template_cols - data_cols))\n",
    "check_columns(df, template_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Checking and excluding blank samples\n",
      "[INFO] found 1 blank samples based on SCIENTIFIC_NAME\n",
      "[ERROR] for blanks, ORGANISM_PART expected to be BLANK_SAMPLE, found {'CONTROL_WELL'}\n",
      "[WARNING] for blanks, NOT_APPLICABLE expected, but not found in columns ['OTHER_ORGANISMS', 'BLOOD_MEAL', 'GRAVIDITY', 'COLLECTOR_SAMPLE_ID', 'WHAT_3_WORDS', 'ELEVATION', 'DNA_EXTRACTION_DESCRIPTION', 'DNA_EXTRACT_VOLUME_PROVIDED', 'DNA_EXTRACT_CONCENTRATION', 'HAZARD_GROUP', 'REGULATORY_COMPLIANCE', 'OTHER_INFORMATION', 'MISC_METADATA', 'COLLECTED_BY', 'COLLECTOR_AFFILIATION', 'IDENTIFIED_BY', 'IDENTIFIER_AFFILIATION', 'PRESERVED_BY', 'PRESERVER_AFFILIATION']\n",
      "[INFO] 95 samples of 96 left for downstream analysis\n"
     ]
    }
   ],
   "source": [
    "def check_exclude_blanks(df):\n",
    "    \n",
    "    logging.info('Checking and excluding blank samples')\n",
    "    \n",
    "    # last well of plate expected to be blank\n",
    "    last_well = df[df['TUBE_OR_WELL_ID'] == 'H12']\n",
    "    last_well_blanks = (last_well['SCIENTIFIC_NAME'] == 'blank sample')\n",
    "    if not last_well_blanks.all():\n",
    "        logging.error('last well H12 is not blank at SERIES {}'.format(\n",
    "                        last_well[~last_well_blanks].index.to_list()))\n",
    "    \n",
    "    is_blank = (df['SCIENTIFIC_NAME'] == 'blank sample')\n",
    "    blanks = df[is_blank]\n",
    "    \n",
    "    logging.info('found {} blank samples based on SCIENTIFIC_NAME'.format(blanks.shape[0]))\n",
    "    \n",
    "    # check organism part\n",
    "    organism_part_pass = (blanks['ORGANISM_PART'] == 'BLANK_SAMPLE')\n",
    "    if not organism_part_pass.all():\n",
    "        logging.error('for blanks, ORGANISM_PART expected to be BLANK_SAMPLE, found {}'.format(\n",
    "                set(blanks.loc[~organism_part_pass, 'ORGANISM_PART'])))\n",
    "    \n",
    "    # check that NOT_APPLICABLE is filled in all columns\n",
    "    blanks_na = blanks.drop(columns=['ORGANISM_PART','SCIENTIFIC_NAME',\n",
    "                                     'TUBE_OR_WELL_ID','RACK_OR_PLATE_ID',\n",
    "                                     'PRESERVATIVE_SOLUTION'])\n",
    "    na_filled = (blanks_na == 'NOT_APPLICABLE').all(axis=0)\n",
    "    if not na_filled.all():\n",
    "        logging.warning('for blanks, NOT_APPLICABLE expected, but not found in columns {}'.format(\n",
    "                            na_filled[~na_filled].index.to_list()))\n",
    "    # exclude blanks from downstream analysis\n",
    "    \n",
    "    df_flt = df[~is_blank]\n",
    "    \n",
    "    logging.info('{} samples of {} left for downstream analysis'.format(df_flt.shape[0], df.shape[0]))\n",
    "    \n",
    "    return df_flt\n",
    "        \n",
    "df = check_exclude_blanks(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] extracting value validation data from '../../analysis/0_partner/data/T222Amplicon_Manifest_V2.0.xlsx'\n"
     ]
    }
   ],
   "source": [
    "def get_valid_dict(fn):\n",
    "    # pick up validation values from data validation sheet\n",
    "    logging.info('extracting value validation data from {!r}'.format(fn))\n",
    "    valid_df = pd.read_excel(fn, dtype=str, sheet_name='Data Validation')\n",
    "    valid_dict = dict()\n",
    "    for col in valid_df.columns:\n",
    "        valid_dict[col] = valid_df[col].dropna().to_list()\n",
    "        \n",
    "    # add 96-well plate well IDs to validation\n",
    "    rid = list('ABCDEFGH')\n",
    "    cid = range(1,13)\n",
    "    valid_dict['TUBE_OR_WELL_ID'] = [r + str(c) for (r,c) in itertools.product(rid, cid)]\n",
    "    \n",
    "    # taxonomy placeholder\n",
    "    valid_dict['ORDER'] = ['Diptera']\n",
    "    valid_dict['FAMILY'] = ['Culicidae']\n",
    "    valid_dict['GENUS'] = ['Anopheles']\n",
    "\n",
    "    \n",
    "    return valid_dict\n",
    "valid_dict = get_valid_dict(template_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] excluding 95 ['NOT_COLLECTED', ''] samples without data in 'TIME_OF_COLLECTION'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Series([], Name: TIME_OF_COLLECTION, dtype: object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def exclude_missing(series, na_values=None):\n",
    "    \n",
    "    # valid missing data \n",
    "    no_data = (series.isin(na_values))\n",
    "    if no_data.sum() > 0:\n",
    "        logging.info('excluding {} {!r} samples without data in {!r}'.format(no_data.sum(), na_values, series.name))\n",
    "    return series[~no_data]\n",
    "    \n",
    "exclude_missing(df['TIME_OF_COLLECTION'], na_values=['NOT_COLLECTED', ''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] validating values in column 'ORGANISM_PART'\n"
     ]
    }
   ],
   "source": [
    "def validate_values(col, df, valid_dict, sep=None, na_values=None, level='e'):\n",
    "    \n",
    "    logging.info('validating values in column {!r}'.format(col))\n",
    "    \n",
    "    if col not in df.columns:\n",
    "        logging.error('{!r} column not found in manifest'.format(col))\n",
    "        return\n",
    "    if col not in valid_dict.keys():\n",
    "        logging.error('{!r} column not found in validation sheet'.format(col))\n",
    "        return\n",
    "    assert level in ('i','w','e'), '{!r} invalid logging level for validate_values'.format(level)\n",
    "    \n",
    "    series = df[col]\n",
    "    if na_values:\n",
    "        series = exclude_missing(series, na_values)\n",
    "    \n",
    "    col_values = set(series.unique())\n",
    "    # use separator to split values\n",
    "    if sep:\n",
    "        sep_col_values = list()\n",
    "        for v in col_values:\n",
    "            sep_col_values.extend([x.strip() for x in v.split(sep)])\n",
    "        col_values = set(sep_col_values)\n",
    "    valid_values = set(valid_dict[col])\n",
    "    invalid_values = col_values - valid_values\n",
    "    if len(invalid_values) > 0:\n",
    "        msg = 'invalid values in {!r}: {}'.format(col, invalid_values)\n",
    "        if level == 'i':\n",
    "            logging.info(msg)\n",
    "        elif level == 'w':\n",
    "            logging.warning(msg)\n",
    "        elif level == 'e':\n",
    "            logging.error(msg)\n",
    "#     else:\n",
    "#         logging.info('all values valid in {!r}'.format(col))\n",
    "            \n",
    "validate_values('ORGANISM_PART', df, valid_dict, sep=\" | \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] validating date column 'DATE_OF_COLLECTION'\n",
      "[ERROR] invalid dates in 'DATE_OF_COLLECTION': ['NOT_COLLECTED']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SERIES\n",
       "2    1905-05-26\n",
       "3    1905-05-26\n",
       "4    1905-05-26\n",
       "5    1905-05-26\n",
       "6    1905-05-26\n",
       "        ...    \n",
       "91   1905-05-14\n",
       "92   1905-05-16\n",
       "93   1905-05-16\n",
       "94   1905-05-10\n",
       "95   1905-05-14\n",
       "Name: DATE_OF_COLLECTION, Length: 94, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def validate_date(col, df):\n",
    "    \n",
    "    logging.info('validating date column {!r}'.format(col))\n",
    "\n",
    "    if col not in df.columns:\n",
    "        logging.error('{!r} column not found in manifest'.format(col))\n",
    "        return\n",
    "    series = df[col]\n",
    "    # missing date not allowed\n",
    "    # series = exclude_missing(series, na_values)\n",
    "    \n",
    "    # invalid date formats\n",
    "    # empty string converted to NaT\n",
    "    date_series = pd.to_datetime(series, format='%Y-%m-%d', errors='coerce')\n",
    "    if date_series.isna().any():\n",
    "        logging.error('invalid dates in {!r}: {}'.format(col, \n",
    "                                                         series[date_series.isna()].unique()))\n",
    "    valid_date_series = date_series[~date_series.isna()]\n",
    "    \n",
    "    # dates in future\n",
    "    future_dates = (valid_date_series > datetime.datetime.today())\n",
    "    if future_dates.any():\n",
    "        logging.error('future dates in {!r}: {}'.format(col,\n",
    "            valid_date_series[future_dates].to_list()))\n",
    "        \n",
    "    # dates too old\n",
    "    old_dates = (valid_date_series < datetime.datetime.strptime('1900-01-01', '%Y-%m-%d'))\n",
    "    if old_dates.any():\n",
    "        logging.error(\"pre-1900 dates in {!r}: {}\".format(col,\n",
    "            valid_date_series[old_dates].to_list())) \n",
    "    \n",
    "    return valid_date_series\n",
    "df.loc[1,'DATE_OF_COLLECTION'] = 'NOT_COLLECTED'\n",
    "validate_date('DATE_OF_COLLECTION', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   2000-01-01\n",
       "dtype: datetime64[ns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series = pd.Series(['2000'])\n",
    "pd.to_datetime(series, format='%Y-%m-%d', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] validating time column 'TIME_OF_COLLECTION'\n",
      "[ERROR] invalid times in 'TIME_OF_COLLECTION': ['']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Series([], Name: TIME_OF_COLLECTION, dtype: datetime64[ns])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def validate_time(col, df, na_values=['NOT_COLLECTED']):\n",
    "    \n",
    "    logging.info('validating time column {!r}'.format(col))\n",
    "    \n",
    "    if col not in df.columns:\n",
    "        logging.error('{!r} column not found in manifest'.format(col))\n",
    "        return\n",
    "    series = df[col]\n",
    "    series = exclude_missing(series, na_values)\n",
    "        \n",
    "    # invalid time formats\n",
    "    # NB empty string converted to NaT\n",
    "    time_series = pd.to_datetime(series, format='%H:%M:%S', errors='coerce')\n",
    "    if time_series.isna().any():\n",
    "        logging.error('invalid times in {!r}: {}'.format(col, \n",
    "                                                         series[time_series.isna()].unique()))\n",
    "    valid_time_series = time_series[~time_series.isna()]\n",
    "    \n",
    "    return valid_time_series\n",
    "# df.loc[1,'TIME_OF_COLLECTION'] = '23'\n",
    "validate_time('TIME_OF_COLLECTION', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] validating time period column 'DURATION_OF_COLLECTION'\n",
      "[ERROR] invalid times in 'DURATION_OF_COLLECTION': ['']\n"
     ]
    }
   ],
   "source": [
    "def validate_time_period(col, df, na_values=['NOT_COLLECTED']):\n",
    "    \n",
    "    logging.info('validating time period column {!r}'.format(col))\n",
    "    \n",
    "    if col not in df.columns:\n",
    "        logging.error('{!r} column not found in manifest'.format(col))\n",
    "        return\n",
    "    series = df[col]\n",
    "    series = exclude_missing(series, na_values)\n",
    "\n",
    "    # conversion with modifications for proper parsing \n",
    "    # by pd.Timedelta (does not accept missing data, e.g. 'PT1H')\n",
    "    # note - will not work for weeks and months\n",
    "    def convert_iso_duration(s):\n",
    "        if s == np.nan:\n",
    "            return np.nan\n",
    "        if not s.startswith('P') or 'T' not in s:\n",
    "            return np.nan\n",
    "        # add days\n",
    "        if s.startswith('PT'):\n",
    "            s = s.replace('PT','P0DT')\n",
    "        # add trailing minutes and seconds\n",
    "        if s.endswith('H'):\n",
    "            s += '0M0S'\n",
    "        elif s.endswith('M'):\n",
    "            s += '0S'\n",
    "        try:\n",
    "            return pd.Timedelta(s)\n",
    "        except:\n",
    "            return np.nan\n",
    "    time_period_series = series.apply(convert_iso_duration)\n",
    "    if time_period_series.isna().any():\n",
    "        logging.error('invalid times in {!r}: {}'.format(col, \n",
    "            series[time_period_series.isna()].unique()))\n",
    "    valid_time_period_series = time_period_series[~time_period_series.isna()]\n",
    "    return valid_time_period_series\n",
    "\n",
    "# df.loc[1,'DURATION_OF_COLLECTION'] = 'PVT1H'\n",
    "validate_time_period('DURATION_OF_COLLECTION', df);\n",
    "# df['DURATION_OF_COLLECTION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] validating country with coordinates\n",
      "[ERROR] country mismatch for coordinates '-3.889444, 14.452778', partner country 'REPUBLIC OF THE CONGO', coordinate country 'CONGO-BRAZZAVILLE'\n",
      "[ERROR] country mismatch for coordinates '9.593611, -5.197222', partner country \"COTE D'IVOIRE\", coordinate country \"CÔTE D'IVOIRE\"\n",
      "[ERROR] country mismatch for coordinates '5.340816, -4.133226', partner country \"COTE D'IVOIRE\", coordinate country \"CÔTE D'IVOIRE\"\n",
      "[ERROR] country mismatch for coordinates '6.6545, -4.710111', partner country \"COTE D'IVOIRE\", coordinate country \"CÔTE D'IVOIRE\"\n",
      "[ERROR] country mismatch for coordinates '6.729722, -3.496389', partner country \"COTE D'IVOIRE\", coordinate country \"CÔTE D'IVOIRE\"\n",
      "[ERROR] no partner location found for coordinates ', '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COLLECTION_LOCATION</th>\n",
       "      <th>DECIMAL_LATITUDE</th>\n",
       "      <th>DECIMAL_LONGITUDE</th>\n",
       "      <th>coord</th>\n",
       "      <th>partner_country</th>\n",
       "      <th>coord_country</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SERIES</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Republic of the Congo | Louhoulou (forêt de Ba...</td>\n",
       "      <td>-3.889444</td>\n",
       "      <td>14.452778</td>\n",
       "      <td>-3.889444, 14.452778</td>\n",
       "      <td>REPUBLIC OF THE CONGO</td>\n",
       "      <td>CONGO-BRAZZAVILLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Republic of the Congo | Louhoulou (forêt de Ba...</td>\n",
       "      <td>-3.889444</td>\n",
       "      <td>14.452778</td>\n",
       "      <td>-3.889444, 14.452778</td>\n",
       "      <td>REPUBLIC OF THE CONGO</td>\n",
       "      <td>CONGO-BRAZZAVILLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Republic of the Congo | Louhoulou (forêt de Ba...</td>\n",
       "      <td>-3.889444</td>\n",
       "      <td>14.452778</td>\n",
       "      <td>-3.889444, 14.452778</td>\n",
       "      <td>REPUBLIC OF THE CONGO</td>\n",
       "      <td>CONGO-BRAZZAVILLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Republic of the Congo | Louhoulou (forêt de Ba...</td>\n",
       "      <td>-3.889444</td>\n",
       "      <td>14.452778</td>\n",
       "      <td>-3.889444, 14.452778</td>\n",
       "      <td>REPUBLIC OF THE CONGO</td>\n",
       "      <td>CONGO-BRAZZAVILLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Republic of the Congo | Louhoulou (forêt de Ba...</td>\n",
       "      <td>-3.889444</td>\n",
       "      <td>14.452778</td>\n",
       "      <td>-3.889444, 14.452778</td>\n",
       "      <td>REPUBLIC OF THE CONGO</td>\n",
       "      <td>CONGO-BRAZZAVILLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Cameroon | Yaoundé Awaé</td>\n",
       "      <td>3.901848</td>\n",
       "      <td>11.880727</td>\n",
       "      <td>3.901848, 11.880727</td>\n",
       "      <td>CAMEROON</td>\n",
       "      <td>CAMEROON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Mauritania | Kaedi</td>\n",
       "      <td>16.153722</td>\n",
       "      <td>-13.503694</td>\n",
       "      <td>16.153722, -13.503694</td>\n",
       "      <td>MAURITANIA</td>\n",
       "      <td>MAURITANIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Burkina Faso | Volta Blanche (au sud route Oua...</td>\n",
       "      <td>12.233479</td>\n",
       "      <td>-1.083331</td>\n",
       "      <td>12.233479, -1.083331</td>\n",
       "      <td>BURKINA FASO</td>\n",
       "      <td>BURKINA FASO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Cote d'Ivoire | Adiopodoumé</td>\n",
       "      <td>5.340816</td>\n",
       "      <td>-4.133226</td>\n",
       "      <td>5.340816, -4.133226</td>\n",
       "      <td>COTE D'IVOIRE</td>\n",
       "      <td>CÔTE D'IVOIRE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Cameroon | Yaoundé Awaé</td>\n",
       "      <td>3.901848</td>\n",
       "      <td>11.880727</td>\n",
       "      <td>3.901848, 11.880727</td>\n",
       "      <td>CAMEROON</td>\n",
       "      <td>CAMEROON</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      COLLECTION_LOCATION DECIMAL_LATITUDE  \\\n",
       "SERIES                                                                       \n",
       "1       Republic of the Congo | Louhoulou (forêt de Ba...        -3.889444   \n",
       "2       Republic of the Congo | Louhoulou (forêt de Ba...        -3.889444   \n",
       "3       Republic of the Congo | Louhoulou (forêt de Ba...        -3.889444   \n",
       "4       Republic of the Congo | Louhoulou (forêt de Ba...        -3.889444   \n",
       "5       Republic of the Congo | Louhoulou (forêt de Ba...        -3.889444   \n",
       "...                                                   ...              ...   \n",
       "91                                Cameroon | Yaoundé Awaé         3.901848   \n",
       "92                                     Mauritania | Kaedi        16.153722   \n",
       "93      Burkina Faso | Volta Blanche (au sud route Oua...        12.233479   \n",
       "94                            Cote d'Ivoire | Adiopodoumé         5.340816   \n",
       "95                                Cameroon | Yaoundé Awaé         3.901848   \n",
       "\n",
       "       DECIMAL_LONGITUDE                  coord        partner_country  \\\n",
       "SERIES                                                                   \n",
       "1              14.452778   -3.889444, 14.452778  REPUBLIC OF THE CONGO   \n",
       "2              14.452778   -3.889444, 14.452778  REPUBLIC OF THE CONGO   \n",
       "3              14.452778   -3.889444, 14.452778  REPUBLIC OF THE CONGO   \n",
       "4              14.452778   -3.889444, 14.452778  REPUBLIC OF THE CONGO   \n",
       "5              14.452778   -3.889444, 14.452778  REPUBLIC OF THE CONGO   \n",
       "...                  ...                    ...                    ...   \n",
       "91             11.880727    3.901848, 11.880727               CAMEROON   \n",
       "92            -13.503694  16.153722, -13.503694             MAURITANIA   \n",
       "93             -1.083331   12.233479, -1.083331           BURKINA FASO   \n",
       "94             -4.133226    5.340816, -4.133226          COTE D'IVOIRE   \n",
       "95             11.880727    3.901848, 11.880727               CAMEROON   \n",
       "\n",
       "            coord_country  \n",
       "SERIES                     \n",
       "1       CONGO-BRAZZAVILLE  \n",
       "2       CONGO-BRAZZAVILLE  \n",
       "3       CONGO-BRAZZAVILLE  \n",
       "4       CONGO-BRAZZAVILLE  \n",
       "5       CONGO-BRAZZAVILLE  \n",
       "...                   ...  \n",
       "91               CAMEROON  \n",
       "92             MAURITANIA  \n",
       "93           BURKINA FASO  \n",
       "94          CÔTE D'IVOIRE  \n",
       "95               CAMEROON  \n",
       "\n",
       "[95 rows x 6 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_location(df, fn):\n",
    "    \n",
    "    logging.info('validating country with coordinates')\n",
    "    \n",
    "    loc_col, lat_col, lon_col = 'COLLECTION_LOCATION', 'DECIMAL_LATITUDE', 'DECIMAL_LONGITUDE'\n",
    "\n",
    "    try:\n",
    "        loc_df_complete = df[[loc_col, lat_col, lon_col]].copy()\n",
    "    except:\n",
    "        logging.error('One of {!r} {!r} {!r} columns not found in manifest'.format(loc_col, lat_col, lon_col))\n",
    "        return\n",
    "#     loc_df_isna = (loc_df.isin(na_values)).all(axis=1)\n",
    "#     if loc_df_isna.any():\n",
    "#         logging.info('removing {} {!r} samples with missing data from coordinate analysis'.format(\n",
    "#                 loc_df_isna.sum(), na_values))\n",
    "#     loc_df_complete = loc_df[~loc_df_isna].copy()\n",
    "    \n",
    "    # coordinates in geopy format\n",
    "    loc_df_complete['coord'] = loc_df_complete.apply(lambda x: '{}, {}'.format(\n",
    "            x[lat_col], x[lon_col]), axis=1)\n",
    "    \n",
    "    # get location data for coordinates\n",
    "    # use local copy of web query results for re-runs\n",
    "    # this \n",
    "    loc_fn = fn+'_loc.pkl'\n",
    "    if os.path.isfile(loc_fn):\n",
    "        locations = pickle.load(open(loc_fn, \"rb\"))\n",
    "    else:\n",
    "        # web map server - openstreetmaps\n",
    "        logging.info('querying coordinates')\n",
    "        locator = Nominatim(user_agent='myGeocoder')\n",
    "        rgeocode = RateLimiter(locator.reverse, min_delay_seconds=1)\n",
    "\n",
    "        locations = dict()\n",
    "        for c in loc_df_complete.coord.unique():\n",
    "            # pre-fill with unknown country\n",
    "            locations[c] = {'address':{'country':'UNKNOWN'}}\n",
    "            # check coordniate correctness\n",
    "            try:\n",
    "                lat, lon = c.split(', ')\n",
    "                lat, lon = float(lat), float(lon)\n",
    "            except:\n",
    "                logging.error('problem parsing coordinates {!r}'.format(c))\n",
    "                continue\n",
    "            if abs(lat) > 90:\n",
    "                logging.error('invalid latitude {}, should be in [-90,90]'.format(lat))\n",
    "                continue\n",
    "            if abs(lon) > 180:\n",
    "                logging.error('invalid longitude {}, should be in [-180,180]'.format(lon))\n",
    "                continue\n",
    "            # web query\n",
    "            location = rgeocode(c, language='en-gb')\n",
    "            # rgeocode returns empty location outside of counries and in some other situations\n",
    "            if location is not None:\n",
    "                locations[c] = location.raw\n",
    "\n",
    "        # save locations to file\n",
    "        pickle.dump(locations, open(loc_fn, \"wb\"))\n",
    "        \n",
    "    # parse country from partner input\n",
    "    loc_df_complete['partner_country'] = loc_df_complete[loc_col].apply(lambda x: x.split('|')[0].strip().upper())\n",
    "    \n",
    "    # extract countries from location data\n",
    "    loc_countries = dict()\n",
    "    for coord in locations.keys():\n",
    "        coord_country = locations[coord]['address']['country'].upper()\n",
    "        loc_countries[coord] = coord_country\n",
    "        \n",
    "        partner_countries = loc_df_complete.loc[loc_df_complete.coord == coord, 'partner_country']\n",
    "        if partner_countries.nunique() > 1:\n",
    "            logging.error('multiple partner countries for coordinates {!r}: {}'\n",
    "                          'skipping coordinate validation'.format(\n",
    "                                coord, partner_countries.unique()))\n",
    "            continue\n",
    "        if partner_countries.shape[0] == 0:\n",
    "            logging.error('no partner location found for coordinates {!r}'.format(coord))\n",
    "            continue\n",
    "        partner_country = partner_countries.iloc[0]\n",
    "        if coord_country == 'UNKNOWN':\n",
    "            logging.warning('could not locate country for coordinates {!r}, partner country {!r}'.format(\n",
    "                    coord, partner_country))\n",
    "        elif partner_country != coord_country:\n",
    "            logging.error('country mismatch for coordinates {!r}, partner country {!r}, '\n",
    "                          'coordinate country {!r}'.format(coord, partner_country, coord_country))\n",
    "    \n",
    "    # countries based on coordinates\n",
    "    loc_df_complete['coord_country'] = loc_df_complete['coord'].replace(loc_countries)\n",
    "    country_mismatch = (loc_df_complete.coord_country != loc_df_complete.partner_country)\n",
    "\n",
    "#     if country_mismatch.any():\n",
    "#         logging.error('coordinates do not match country for SERIES: {}'.format(\n",
    "#                 country_mismatch[country_mismatch].index.to_list()))\n",
    "    \n",
    "    # location data can be re-used, e.g. as an additional field\n",
    "    return loc_df_complete\n",
    "# df.loc[2,'DECIMAL_LATITUDE'] = '65'\n",
    "loc_test = check_location(df, fn)\n",
    "loc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loc_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/g7/vnlptw7d5pb544zsj_bypk44000h93/T/ipykernel_15574/1407161926.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'address'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loc_dict' is not defined"
     ]
    }
   ],
   "source": [
    "l = pd.DataFrame(loc_dict).T\n",
    "pd.DataFrame(l['address'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "496"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# valid_spp = list()\n",
    "valid_spp = pd.read_csv(spp_fn, header=None)[0].to_list()\n",
    "len(valid_spp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] validating species names against '../../analysis/0_partner/data/harbach_spp_201910.txt'\n"
     ]
    }
   ],
   "source": [
    "def validate_scientific_names(col, df, spp_fn, na_values = ['NOT_COLLECTED']):\n",
    "    \n",
    "    logging.info('validating species names against {!r}'.format(spp_fn))\n",
    "\n",
    "    # read species names from file\n",
    "    valid_spp = pd.read_csv(spp_fn, header=None)[0].to_list()\n",
    "    \n",
    "    if col not in df.columns:\n",
    "        logging.error('{!r} column not found in manifest'.format(col))\n",
    "        return\n",
    "    series = df[col]\n",
    "    series = exclude_missing(series, na_values)\n",
    "    \n",
    "    correct_genus = series.str.startswith('Anopheles')\n",
    "    if not correct_genus.all():\n",
    "        logging.error('expected Anopheles as genus in {!r} column, found: {}'.format(\n",
    "                      col, series[~correct_genus].unique()))\n",
    "    \n",
    "    species = series.str.split(' ').str.get(1)\n",
    "    correct_species = species.isin(valid_spp)\n",
    "    \n",
    "    if not correct_species.all():\n",
    "        logging.error('species not in {!r} found in {!r} column: {}'.format(\n",
    "                      spp_fn, col, series[~correct_species].unique()))\n",
    "# df.loc[1, 'SCIENTIFIC_NAME'] = 'Anapherla askfnvjn'\n",
    "validate_scientific_names('SCIENTIFIC_NAME', df, spp_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_undeleted_example(df):\n",
    "    \n",
    "    logging.info('checking if example row was not deleted')\n",
    "    \n",
    "    if (df.index == 'example').any() | \\\n",
    "       (df.RACK_OR_PLATE_ID == 'PLATE_MMMM2222').any() | \\\n",
    "       (df.CATCH_ID == 'ignore this field').any():\n",
    "        logging.error('example row was not deleted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] for blanks, ORGANISM_PART expected to be BLANK_SAMPLE, found {'CONTROL_WELL'}\n",
      "[WARNING] for blanks, NOT_APPLICABLE expected, but not found in columns ['OTHER_ORGANISMS', 'BLOOD_MEAL', 'GRAVIDITY', 'COLLECTOR_SAMPLE_ID', 'WHAT_3_WORDS', 'ELEVATION', 'DNA_EXTRACTION_DESCRIPTION', 'DNA_EXTRACT_VOLUME_PROVIDED', 'DNA_EXTRACT_CONCENTRATION', 'HAZARD_GROUP', 'REGULATORY_COMPLIANCE', 'OTHER_INFORMATION', 'MISC_METADATA', 'COLLECTED_BY', 'COLLECTOR_AFFILIATION', 'IDENTIFIED_BY', 'IDENTIFIER_AFFILIATION', 'PRESERVED_BY', 'PRESERVER_AFFILIATION']\n",
      "[ERROR] country mismatch for coordinates '-3.889444, 14.452778', partner country 'REPUBLIC OF THE CONGO', coordinate country 'CONGO-BRAZZAVILLE'\n",
      "[ERROR] country mismatch for coordinates '9.593611, -5.197222', partner country \"COTE D'IVOIRE\", coordinate country \"CÔTE D'IVOIRE\"\n",
      "[ERROR] country mismatch for coordinates '5.340816, -4.133226', partner country \"COTE D'IVOIRE\", coordinate country \"CÔTE D'IVOIRE\"\n",
      "[ERROR] country mismatch for coordinates '6.6545, -4.710111', partner country \"COTE D'IVOIRE\", coordinate country \"CÔTE D'IVOIRE\"\n",
      "[ERROR] country mismatch for coordinates '6.729722, -3.496389', partner country \"COTE D'IVOIRE\", coordinate country \"CÔTE D'IVOIRE\"\n",
      "[ERROR] no partner location found for coordinates ', '\n",
      "[ERROR] invalid times in 'TIME_OF_COLLECTION': ['']\n",
      "[ERROR] invalid times in 'DURATION_OF_COLLECTION': ['']\n",
      "[WARNING] invalid values in 'COLLECTION_METHOD': {''}\n",
      "[ERROR] invalid values in 'OUTDOORS_INDOORS': {''}\n",
      "[WARNING] invalid values in 'PRESERVATIVE_SOLUTION': {''}\n",
      "[ERROR] invalid dates in 'DATE_OF_PRESERVATION': ['']\n"
     ]
    }
   ],
   "source": [
    "def validate(fn, template_fn, spp_fn, verbose=False, version='2.0'):\n",
    "    '''\n",
    "    Validation follows the order of columns order in data entry sheet\n",
    "    '''\n",
    "\n",
    "    setup_logging(verbose=verbose)\n",
    "\n",
    "    logging.info('# started validate_partner_manifest_v.{}'.format(version))\n",
    "\n",
    "    # read data\n",
    "    df = get_data(fn)\n",
    "    \n",
    "    # prepare validation\n",
    "    template_df = get_data(template_fn)\n",
    "    check_columns(df, template_df)\n",
    "    valid_dict = get_valid_dict(template_fn)\n",
    "\n",
    "    # exclude blanks\n",
    "    df = check_exclude_blanks(df)\n",
    "    \n",
    "    \n",
    "    validate_values('TUBE_OR_WELL_ID', df, valid_dict)\n",
    "    validate_values('ORGANISM_PART', df, valid_dict, sep='|')\n",
    "\n",
    "    validate_values('ORDER', df, valid_dict)\n",
    "    validate_values('FAMILY', df, valid_dict)\n",
    "    validate_values('GENUS', df, valid_dict)\n",
    "    \n",
    "    validate_scientific_names('SCIENTIFIC_NAME', df, spp_fn, na_values=['NOT_COLLECTED'])\n",
    "        \n",
    "    validate_values('LIFESTAGE', df, valid_dict)\n",
    "    validate_values('SEX', df, valid_dict)\n",
    "    \n",
    "    validate_values('BLOOD_MEAL', df, valid_dict, na_values=[''])\n",
    "    validate_values('GRAVIDITY', df, valid_dict, na_values=[''])\n",
    "    \n",
    "    validate_date('DATE_OF_COLLECTION', df)\n",
    "\n",
    "    check_location(df, fn)\n",
    "    \n",
    "    #validate_format('ELEVATION', dtype=int)\n",
    "    \n",
    "    validate_time('TIME_OF_COLLECTION', df)\n",
    "    validate_time_period('DURATION_OF_COLLECTION', df)\n",
    "\n",
    "    validate_values('COLLECTION_METHOD', df, valid_dict, level='w')\n",
    "    validate_values('OUTDOORS_INDOORS', df, valid_dict)\n",
    "    validate_values('PRESERVATIVE_SOLUTION', df, valid_dict, level='w')\n",
    "    \n",
    "    #validate_format('TIME_ELAPSED_FROM_COLLECTION_TO_PRESERVATION', dtype=int)\n",
    "    validate_date('DATE_OF_PRESERVATION', df)\n",
    "    #compare_dates(before='DATE_OF_COLLECTION', after='DATE_OF_PRESERVATION')\n",
    "    \n",
    "    validate_values('HAZARD_GROUP', df, valid_dict)\n",
    "    validate_values('REGULATORY_COMPLIANCE', df, valid_dict)\n",
    "    \n",
    "    check_undeleted_example(df)\n",
    "    \n",
    "    logging.info('# ended validate_partner_manifest_v.{}'.format(version))\n",
    "\n",
    "    return df\n",
    "\n",
    "fn = '../../results/partner_manifests/IRD-Neandersquito_T222Amplicon_Manifest_V2.0.xlsx'\n",
    "df = validate(fn, template_fn, spp_fn, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] last well H12 is not blank at SERIES [97, 193, 289, 385, 481, 668, 764]\n",
      "[ERROR] invalid times in 'DURATION_OF_COLLECTION': ['']\n",
      "[ERROR] invalid dates in 'DATE_OF_PRESERVATION': ['NOT_COLLECTED']\n",
      "[ERROR] example row was not deleted\n"
     ]
    }
   ],
   "source": [
    "fn = '../../results/partner_manifests/1264_PAMCA_16Apr2021_T222Amplicon_Manifest_V1.1.xlsx'\n",
    "df = validate(fn, template_fn, spp_fn, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] trailing spaces found in 'SCIENTIFIC_NAME', removing for validation\n",
      "[ERROR] template columns missing from filled manifest: {'MISC_METADATA'}\n",
      "[ERROR] for blanks, ORGANISM_PART expected to be BLANK_SAMPLE, found {''}\n",
      "[WARNING] for blanks, NOT_APPLICABLE expected, but not found in columns ['CATCH_ID', 'OTHER_ORGANISMS', 'COLLECTOR_SAMPLE_ID', 'COLLECTED_BY', 'COLLECTOR_AFFILIATION', 'WHAT_3_WORDS', 'ELEVATION', 'IDENTIFIED_BY', 'IDENTIFIER_AFFILIATION', 'IDENTIFIED_HOW', 'PRESERVED_BY', 'PRESERVER_AFFILIATION', 'PRESERVATION_APPROACH', 'TIME_ELAPSED_FROM_COLLECTION_TO_PRESERVATION', 'DATE_OF_PRESERVATION', 'DNA_EXTRACTION_DESCRIPTION', 'DNA_EXTRACT_VOLUME_PROVIDED', 'DNA_EXTRACT_CONCENTRATION', 'HAZARD_GROUP', 'REGULATORY_COMPLIANCE', 'OTHER_INFORMATION']\n",
      "[ERROR] invalid values in 'TUBE_OR_WELL_ID': {'6A'}\n",
      "[ERROR] invalid values in 'ORGANISM_PART': {''}\n",
      "[ERROR] expected Anopheles as genus in 'SCIENTIFIC_NAME' column, found: ['unknown']\n",
      "[ERROR] species not in 'data/harbach_spp_201910.txt' found in 'SCIENTIFIC_NAME' column: ['unknown']\n",
      "[ERROR] invalid dates in 'DATE_OF_PRESERVATION': ['NOT_COLLECTED' 'NOT_APPLICABLE']\n",
      "[ERROR] example row was not deleted\n"
     ]
    }
   ],
   "source": [
    "fn = '../../results/partner_manifests/1272_PAMCA_16_Apr2021_T222Amplicon_Manifest_V1.1.xlsx'\n",
    "df = validate(fn, template_fn, spp_fn,  verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../results/partner_manifests/AYALA_20210426_T222Amplicon_Manifest_V1.1.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-2e29132266a0>\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m      6\u001b[0m         df = pd.read_excel(fn, dtype=str, index_col=0, keep_default_na=False,\n\u001b[0;32m----> 7\u001b[0;31m                            sheet_name='TAB1 Specimen Metadata Entry')\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/validate_partner_manifest_dev/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/validate_partner_manifest_dev/lib/python3.7/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/validate_partner_manifest_dev/lib/python3.7/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine)\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/validate_partner_manifest_dev/lib/python3.7/site-packages/pandas/io/excel/_xlrd.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"xlrd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/validate_partner_manifest_dev/lib/python3.7/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/validate_partner_manifest_dev/lib/python3.7/site-packages/pandas/io/excel/_xlrd.py\u001b[0m in \u001b[0;36mload_workbook\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/validate_partner_manifest_dev/lib/python3.7/site-packages/xlrd/__init__.py\u001b[0m in \u001b[0;36mopen_workbook\u001b[0;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../results/partner_manifests/AYALA_20210426_T222Amplicon_Manifest_V1.1.xlsx'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-fb3af23908d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../../results/partner_manifests/AYALA_20210426_T222Amplicon_Manifest_V1.1.xlsx'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemplate_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspp_fn\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-893d4971df30>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(fn, template_fn, spp_fn, verbose, version)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# read data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# prepare validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-2e29132266a0>\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         df = pd.read_excel(fn, dtype=str, index_col=0, keep_default_na=False,\n\u001b[0;32m---> 10\u001b[0;31m                            sheet_name='Specimen Metadata Entry')\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduplicated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/validate_partner_manifest_dev/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m                 )\n\u001b[1;32m    295\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/validate_partner_manifest_dev/lib/python3.7/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m~/miniconda3/envs/validate_partner_manifest_dev/lib/python3.7/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine)\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstringify_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/validate_partner_manifest_dev/lib/python3.7/site-packages/pandas/io/excel/_xlrd.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Install xlrd >= 1.0.0 for Excel support\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"xlrd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/validate_partner_manifest_dev/lib/python3.7/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/validate_partner_manifest_dev/lib/python3.7/site-packages/pandas/io/excel/_xlrd.py\u001b[0m in \u001b[0;36mload_workbook\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/validate_partner_manifest_dev/lib/python3.7/site-packages/xlrd/__init__.py\u001b[0m in \u001b[0;36mopen_workbook\u001b[0;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpeek\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb\"PK\\x03\\x04\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# a ZIP file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../results/partner_manifests/AYALA_20210426_T222Amplicon_Manifest_V1.1.xlsx'"
     ]
    }
   ],
   "source": [
    "fn = '../../results/partner_manifests/AYALA_20210426_T222Amplicon_Manifest_V1.1.xlsx'\n",
    "validate(fn, template_fn, spp_fn,  verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loc_fn = 'data/WALTON_09_Mar2021_T222Amplicon_Manifest_V1.0.xlsx_loc.pkl'\n",
    "locations = pickle.load(open(test_loc_fn, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
