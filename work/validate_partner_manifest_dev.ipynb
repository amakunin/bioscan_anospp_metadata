{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "import itertools\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import ete3\n",
    "import geopy\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] test\n"
     ]
    }
   ],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)\n",
    "# logging.getLogger().setFormat('[%(levelname)s] %(message)s')\n",
    "\n",
    "def setup_logging(verbose=False):\n",
    "    try: \n",
    "        del logging.root.handlers[:]\n",
    "    except:\n",
    "        pass\n",
    "    if verbose:\n",
    "        logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n",
    "    else:\n",
    "        logging.basicConfig(level=logging.WARNING, format='[%(levelname)s] %(message)s')\n",
    "setup_logging(verbose=True)   \n",
    "logging.info('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "anospp_fn = '../data/Anopheles_Metadata_Manifest_V4.0_20220825.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and install taxonomy\n",
    "ncbi = ete3.NCBITaxa()\n",
    "# run update if needed\n",
    "# ncbi.update_taxonomy_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] reading data from '../data/Anopheles_Metadata_Manifest_V4.0_20220825.xlsx'\n"
     ]
    }
   ],
   "source": [
    "def get_data(fn, sheet='TAB 2 Metadata Entry'):\n",
    "\n",
    "    logging.info('reading data from {!r}'.format(fn))\n",
    "    \n",
    "    df = pd.read_excel(fn, dtype=str, index_col=0, keep_default_na=False,\n",
    "                       sheet_name=sheet)\n",
    "        \n",
    "    return df\n",
    "df = get_data(anospp_fn, sheet='TAB 3 TEST Metadata Entry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] trailing spaces found in column 'COLLECTION_LOCATION', SERIES [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]. Removing for validation\n",
      "[WARNING] trailing spaces found in column 'PREDICTED_SCIENTIFIC_NAME', SERIES [2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]. Removing for validation\n",
      "[WARNING] trailing spaces found in column 'PRESERVATION_APPROACH', SERIES [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288]. Removing for validation\n",
      "[WARNING] trailing spaces found in column 'HABITAT', SERIES [2]. Removing for validation\n"
     ]
    }
   ],
   "source": [
    "def remove_trailing_spaces(df):\n",
    "    for col in df.columns:\n",
    "        trailing_spaces = (df[col].str.startswith(' ') | df[col].str.endswith(' '))\n",
    "        if trailing_spaces.any():\n",
    "            logging.warning('trailing spaces found in column {!r}, SERIES {}. Removing for validation'.format(col,\n",
    "                df.loc[trailing_spaces].index.to_list()))\n",
    "            df[col] = df[col].str.strip()\n",
    "            \n",
    "    return df\n",
    "df = remove_trailing_spaces(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] reading data from '../data/Anopheles_Metadata_Manifest_V4.0_20220825.xlsx'\n"
     ]
    }
   ],
   "source": [
    "template_df = get_data(anospp_fn, sheet='TAB 2 Metadata Entry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] checking manifest columns against template\n"
     ]
    }
   ],
   "source": [
    "def check_columns(df, template_df):\n",
    "    \n",
    "    logging.info('checking manifest columns against template')\n",
    "    \n",
    "    data_cols = set(df.columns)\n",
    "    template_cols = set(template_df.columns)\n",
    "        \n",
    "    if data_cols - template_cols != set():\n",
    "        logging.warning('extra columns in filled manifest compared to template: {}'.format(data_cols - template_cols))\n",
    "    if template_cols - data_cols != set():\n",
    "        logging.error('template columns missing from filled manifest: {}'.format(template_cols - data_cols))\n",
    "check_columns(df, template_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] extracting value validation data from '../data/Anopheles_Metadata_Manifest_V4.0_20220825.xlsx'\n"
     ]
    }
   ],
   "source": [
    "def get_valid_dict(fn, validation_sheet='Data Validation - do not edit'):\n",
    "    \n",
    "    # pick up validation values from data validation sheet\n",
    "    logging.info('extracting value validation data from {!r}'.format(fn))\n",
    "    valid_df = pd.read_excel(fn, dtype=str, sheet_name=validation_sheet)\n",
    "    valid_dict = dict()\n",
    "    for col in valid_df.columns:\n",
    "        valid_dict[col] = valid_df[col].dropna().to_list()\n",
    "    \n",
    "    return valid_dict\n",
    "valid_dict = get_valid_dict(anospp_fn, validation_sheet='TAB 5 Data Validation - do not ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] excluding 675 ['NOT_COLLECTED', ''] samples without data in 'TIME_OF_COLLECTION'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SERIES\n",
       "1     18:00:00\n",
       "2     22:43:00\n",
       "3     04:00:00\n",
       "4     04:00:00\n",
       "5      \"33.00\"\n",
       "        ...   \n",
       "91    09:00:00\n",
       "92    09:00:00\n",
       "93    09:00:00\n",
       "94    09:00:00\n",
       "95    09:00:00\n",
       "Name: TIME_OF_COLLECTION, Length: 95, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def exclude_missing(series, na_values=[]):\n",
    "    \n",
    "    # valid missing data \n",
    "    if len(na_values) > 0:\n",
    "        no_data = (series.isin(na_values))\n",
    "        logging.info('excluding {} {!r} samples without data in {!r}'.format(no_data.sum(), na_values, series.name))\n",
    "        return series[~no_data]\n",
    "    return series\n",
    "    \n",
    "exclude_missing(df['TIME_OF_COLLECTION'], na_values=['NOT_COLLECTED',''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] validating SERIES\n",
      "[ERROR] In SERIES, ['770'] are missing, ['771'] are unexpected\n"
     ]
    }
   ],
   "source": [
    "def validate_series(df):\n",
    "    \n",
    "    # series should be 1,2, ..., nsamples\n",
    "    logging.info('validating SERIES')\n",
    "    \n",
    "    if df.index.duplicated().any():\n",
    "        logging.error('duplicate SERIES: {}'.format(df.index[df.index.duplicated()].to_list()))\n",
    "    \n",
    "    # exclude non-numeric SERIES\n",
    "    series_numeric = df.index.astype(str).str.isnumeric()\n",
    "    if not series_numeric.all():\n",
    "        logging.error(f'Found and excluded non-numeric SERIES: {df.index[~series_numeric].to_list()}')\n",
    "        df = df.loc[series_numeric]\n",
    "        \n",
    "    # check the remaining SERIES are continuous\n",
    "    expected_series = set([str(i) for i in range(1, df.shape[0] + 1)])\n",
    "    observed_series = set(df.index.astype(str))\n",
    "    if expected_series != observed_series:\n",
    "        logging.error(f'In SERIES, {sorted(list(expected_series - observed_series))} are missing, '\n",
    "                      f'{sorted(list(observed_series - expected_series))} are unexpected')\n",
    "        \n",
    "    return df\n",
    "        \n",
    "df = validate_series(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] validating RACK_OR_PLATE_ID and TUBE_OR_WELL_ID\n",
      "[ERROR] Found and excluded 482 empty rows based on RACK_OR_PLATE_ID and TUBE_OR_WELL_ID\n",
      "[INFO] found 288 samples across 3 plates\n"
     ]
    }
   ],
   "source": [
    "def validate_plates_wells(df, plate_col, well_col, bioscan=False):\n",
    "    \n",
    "    # expect only complete 96-well plates\n",
    "    logging.info(f'validating {plate_col} and {well_col}')\n",
    "    \n",
    "    empty_rows = (df[plate_col] == '') | (df[well_col] == '')\n",
    "    \n",
    "    if empty_rows.any():\n",
    "        logging.error(f'Found and excluded {empty_rows.sum()} empty rows based on {plate_col} and {well_col}')\n",
    "        df = df.loc[~empty_rows]\n",
    "    \n",
    "    # add 96-well plate well IDs to validation\n",
    "    row_id = list('ABCDEFGH')\n",
    "    col_id = range(1,13)\n",
    "    expected_wells = [r + str(c) for (r,c) in itertools.product(row_id, col_id)]\n",
    "    \n",
    "    pdfs = []\n",
    "    \n",
    "    for plate, pdf in df.groupby(plate_col):\n",
    "        # If plate level only metadata is being entered\n",
    "        # put “PLATE_ONLY” in well \n",
    "        # and use only one row to capture the metadata for the whole plate\n",
    "        if bioscan and (pdf[well_col] == 'PLATE_ONLY').any():\n",
    "            # elevate to warning\n",
    "            logging.warning(f'found PLATE_ONLY plate {plate}, expanding to well-level')\n",
    "            if pdf.shape[0] > 1:\n",
    "                logging.error(f'too many rows in PLATE_ONLY plate {plate}, expected one')\n",
    "            # expand to 96 rows\n",
    "            pdf = pd.DataFrame(pdf.iloc[0] for i in range(len(expected_wells)))\n",
    "            pdf[well_col] = expected_wells\n",
    "            # H12 blank\n",
    "            for col in pdf.columns:\n",
    "                if col == 'ORGANISM_PART':\n",
    "                    pdf[col].iloc[-1]='NOT_APPLICABLE'\n",
    "                elif col not in ['SERIES','CATCH_LOT','RACK_OR_PLATE_ID',\n",
    "                                 'TUBE_OR_WELL_ID','ORGANISM_PART','PRESERVATIVE_SOLUTION']:\n",
    "                    pdf[col].iloc[-1]=''\n",
    "        dup_wells =  pdf[well_col].duplicated()\n",
    "        if dup_wells.any():\n",
    "            logging.error(f'duplicate {well_col} for plate {plate}: {pdf.loc[dup_wells, well_col].unique()}')\n",
    "        observed_wells = set(pdf[well_col])\n",
    "        expected_wells = set(expected_wells)\n",
    "        if observed_wells != expected_wells:\n",
    "            logging.error(f'in {well_col} for plate {plate}, wells {expected_wells - observed_wells} '\n",
    "                          f'are missing, wells {observed_wells - expected_wells} are excessive')\n",
    "        pdfs.append(pdf)\n",
    "    \n",
    "    if bioscan:\n",
    "        df = pd.concat(pdfs)\n",
    "    \n",
    "    logging.info(f'found {df.shape[0]} samples across {df[plate_col].nunique()} plates')\n",
    "    \n",
    "    return df\n",
    "        \n",
    "df = validate_plates_wells(df, 'RACK_OR_PLATE_ID', 'TUBE_OR_WELL_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Checking and excluding blank samples\n",
      "[ERROR] last well H12 is not blank at SERIES [288]: in ORGANISM_PART, expected \"NOT_APPLICABLE\", found ['TEST']. These samples will be included in further analysis.\n",
      "[WARNING] 2 blanks located across 3 plates, 286 samples of 288 left for downstream analysis\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(288, 35)\n",
      "(286, 35)\n"
     ]
    }
   ],
   "source": [
    "## TODO - which columns require NA, do not remove blanks to be able to get taxids for all\n",
    "def check_blanks(df):\n",
    "    \n",
    "    logging.info('Checking and excluding blank samples')    \n",
    "    \n",
    "    # blank criterion\n",
    "    is_blank = (df['ORGANISM_PART'] == 'NOT_APPLICABLE')\n",
    "    \n",
    "    blank_df = df[is_blank]\n",
    "    \n",
    "    non_blank_df = df[~is_blank]\n",
    "    \n",
    "    # last well of plate expected to be blank\n",
    "    non_blank_last_well_df = non_blank_df[non_blank_df['TUBE_OR_WELL_ID'] == 'H12']\n",
    "    if non_blank_last_well_df.shape[0] > 0:\n",
    "        logging.error('last well H12 is not blank at SERIES {}: in ORGANISM_PART, '\n",
    "                      'expected \"NOT_APPLICABLE\", found {}. '\n",
    "                      'These samples will be included in further analysis.'.format(\n",
    "                        non_blank_last_well_df.index.to_list(),\n",
    "                        non_blank_last_well_df.ORGANISM_PART.to_list()\n",
    "        ))\n",
    "    \n",
    "    # raise to warning for sanity check\n",
    "    logging.warning('{} blanks located across {} plates, {} samples of {} left for downstream analysis'.format(\n",
    "        is_blank.sum(), df.RACK_OR_PLATE_ID.nunique(), non_blank_df.shape[0], df.shape[0]))\n",
    "    \n",
    "    \n",
    "    return is_blank\n",
    "print(df.shape)\n",
    "is_blank = check_blanks(df)\n",
    "print(df[~is_blank].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] validating values in column 'ORGANISM_PART'\n",
      "[INFO] excluding 0 [] samples without data in 'ORGANISM_PART'\n",
      "[ERROR] invalid values in 'ORGANISM_PART': {'TEST'}\n"
     ]
    }
   ],
   "source": [
    "def validate_values(col, df, valid_dict, sep=None, na_values=[], level='e'):\n",
    "    \n",
    "    logging.info('validating values in column {!r}'.format(col))\n",
    "    \n",
    "    if col not in df.columns:\n",
    "        logging.error('{!r} column not found in manifest'.format(col))\n",
    "        return\n",
    "    if col not in valid_dict.keys():\n",
    "        logging.error('{!r} column not found in validation sheet'.format(col))\n",
    "        return\n",
    "    assert level in ('i','w','e'), '{!r} invalid logging level for validate_values'.format(level)\n",
    "    \n",
    "    series = df[col]\n",
    "    series = exclude_missing(series, na_values)\n",
    "    \n",
    "    col_values = set(series.unique())\n",
    "    # use separator to split values\n",
    "    if sep:\n",
    "        sep_col_values = list()\n",
    "        for v in col_values:\n",
    "            sep_col_values.extend([x.strip() for x in v.split(sep)])\n",
    "        col_values = set(sep_col_values)\n",
    "    \n",
    "    valid_values = set(valid_dict[col])\n",
    "    \n",
    "    invalid_values = col_values - valid_values\n",
    "    \n",
    "    if len(invalid_values) > 0:\n",
    "        msg = 'invalid values in {!r}: {}'.format(col, invalid_values)\n",
    "        if level == 'i':\n",
    "            logging.info(msg)\n",
    "        elif level == 'w':\n",
    "            logging.warning(msg)\n",
    "        elif level == 'e':\n",
    "            logging.error(msg)\n",
    "#     else:\n",
    "#         logging.info('all values valid in {!r}'.format(col))\n",
    "            \n",
    "validate_values('ORGANISM_PART', df, valid_dict, sep=\" | \", na_values=[], level='e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] validating date column 'DATE_OF_COLLECTION'\n",
      "[INFO] excluding 0 [] samples without data in 'DATE_OF_COLLECTION'\n",
      "[ERROR] invalid dates in 'DATE_OF_COLLECTION': ['TEST' '']\n",
      "[ERROR] future dates in 'DATE_OF_COLLECTION': [Timestamp('2117-08-21 00:00:00')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SERIES\n",
       "1     2022-07-06\n",
       "2     2021-03-03\n",
       "3     2017-08-21\n",
       "4     2017-08-21\n",
       "5     2017-08-21\n",
       "         ...    \n",
       "284   2019-03-03\n",
       "285   2019-03-03\n",
       "286   2019-03-03\n",
       "287   2019-03-03\n",
       "288   2019-03-03\n",
       "Name: DATE_OF_COLLECTION, Length: 285, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def validate_date(col, df, na_values=[]):\n",
    "    \n",
    "    logging.info('validating date column {!r}'.format(col))\n",
    "\n",
    "    if col not in df.columns:\n",
    "        logging.error('{!r} column not found in manifest'.format(col))\n",
    "        return\n",
    "    series = df[col]\n",
    "    series = exclude_missing(series, na_values)\n",
    "    \n",
    "    # invalid date formats\n",
    "    # empty string converted to NaT\n",
    "    date_series = pd.to_datetime(series, format='%Y-%m-%d', errors='coerce')\n",
    "    if date_series.isna().any():\n",
    "        logging.error('invalid dates in {!r}: {}'.format(col, \n",
    "                                                         series[date_series.isna()].unique()))\n",
    "    valid_date_series = date_series[~date_series.isna()]\n",
    "    \n",
    "    # dates in future\n",
    "    future_dates = (valid_date_series > datetime.datetime.today())\n",
    "    if future_dates.any():\n",
    "        logging.error('future dates in {!r}: {}'.format(col,\n",
    "            valid_date_series[future_dates].to_list()))\n",
    "        \n",
    "    # dates too old\n",
    "    old_dates = (valid_date_series < datetime.datetime.strptime('1900-01-01', '%Y-%m-%d'))\n",
    "    if old_dates.any():\n",
    "        logging.error(\"pre-1900 dates in {!r}: {}\".format(col,\n",
    "            valid_date_series[old_dates].to_list())) \n",
    "    \n",
    "    return valid_date_series\n",
    "validate_date('DATE_OF_COLLECTION', df, na_values=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] validating time column 'TIME_OF_COLLECTION'\n",
      "[INFO] excluding 193 [''] samples without data in 'TIME_OF_COLLECTION'\n",
      "[ERROR] invalid times in 'TIME_OF_COLLECTION': ['\"33.00\"']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SERIES\n",
       "1    1900-01-01 18:00:00\n",
       "2    1900-01-01 22:43:00\n",
       "3    1900-01-01 04:00:00\n",
       "4    1900-01-01 04:00:00\n",
       "6    1900-01-01 04:00:00\n",
       "             ...        \n",
       "91   1900-01-01 09:00:00\n",
       "92   1900-01-01 09:00:00\n",
       "93   1900-01-01 09:00:00\n",
       "94   1900-01-01 09:00:00\n",
       "95   1900-01-01 09:00:00\n",
       "Name: TIME_OF_COLLECTION, Length: 94, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def validate_time(col, df, na_values=[]):\n",
    "    \n",
    "    logging.info('validating time column {!r}'.format(col))\n",
    "    \n",
    "    if col not in df.columns:\n",
    "        logging.error('{!r} column not found in manifest'.format(col))\n",
    "        return\n",
    "    series = df[col]\n",
    "    series = exclude_missing(series, na_values)\n",
    "        \n",
    "    # invalid time formats\n",
    "    # NB empty string converted to NaT\n",
    "    time_series = pd.to_datetime(series, format='%H:%M:%S', errors='coerce')\n",
    "    if time_series.isna().any():\n",
    "        logging.error('invalid times in {!r}: {}'.format(col, \n",
    "                                                         series[time_series.isna()].unique()))\n",
    "    valid_time_series = time_series[~time_series.isna()]\n",
    "    \n",
    "    return valid_time_series\n",
    "validate_time('TIME_OF_COLLECTION', df, na_values=[''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] validating time period column 'DURATION_OF_COLLECTION'\n",
      "[INFO] excluding 194 [''] samples without data in 'DURATION_OF_COLLECTION'\n",
      "[ERROR] invalid times in 'DURATION_OF_COLLECTION': ['P1WT1H' 'TEST']\n"
     ]
    }
   ],
   "source": [
    "def validate_time_period(col, df, na_values=[]):\n",
    "    \n",
    "    logging.info('validating time period column {!r}'.format(col))\n",
    "    \n",
    "    if col not in df.columns:\n",
    "        logging.error('{!r} column not found in manifest'.format(col))\n",
    "        return\n",
    "    series = df[col]\n",
    "    series = exclude_missing(series, na_values)\n",
    "\n",
    "    # conversion with modifications for proper parsing \n",
    "    # by pd.Timedelta (does not accept missing data, e.g. 'PT1H')\n",
    "    # note - will not work for weeks and months\n",
    "    def convert_iso_duration(s):\n",
    "        if s == np.nan:\n",
    "            return np.nan\n",
    "        if not s.startswith('P') or 'T' not in s:\n",
    "            return np.nan\n",
    "        # add days\n",
    "        if s.startswith('PT'):\n",
    "            s = s.replace('PT','P0DT')\n",
    "        # add trailing minutes and seconds\n",
    "        if s.endswith('H'):\n",
    "            s += '0M0S'\n",
    "        elif s.endswith('M'):\n",
    "            s += '0S'\n",
    "        try:\n",
    "            return pd.Timedelta(s)\n",
    "        except:\n",
    "            return np.nan\n",
    "    time_period_series = series.apply(convert_iso_duration)\n",
    "    if time_period_series.isna().any():\n",
    "        logging.error('invalid times in {!r}: {}'.format(col, \n",
    "            series[time_period_series.isna()].unique()))\n",
    "    valid_time_period_series = time_period_series[~time_period_series.isna()]\n",
    "    return valid_time_period_series\n",
    "\n",
    "# df.loc[1,'DURATION_OF_COLLECTION'] = 'PVT1H'\n",
    "validate_time_period('DURATION_OF_COLLECTION', df, na_values=['']);\n",
    "# df['DURATION_OF_COLLECTION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] validating country with coordinates\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COLLECTION_COUNTRY</th>\n",
       "      <th>DECIMAL_LATITUDE</th>\n",
       "      <th>DECIMAL_LONGITUDE</th>\n",
       "      <th>coord</th>\n",
       "      <th>partner_country</th>\n",
       "      <th>coord_country</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SERIES</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UNITED KINGDOM</td>\n",
       "      <td>52.0193</td>\n",
       "      <td>0.2424</td>\n",
       "      <td>52.0193, 0.2424</td>\n",
       "      <td>UNITED KINGDOM</td>\n",
       "      <td>UNITED KINGDOM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GABON</td>\n",
       "      <td>-0.218</td>\n",
       "      <td>11.584</td>\n",
       "      <td>-0.218, 11.584</td>\n",
       "      <td>GABON</td>\n",
       "      <td>GABON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BURKINA FASO</td>\n",
       "      <td>11.2339822</td>\n",
       "      <td>-4.4725306</td>\n",
       "      <td>11.2339822, -4.4725306</td>\n",
       "      <td>BURKINA FASO</td>\n",
       "      <td>BURKINA FASO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BURKINA FASO</td>\n",
       "      <td>11.2339822</td>\n",
       "      <td>-4.4725306</td>\n",
       "      <td>11.2339822, -4.4725306</td>\n",
       "      <td>BURKINA FASO</td>\n",
       "      <td>BURKINA FASO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BURKINA FASO</td>\n",
       "      <td>11.2339822</td>\n",
       "      <td>-4.4725306</td>\n",
       "      <td>11.2339822, -4.4725306</td>\n",
       "      <td>BURKINA FASO</td>\n",
       "      <td>BURKINA FASO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>UNITED KINGDOM</td>\n",
       "      <td>52.0193</td>\n",
       "      <td>0.2424</td>\n",
       "      <td>52.0193, 0.2424</td>\n",
       "      <td>UNITED KINGDOM</td>\n",
       "      <td>UNITED KINGDOM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>UNITED KINGDOM</td>\n",
       "      <td>52.0193</td>\n",
       "      <td>0.2424</td>\n",
       "      <td>52.0193, 0.2424</td>\n",
       "      <td>UNITED KINGDOM</td>\n",
       "      <td>UNITED KINGDOM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>UNITED KINGDOM</td>\n",
       "      <td>52.0193</td>\n",
       "      <td>0.2424</td>\n",
       "      <td>52.0193, 0.2424</td>\n",
       "      <td>UNITED KINGDOM</td>\n",
       "      <td>UNITED KINGDOM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>UNITED KINGDOM</td>\n",
       "      <td>52.0193</td>\n",
       "      <td>0.2424</td>\n",
       "      <td>52.0193, 0.2424</td>\n",
       "      <td>UNITED KINGDOM</td>\n",
       "      <td>UNITED KINGDOM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>UNITED KINGDOM</td>\n",
       "      <td>52.0193</td>\n",
       "      <td>0.2424</td>\n",
       "      <td>52.0193, 0.2424</td>\n",
       "      <td>UNITED KINGDOM</td>\n",
       "      <td>UNITED KINGDOM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>288 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       COLLECTION_COUNTRY DECIMAL_LATITUDE DECIMAL_LONGITUDE  \\\n",
       "SERIES                                                         \n",
       "1          UNITED KINGDOM          52.0193            0.2424   \n",
       "2                   GABON           -0.218            11.584   \n",
       "3            BURKINA FASO       11.2339822        -4.4725306   \n",
       "4            BURKINA FASO       11.2339822        -4.4725306   \n",
       "5            BURKINA FASO       11.2339822        -4.4725306   \n",
       "...                   ...              ...               ...   \n",
       "284        UNITED KINGDOM          52.0193            0.2424   \n",
       "285        UNITED KINGDOM          52.0193            0.2424   \n",
       "286        UNITED KINGDOM          52.0193            0.2424   \n",
       "287        UNITED KINGDOM          52.0193            0.2424   \n",
       "288        UNITED KINGDOM          52.0193            0.2424   \n",
       "\n",
       "                         coord partner_country   coord_country  \n",
       "SERIES                                                          \n",
       "1              52.0193, 0.2424  UNITED KINGDOM  UNITED KINGDOM  \n",
       "2               -0.218, 11.584           GABON           GABON  \n",
       "3       11.2339822, -4.4725306    BURKINA FASO    BURKINA FASO  \n",
       "4       11.2339822, -4.4725306    BURKINA FASO    BURKINA FASO  \n",
       "5       11.2339822, -4.4725306    BURKINA FASO    BURKINA FASO  \n",
       "...                        ...             ...             ...  \n",
       "284            52.0193, 0.2424  UNITED KINGDOM  UNITED KINGDOM  \n",
       "285            52.0193, 0.2424  UNITED KINGDOM  UNITED KINGDOM  \n",
       "286            52.0193, 0.2424  UNITED KINGDOM  UNITED KINGDOM  \n",
       "287            52.0193, 0.2424  UNITED KINGDOM  UNITED KINGDOM  \n",
       "288            52.0193, 0.2424  UNITED KINGDOM  UNITED KINGDOM  \n",
       "\n",
       "[288 rows x 6 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO include tests\n",
    "def validate_country_and_coordinates(df, fn, na_values=[], bioscan=False):\n",
    "    \n",
    "    logging.info('validating country with coordinates')\n",
    "    \n",
    "    if bioscan:\n",
    "        country_col, lat_col, lon_col = 'COUNTRY_OF_COLLECTION', 'DECIMAL_LATITUDE', 'DECIMAL_LONGITUDE'\n",
    "    else:\n",
    "        country_col, lat_col, lon_col = 'COLLECTION_COUNTRY', 'DECIMAL_LATITUDE', 'DECIMAL_LONGITUDE'\n",
    "\n",
    "    try:\n",
    "        loc_df_complete = df[[country_col, lat_col, lon_col]].copy()\n",
    "    except:\n",
    "        logging.error('One of {!r} {!r} {!r} columns not found in manifest'.format(country_col, lat_col, lon_col))\n",
    "        return\n",
    "    loc_df_isna = (loc_df_complete.isin(na_values)).all(axis=1)\n",
    "    if loc_df_isna.any():\n",
    "        logging.info('removing {} {!r} samples with missing data from coordinate analysis'.format(\n",
    "                loc_df_isna.sum(), na_values))\n",
    "    loc_df_complete = loc_df_complete[~loc_df_isna].copy()\n",
    "    \n",
    "    # coordinates in geopy format\n",
    "    loc_df_complete['coord'] = loc_df_complete.apply(lambda x: '{}, {}'.format(\n",
    "            x[lat_col], x[lon_col]), axis=1)\n",
    "    \n",
    "    # get location data for coordinates\n",
    "    # use local copy of web query results for re-runs\n",
    "    # this \n",
    "    loc_fn = fn+'_loc.pkl'\n",
    "    if os.path.isfile(loc_fn):\n",
    "        locations = pickle.load(open(loc_fn, \"rb\"))\n",
    "    else:\n",
    "        # web map server - openstreetmaps\n",
    "        logging.info('querying coordinates')\n",
    "        locator = Nominatim(user_agent='myGeocoder')\n",
    "        rgeocode = RateLimiter(locator.reverse, min_delay_seconds=1)\n",
    "\n",
    "        locations = dict()\n",
    "        for c in loc_df_complete.coord.unique():\n",
    "            # pre-fill with unknown country\n",
    "            locations[c] = {'address':{'country':'UNKNOWN'}}\n",
    "            # check coordniate correctness\n",
    "            try:\n",
    "                lat, lon = c.split(', ')\n",
    "                lat, lon = float(lat), float(lon)\n",
    "            except:\n",
    "                logging.error('problem parsing coordinates {!r}'.format(c))\n",
    "                continue\n",
    "            if abs(lat) > 90:\n",
    "                logging.error('invalid latitude {}, should be in [-90,90]'.format(lat))\n",
    "                continue\n",
    "            if abs(lon) > 180:\n",
    "                logging.error('invalid longitude {}, should be in [-180,180]'.format(lon))\n",
    "                continue\n",
    "            # web query\n",
    "            location = rgeocode(c, language='en-gb')\n",
    "            # rgeocode returns empty location outside of counries and in some other situations\n",
    "            if location is not None:\n",
    "                locations[c] = location.raw\n",
    "\n",
    "        # save locations to file\n",
    "        pickle.dump(locations, open(loc_fn, \"wb\"))\n",
    "        \n",
    "    # parse country from partner input\n",
    "    loc_df_complete['partner_country'] = loc_df_complete[country_col].str.strip().str.upper()\n",
    "    \n",
    "    # extract countries from location data\n",
    "    loc_countries = dict()\n",
    "    for coord in locations.keys():\n",
    "        coord_country = locations[coord]['address']['country'].upper()\n",
    "        loc_countries[coord] = coord_country\n",
    "        \n",
    "        partner_countries = loc_df_complete.loc[loc_df_complete.coord == coord, 'partner_country']\n",
    "        if partner_countries.nunique() > 1:\n",
    "            logging.error('multiple partner countries for coordinates {!r}: {}'\n",
    "                          'skipping coordinate validation'.format(\n",
    "                                coord, partner_countries.unique()))\n",
    "            continue\n",
    "        if partner_countries.shape[0] == 0:\n",
    "            logging.error('no partner location found for coordinates {!r}'.format(coord))\n",
    "            continue\n",
    "        partner_country = partner_countries.iloc[0]\n",
    "        if coord_country == 'UNKNOWN':\n",
    "            logging.warning('could not locate country for coordinates {!r}, partner country {!r}'.format(\n",
    "                    coord, partner_country))\n",
    "        elif partner_country != coord_country:\n",
    "            logging.error('country mismatch for coordinates {!r}, partner country {!r}, '\n",
    "                          'coordinate country {!r}'.format(coord, partner_country, coord_country))\n",
    "    \n",
    "    # countries based on coordinates\n",
    "    loc_df_complete['coord_country'] = loc_df_complete['coord'].replace(loc_countries)\n",
    "    country_mismatch = (loc_df_complete.coord_country != loc_df_complete.partner_country)\n",
    "\n",
    "#     if country_mismatch.any():\n",
    "#         logging.error('coordinates do not match country for SERIES: {}'.format(\n",
    "#                 country_mismatch[country_mismatch].index.to_list()))\n",
    "    \n",
    "    # location data can be re-used, e.g. as an additional field\n",
    "    return loc_df_complete\n",
    "# df.loc[2,'DECIMAL_LATITUDE'] = '65'\n",
    "loc_test = validate_country_and_coordinates(df, anospp_fn)\n",
    "loc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] validating taxonomy against NCBI\n",
      "[INFO] validating PREDICTED_ORDER_OR_GROUP against NCBI\n",
      "[INFO] validating PREDICTED_FAMILY against NCBI\n",
      "[INFO] validating PREDICTED_GENUS against NCBI\n",
      "[INFO] genus: using only first matching rank for Anopheles (taxid 7164): genus\n",
      "[INFO] validating PREDICTED_SCIENTIFIC_NAME against NCBI\n",
      "[ERROR] species: {'', 'Anopheles stricklandi', 'Anopheles test'} not found in NCBI Taxonomy\n",
      "[ERROR] species: \"Anopheles stricklandi\" found in Harbach list, but not in NCBI Taxonomy\n",
      "[INFO] cannot validate PREDICTED_SCIENTIFIC_NAME for \"Anopheles test\", skipping taxonomy consistency check\n",
      "[INFO] cannot validate PREDICTED_SCIENTIFIC_NAME for \"Anopheles stricklandi\", skipping taxonomy consistency check\n",
      "[INFO] cannot validate PREDICTED_SCIENTIFIC_NAME for \"\", skipping taxonomy consistency check\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RACK_OR_PLATE_ID</th>\n",
       "      <th>TUBE_OR_WELL_ID</th>\n",
       "      <th>PRESERVATIVE_SOLUTION</th>\n",
       "      <th>ORGANISM_PART</th>\n",
       "      <th>DATE_OF_COLLECTION</th>\n",
       "      <th>COLLECTION_COUNTRY</th>\n",
       "      <th>COLLECTION_LOCATION</th>\n",
       "      <th>DECIMAL_LATITUDE</th>\n",
       "      <th>DECIMAL_LONGITUDE</th>\n",
       "      <th>SAMPLING_LOCATION_SIZE</th>\n",
       "      <th>...</th>\n",
       "      <th>DNA_EXTRACTION_DESCRIPTION</th>\n",
       "      <th>DNA_EXTRACT_VOLUME_PROVIDED</th>\n",
       "      <th>DNA_EXTRACT_CONCENTRATION</th>\n",
       "      <th>PREDICTED_ORDER_OR_GROUP</th>\n",
       "      <th>PREDICTED_FAMILY</th>\n",
       "      <th>PREDICTED_GENUS</th>\n",
       "      <th>PREDICTED_ORDER_OR_GROUP_TAXID</th>\n",
       "      <th>PREDICTED_FAMILY_TAXID</th>\n",
       "      <th>PREDICTED_GENUS_TAXID</th>\n",
       "      <th>PREDICTED_SCIENTIFIC_NAME_TAXID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SERIES</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DACH_001</td>\n",
       "      <td>A1</td>\n",
       "      <td>100%_ETHANOL</td>\n",
       "      <td>WHOLE_ORGANISM</td>\n",
       "      <td>2022-07-06 00:00:00</td>\n",
       "      <td>UNITED KINGDOM</td>\n",
       "      <td>ENGLAND | ESSEX | UTTLESFORD | SAFFRON WALDEN ...</td>\n",
       "      <td>52.0193</td>\n",
       "      <td>0.2424</td>\n",
       "      <td>1m²</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Diptera</td>\n",
       "      <td>Culicidae</td>\n",
       "      <td>Anopheles</td>\n",
       "      <td>7147</td>\n",
       "      <td>7157</td>\n",
       "      <td>7164</td>\n",
       "      <td>227531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DACH_001</td>\n",
       "      <td>B1</td>\n",
       "      <td>100%_ETHANOL</td>\n",
       "      <td>WHOLE_ORGANISM</td>\n",
       "      <td>2021-03-03 00:00:00</td>\n",
       "      <td>GABON</td>\n",
       "      <td>LOPE NATIONAL PARK</td>\n",
       "      <td>-0.218</td>\n",
       "      <td>11.584</td>\n",
       "      <td>100m²</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Diptera</td>\n",
       "      <td>Culicidae</td>\n",
       "      <td>Anopheles</td>\n",
       "      <td>7147</td>\n",
       "      <td>7157</td>\n",
       "      <td>7164</td>\n",
       "      <td>7165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DACH_001</td>\n",
       "      <td>C1</td>\n",
       "      <td>100%_ETHANOL</td>\n",
       "      <td>THORAX | ABDOMEN</td>\n",
       "      <td>2017-08-21 00:00:00</td>\n",
       "      <td>BURKINA FASO</td>\n",
       "      <td>HAUTS-BASSINS | HOEUT | ARRONDISSEMENT N°7 DE ...</td>\n",
       "      <td>11.2339822</td>\n",
       "      <td>-4.4725306</td>\n",
       "      <td>1km²</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Diptera</td>\n",
       "      <td>Culicidae</td>\n",
       "      <td>Anopheles</td>\n",
       "      <td>7147</td>\n",
       "      <td>7157</td>\n",
       "      <td>7164</td>\n",
       "      <td>1518534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DACH_001</td>\n",
       "      <td>D1</td>\n",
       "      <td>100%_ETHANOL</td>\n",
       "      <td>WHOLE_ORGANISM</td>\n",
       "      <td>2017-08-21 00:00:00</td>\n",
       "      <td>BURKINA FASO</td>\n",
       "      <td>HAUTS-BASSINS | HOEUT | ARRONDISSEMENT N°7 DE ...</td>\n",
       "      <td>11.2339822</td>\n",
       "      <td>-4.4725306</td>\n",
       "      <td>1km²</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Diptera</td>\n",
       "      <td>Culicidae</td>\n",
       "      <td>Anopheles</td>\n",
       "      <td>7147</td>\n",
       "      <td>7157</td>\n",
       "      <td>7164</td>\n",
       "      <td>Anopheles test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DACH_001</td>\n",
       "      <td>E1</td>\n",
       "      <td>100%_ETHANOL</td>\n",
       "      <td>WHOLE_ORGANISM</td>\n",
       "      <td>2017-08-21 00:00:00</td>\n",
       "      <td>BURKINA FASO</td>\n",
       "      <td>HAUTS-BASSINS | HOEUT | ARRONDISSEMENT N°7 DE ...</td>\n",
       "      <td>11.2339822</td>\n",
       "      <td>-4.4725306</td>\n",
       "      <td>1m²</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Diptera</td>\n",
       "      <td>Culicidae</td>\n",
       "      <td>Anopheles</td>\n",
       "      <td>7147</td>\n",
       "      <td>7157</td>\n",
       "      <td>7164</td>\n",
       "      <td>Anopheles stricklandi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>DACH_003</td>\n",
       "      <td>D12</td>\n",
       "      <td>100%_ETHANOL</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>2019-03-03 00:00:00</td>\n",
       "      <td>UNITED KINGDOM</td>\n",
       "      <td>ENGLAND | ESSEX | UTTLESFORD | SAFFRON WALDEN ...</td>\n",
       "      <td>52.0193</td>\n",
       "      <td>0.2424</td>\n",
       "      <td>10m²</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Diptera</td>\n",
       "      <td>Culicidae</td>\n",
       "      <td>Anopheles</td>\n",
       "      <td>7147</td>\n",
       "      <td>7157</td>\n",
       "      <td>7164</td>\n",
       "      <td>227531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>DACH_003</td>\n",
       "      <td>E12</td>\n",
       "      <td>100%_ETHANOL</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>2019-03-03 00:00:00</td>\n",
       "      <td>UNITED KINGDOM</td>\n",
       "      <td>ENGLAND | ESSEX | UTTLESFORD | SAFFRON WALDEN ...</td>\n",
       "      <td>52.0193</td>\n",
       "      <td>0.2424</td>\n",
       "      <td>10m²</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Diptera</td>\n",
       "      <td>Culicidae</td>\n",
       "      <td>Anopheles</td>\n",
       "      <td>7147</td>\n",
       "      <td>7157</td>\n",
       "      <td>7164</td>\n",
       "      <td>227531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>DACH_003</td>\n",
       "      <td>F12</td>\n",
       "      <td>100%_ETHANOL</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>2019-03-03 00:00:00</td>\n",
       "      <td>UNITED KINGDOM</td>\n",
       "      <td>ENGLAND | ESSEX | UTTLESFORD | SAFFRON WALDEN ...</td>\n",
       "      <td>52.0193</td>\n",
       "      <td>0.2424</td>\n",
       "      <td>10m²</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Diptera</td>\n",
       "      <td>Culicidae</td>\n",
       "      <td>Anopheles</td>\n",
       "      <td>7147</td>\n",
       "      <td>7157</td>\n",
       "      <td>7164</td>\n",
       "      <td>227531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>DACH_003</td>\n",
       "      <td>G12</td>\n",
       "      <td>100%_ETHANOL</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>2019-03-03 00:00:00</td>\n",
       "      <td>UNITED KINGDOM</td>\n",
       "      <td>ENGLAND | ESSEX | UTTLESFORD | SAFFRON WALDEN ...</td>\n",
       "      <td>52.0193</td>\n",
       "      <td>0.2424</td>\n",
       "      <td>10m²</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Diptera</td>\n",
       "      <td>Culicidae</td>\n",
       "      <td>Anopheles</td>\n",
       "      <td>7147</td>\n",
       "      <td>7157</td>\n",
       "      <td>7164</td>\n",
       "      <td>227531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>DACH_003</td>\n",
       "      <td>H12</td>\n",
       "      <td>100%_ETHANOL</td>\n",
       "      <td>TEST</td>\n",
       "      <td>2019-03-03 00:00:00</td>\n",
       "      <td>UNITED KINGDOM</td>\n",
       "      <td>ENGLAND | ESSEX | UTTLESFORD | SAFFRON WALDEN ...</td>\n",
       "      <td>52.0193</td>\n",
       "      <td>0.2424</td>\n",
       "      <td>10m²</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Diptera</td>\n",
       "      <td>Culicidae</td>\n",
       "      <td>Anopheles</td>\n",
       "      <td>7147</td>\n",
       "      <td>7157</td>\n",
       "      <td>7164</td>\n",
       "      <td>227531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>288 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       RACK_OR_PLATE_ID TUBE_OR_WELL_ID PRESERVATIVE_SOLUTION  \\\n",
       "SERIES                                                          \n",
       "1              DACH_001              A1          100%_ETHANOL   \n",
       "2              DACH_001              B1          100%_ETHANOL   \n",
       "3              DACH_001              C1          100%_ETHANOL   \n",
       "4              DACH_001              D1          100%_ETHANOL   \n",
       "5              DACH_001              E1          100%_ETHANOL   \n",
       "...                 ...             ...                   ...   \n",
       "284            DACH_003             D12          100%_ETHANOL   \n",
       "285            DACH_003             E12          100%_ETHANOL   \n",
       "286            DACH_003             F12          100%_ETHANOL   \n",
       "287            DACH_003             G12          100%_ETHANOL   \n",
       "288            DACH_003             H12          100%_ETHANOL   \n",
       "\n",
       "           ORGANISM_PART   DATE_OF_COLLECTION COLLECTION_COUNTRY  \\\n",
       "SERIES                                                             \n",
       "1         WHOLE_ORGANISM  2022-07-06 00:00:00     UNITED KINGDOM   \n",
       "2         WHOLE_ORGANISM  2021-03-03 00:00:00              GABON   \n",
       "3       THORAX | ABDOMEN  2017-08-21 00:00:00       BURKINA FASO   \n",
       "4         WHOLE_ORGANISM  2017-08-21 00:00:00       BURKINA FASO   \n",
       "5         WHOLE_ORGANISM  2017-08-21 00:00:00       BURKINA FASO   \n",
       "...                  ...                  ...                ...   \n",
       "284                 HEAD  2019-03-03 00:00:00     UNITED KINGDOM   \n",
       "285                 HEAD  2019-03-03 00:00:00     UNITED KINGDOM   \n",
       "286                 HEAD  2019-03-03 00:00:00     UNITED KINGDOM   \n",
       "287                 HEAD  2019-03-03 00:00:00     UNITED KINGDOM   \n",
       "288                 TEST  2019-03-03 00:00:00     UNITED KINGDOM   \n",
       "\n",
       "                                      COLLECTION_LOCATION DECIMAL_LATITUDE  \\\n",
       "SERIES                                                                       \n",
       "1       ENGLAND | ESSEX | UTTLESFORD | SAFFRON WALDEN ...          52.0193   \n",
       "2                                      LOPE NATIONAL PARK           -0.218   \n",
       "3       HAUTS-BASSINS | HOEUT | ARRONDISSEMENT N°7 DE ...       11.2339822   \n",
       "4       HAUTS-BASSINS | HOEUT | ARRONDISSEMENT N°7 DE ...       11.2339822   \n",
       "5       HAUTS-BASSINS | HOEUT | ARRONDISSEMENT N°7 DE ...       11.2339822   \n",
       "...                                                   ...              ...   \n",
       "284     ENGLAND | ESSEX | UTTLESFORD | SAFFRON WALDEN ...          52.0193   \n",
       "285     ENGLAND | ESSEX | UTTLESFORD | SAFFRON WALDEN ...          52.0193   \n",
       "286     ENGLAND | ESSEX | UTTLESFORD | SAFFRON WALDEN ...          52.0193   \n",
       "287     ENGLAND | ESSEX | UTTLESFORD | SAFFRON WALDEN ...          52.0193   \n",
       "288     ENGLAND | ESSEX | UTTLESFORD | SAFFRON WALDEN ...          52.0193   \n",
       "\n",
       "       DECIMAL_LONGITUDE SAMPLING_LOCATION_SIZE  ...  \\\n",
       "SERIES                                           ...   \n",
       "1                 0.2424                    1m²  ...   \n",
       "2                 11.584                  100m²  ...   \n",
       "3             -4.4725306                   1km²  ...   \n",
       "4             -4.4725306                   1km²  ...   \n",
       "5             -4.4725306                    1m²  ...   \n",
       "...                  ...                    ...  ...   \n",
       "284               0.2424                   10m²  ...   \n",
       "285               0.2424                   10m²  ...   \n",
       "286               0.2424                   10m²  ...   \n",
       "287               0.2424                   10m²  ...   \n",
       "288               0.2424                   10m²  ...   \n",
       "\n",
       "       DNA_EXTRACTION_DESCRIPTION DNA_EXTRACT_VOLUME_PROVIDED  \\\n",
       "SERIES                                                          \n",
       "1                                                               \n",
       "2                                                               \n",
       "3                                                               \n",
       "4                                                               \n",
       "5                                                               \n",
       "...                           ...                         ...   \n",
       "284                                                             \n",
       "285                                                             \n",
       "286                                                             \n",
       "287                                                             \n",
       "288                                                             \n",
       "\n",
       "       DNA_EXTRACT_CONCENTRATION PREDICTED_ORDER_OR_GROUP PREDICTED_FAMILY  \\\n",
       "SERIES                                                                       \n",
       "1                                                 Diptera        Culicidae   \n",
       "2                                                 Diptera        Culicidae   \n",
       "3                                                 Diptera        Culicidae   \n",
       "4                                                 Diptera        Culicidae   \n",
       "5                                                 Diptera        Culicidae   \n",
       "...                          ...                      ...              ...   \n",
       "284                                               Diptera        Culicidae   \n",
       "285                                               Diptera        Culicidae   \n",
       "286                                               Diptera        Culicidae   \n",
       "287                                               Diptera        Culicidae   \n",
       "288                                               Diptera        Culicidae   \n",
       "\n",
       "       PREDICTED_GENUS PREDICTED_ORDER_OR_GROUP_TAXID PREDICTED_FAMILY_TAXID  \\\n",
       "SERIES                                                                         \n",
       "1            Anopheles                           7147                   7157   \n",
       "2            Anopheles                           7147                   7157   \n",
       "3            Anopheles                           7147                   7157   \n",
       "4            Anopheles                           7147                   7157   \n",
       "5            Anopheles                           7147                   7157   \n",
       "...                ...                            ...                    ...   \n",
       "284          Anopheles                           7147                   7157   \n",
       "285          Anopheles                           7147                   7157   \n",
       "286          Anopheles                           7147                   7157   \n",
       "287          Anopheles                           7147                   7157   \n",
       "288          Anopheles                           7147                   7157   \n",
       "\n",
       "       PREDICTED_GENUS_TAXID PREDICTED_SCIENTIFIC_NAME_TAXID  \n",
       "SERIES                                                        \n",
       "1                       7164                          227531  \n",
       "2                       7164                            7165  \n",
       "3                       7164                         1518534  \n",
       "4                       7164                  Anopheles test  \n",
       "5                       7164           Anopheles stricklandi  \n",
       "...                      ...                             ...  \n",
       "284                     7164                          227531  \n",
       "285                     7164                          227531  \n",
       "286                     7164                          227531  \n",
       "287                     7164                          227531  \n",
       "288                     7164                          227531  \n",
       "\n",
       "[288 rows x 42 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO hierarchy tests in BIOSCAN\n",
    "def validate_taxonomy(df, ncbi, na_values = [], anospp=False):\n",
    "\n",
    "    logging.info('validating taxonomy against NCBI')\n",
    "    \n",
    "    if anospp:\n",
    "        df['PREDICTED_ORDER_OR_GROUP'] = 'Diptera'\n",
    "        df['PREDICTED_FAMILY'] = 'Culicidae'\n",
    "        df['PREDICTED_GENUS'] = 'Anopheles'\n",
    "        \n",
    "        harbach_spp = []\n",
    "        with open('../data/harbach_spp_201910.txt') as f:\n",
    "            for line in f:\n",
    "                harbach_spp.append('Anopheles ' + line.strip())\n",
    "        harbach_spp = set(harbach_spp)\n",
    "        \n",
    "    tax_levels = {\n",
    "        'PREDICTED_ORDER_OR_GROUP':'order',\n",
    "        'PREDICTED_FAMILY':'family',\n",
    "        'PREDICTED_GENUS':'genus',\n",
    "        'PREDICTED_SCIENTIFIC_NAME':'species'\n",
    "    }\n",
    "    \n",
    "    hierarchies = df[tax_levels.keys()].drop_duplicates().copy()\n",
    "    \n",
    "    hierarchies.columns = list(tax_levels.values())\n",
    "        \n",
    "    tax_info = dict()\n",
    "    \n",
    "    for tax_col, tax_level in tax_levels.items():\n",
    "        \n",
    "        logging.info(f'validating {tax_col} against NCBI')\n",
    "        \n",
    "        if tax_col not in df.columns:\n",
    "                logging.error(f'{tax_col} column not found in manifest')\n",
    "                continue\n",
    "            \n",
    "        tax_names = list(hierarchies[tax_level].unique())\n",
    "        \n",
    "        for na_value in na_values:\n",
    "            try:\n",
    "                tax_names.remove(na_value)\n",
    "            except:\n",
    "                pass \n",
    "            \n",
    "        for i, tax_name in enumerate(tax_names):\n",
    "            if len(tax_name) == 0:\n",
    "                continue\n",
    "            corr_tax_name = tax_name[0].upper() + tax_name[1:].lower()\n",
    "            if corr_tax_name != tax_name and tax_name != 'blank sample':\n",
    "                logging.error(f'{tax_level}: unexpected case for \"{tax_name}\", '\n",
    "                              f'changing to \"{corr_tax_name}\" for validation')\n",
    "            tax_names[i] = corr_tax_name\n",
    "        \n",
    "        tax_info[tax_level] = ncbi.get_name_translator(tax_names) \n",
    "        \n",
    "        unmatched_names = set(tax_names) - set(tax_info[tax_level].keys())\n",
    "        if len(unmatched_names) > 0:\n",
    "            logging.error(f'{tax_level}: {unmatched_names} not found in NCBI Taxonomy')\n",
    "            if tax_level == 'species' and anospp:\n",
    "                for sp in unmatched_names:\n",
    "                    if sp in harbach_spp:\n",
    "                        logging.error(f'{tax_level}: \"{sp}\" found in Harbach list, but not in NCBI Taxonomy')\n",
    "        \n",
    "        expected_rank = tax_level\n",
    "        \n",
    "        for tname, tids in tax_info[tax_level].items():\n",
    "            \n",
    "            ranks = ncbi.get_rank(tids)\n",
    "            \n",
    "            upd_tid = tids[0]\n",
    "            \n",
    "            if len(tids) == 1:\n",
    "                if ranks[upd_tid] != expected_rank: \n",
    "                    # TODO warning->info for ORDER\n",
    "                    logging.warning(f'{tax_level}: found unexpected rank for {tname} (taxid {upd_tid}): {ranks[upd_tid]}')\n",
    "            if len(tids) > 1:            \n",
    "                for tid, r in ranks.items():\n",
    "                    if r == expected_rank and len(tids) > 1:\n",
    "                        logging.info(f'{tax_level}: using only first matching rank for {tname} (taxid {tid}): {r}')\n",
    "                        upd_tid = tid\n",
    "                        break\n",
    "                else:\n",
    "                    logging.warning(f'{tax_level}: could not find matching rank for {tname}, '\n",
    "                                    f'using (taxid {upd_tid}): {ranks[upd_tid]}')\n",
    "                    \n",
    "            tax_info[tax_level][tname] = upd_tid\n",
    "        \n",
    "        #logging.info(f'{tax_level} {tax_info[tax_level]}')\n",
    "                    \n",
    "    # check consistency of taxonomy\n",
    "    for _, r in hierarchies.iterrows():\n",
    "        \n",
    "        if r.order in na_values:\n",
    "            continue\n",
    "        try:\n",
    "            order_id = tax_info['order'][r.order]\n",
    "        except KeyError:\n",
    "            logging.info(f'cannot validate PREDICTED_ORDER_OR_GROUP for \"{r.order}\", skipping taxonomy consistency check')\n",
    "            continue\n",
    "            \n",
    "        if r.genus in na_values:\n",
    "            continue\n",
    "        try:\n",
    "            family_id = tax_info['family'][r.family]\n",
    "            \n",
    "            family_lineage = ncbi.get_lineage(family_id)\n",
    "            \n",
    "            if order_id not in family_lineage:\n",
    "                logging.error(f'Family {r.family} (taxid {family_id}) does not belong to {r.order} (taxid {order_id})')\n",
    "        except KeyError:\n",
    "            logging.info(f'cannot validate PREDICTED_FAMILY for \"{r.family}\", skipping taxonomy consistency check')\n",
    "            continue\n",
    "            \n",
    "        if r.genus in na_values:\n",
    "            continue\n",
    "        try:\n",
    "            genus_id = tax_info['genus'][r.genus]\n",
    "            \n",
    "            genus_lineage = ncbi.get_lineage(genus_id)\n",
    "            \n",
    "            if order_id not in genus_lineage:\n",
    "                logging.error(f'Genus {r.genus} (taxid {genus_id}) does not belong to {r.order} (taxid {order_id})')\n",
    "            if family_id not in genus_lineage:\n",
    "                logging.error(f'Genus {r.genus} (taxid {genus_id}) does not belong to {r.family} (taxid {family_id})')\n",
    "        except KeyError:\n",
    "            logging.info(f'cannot validate PREDICTED_GENUS for \"{r.genus}\", skipping taxonomy consistency check')\n",
    "            continue\n",
    "            \n",
    "        if r.species in na_values:\n",
    "            continue\n",
    "        try:\n",
    "            species_id = tax_info['species'][r.species]\n",
    "            \n",
    "            species_lineage = ncbi.get_lineage(species_id)\n",
    "            \n",
    "            if order_id not in species_lineage:\n",
    "                logging.error(f'Species {r.species} (taxid {species_id}) does not belong to {r.order} (taxid {order_id})')\n",
    "            if family_id not in species_lineage:\n",
    "                logging.error(f'Species {r.species} (taxid {species_id}) does not belong to {r.family} (taxid {family_id})')\n",
    "            if genus_id not in species_lineage:\n",
    "                logging.error(f'Species {r.species} (taxid {species_id}) does not belong to {r.family} (taxid {genus_id})')\n",
    "        except KeyError:\n",
    "            logging.info(f'cannot validate PREDICTED_SCIENTIFIC_NAME for \"{r.species}\", skipping taxonomy consistency check')\n",
    "            continue\n",
    "            \n",
    "    for tc in tax_levels.keys():\n",
    "        df[f'{tc}_TAXID'] = df[tc].replace(tax_info[tax_levels[tc]])\n",
    "            \n",
    "    return df\n",
    "        \n",
    "                \n",
    "validate_taxonomy(df, ncbi, anospp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] validating SPECIMEN_IDENTITY_RISK\n",
      "[ERROR] SPECIMEN_IDENTITY_RISK should be Y in SERIES [8]\n"
     ]
    }
   ],
   "source": [
    "def validate_specimen_id_risk(df):\n",
    "    \n",
    "    logging.info(f'validating SPECIMEN_IDENTITY_RISK')\n",
    "    \n",
    "    if 'SPECIMEN_IDENTITY_RISK' not in df.columns:\n",
    "        logging.error(f'SPECIMEN_IDENTITY_RISK column not found in manifest')\n",
    "        return\n",
    "    \n",
    "    # missing species name, but no idenitity risk\n",
    "    invalid_risk = ((df.PREDICTED_SCIENTIFIC_NAME == '') & (df.SPECIMEN_IDENTITY_RISK == 'N'))\n",
    "    \n",
    "    if invalid_risk.any():\n",
    "        logging.error(f'SPECIMEN_IDENTITY_RISK should be Y in SERIES {df.loc[invalid_risk].index.to_list()}')\n",
    "validate_specimen_id_risk(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] validating numeric format in ELEVATION\n",
      "[INFO] excluding 193 [''] samples without data in 'ELEVATION'\n",
      "[ERROR] found non-numeric value in ELEVATION: \"700m\"\n"
     ]
    }
   ],
   "source": [
    "def validate_float(col, df, na_values=[]):\n",
    "    \n",
    "    logging.info(f'validating numeric format in {col}')\n",
    "    \n",
    "    if col not in df.columns:\n",
    "        logging.error(f'{col} column not found in manifest')\n",
    "        return\n",
    "    series = df[col]\n",
    "    series = exclude_missing(series, na_values)\n",
    "    \n",
    "    for val in series.unique():\n",
    "        try:\n",
    "            float(val)\n",
    "        except:\n",
    "            logging.error(f'found non-numeric value in {col}: \"{val}\"')\n",
    "validate_float('ELEVATION', df, na_values=[''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] validating freetext chars in IDENTIFIED_HOW\n",
      "[INFO] excluding 97 [''] samples without data in 'IDENTIFIED_HOW'\n",
      "[ERROR] found non-standard characters in column IDENTIFIED_HOW, SERIES [6]. Regex: \"^[A-z0-9.,\\- ]+$\"\n"
     ]
    }
   ],
   "source": [
    "def validate_freetext(col, df, na_values=['']):\n",
    "    \n",
    "    logging.info(f'validating freetext chars in {col}')\n",
    "    \n",
    "    if col not in df.columns:\n",
    "        logging.error(f'{col} column not found in manifest')\n",
    "        return\n",
    "    series = df[col]\n",
    "    series = exclude_missing(series, na_values)\n",
    "    \n",
    "    regex = '^[A-z0-9.,\\-_ ]+$'\n",
    "    \n",
    "    is_valid_freetext = series.str.match(regex)\n",
    "    if not is_valid_freetext.all():\n",
    "        logging.warning('found non-standard characters in column {}, SERIES {}. Regex: \"{}\"'.format(\n",
    "        col, series.loc[~is_valid_freetext].index.to_list(), regex))\n",
    "\n",
    "validate_freetext('IDENTIFIED_HOW', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] validating date column 'DATE_OF_COLLECTION'\n",
      "[INFO] excluding 0 [] samples without data in 'DATE_OF_COLLECTION'\n",
      "[ERROR] invalid dates in 'DATE_OF_COLLECTION': ['TEST' '']\n",
      "[ERROR] future dates in 'DATE_OF_COLLECTION': [Timestamp('2117-08-21 00:00:00')]\n",
      "[INFO] validating date column 'DATE_OF_PRESERVATION'\n",
      "[INFO] excluding 0 [] samples without data in 'DATE_OF_PRESERVATION'\n",
      "[ERROR] invalid dates in 'DATE_OF_PRESERVATION': ['']\n"
     ]
    }
   ],
   "source": [
    "bd = validate_date('DATE_OF_COLLECTION', df)\n",
    "ad = validate_date('DATE_OF_PRESERVATION', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] checking that DATE_OF_COLLECTION are earlier than DATE_OF_PRESERVATION\n",
      "[ERROR] DATE_OF_COLLECTION values are later than DATE_OF_PRESERVATION for SERIES [9]\n"
     ]
    }
   ],
   "source": [
    "def compare_dates(before, after):\n",
    "        \n",
    "    logging.info(f'checking that {before.name} are earlier than {after.name}')\n",
    "\n",
    "    ctdf = pd.concat([before, after], axis=1)\n",
    "    date_conflict = ctdf[before.name] > ctdf[after.name]\n",
    "    \n",
    "    if date_conflict.any():\n",
    "        logging.error(f'{before.name} values are later than {after.name} for SERIES'\n",
    "                      f' {ctdf[date_conflict].index.to_list()}')\n",
    "compare_dates(bd, ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] validating contributors in '../data/Anopheles_Metadata_Manifest_V4.0_20220825.xlsx'\n",
      "[WARNING] trailing spaces found in column 'FIRST_NAME', SERIES [0, 1]. Removing for validation\n",
      "[WARNING] suspect template contributor was not removed\n",
      "[ERROR] duplicated names ['Darwin Charles R.']\n",
      "[ERROR] invalid email addresses ['darwin_darwin.darwin']\n",
      "[ERROR] confirmation lacking for any contributors\n"
     ]
    }
   ],
   "source": [
    "def validate_contributors(fn, contrib_sheet='TAB 1 Contributors'):\n",
    "    \n",
    "    df = logging.info('validating contributors in {!r}'.format(fn))\n",
    "    \n",
    "    df = pd.read_excel(fn, dtype=str, keep_default_na=False,\n",
    "                       sheet_name=contrib_sheet)\n",
    "    \n",
    "    df = remove_trailing_spaces(df)\n",
    "    \n",
    "    df['NAME'] = df['SURNAME'] + ' ' + df['FIRST_NAME']\n",
    "    is_template_name = (df['NAME'] == 'Darwin Charles R.')\n",
    "    if is_template_name.any():\n",
    "        logging.warning('suspect template contributor was not removed')\n",
    "    \n",
    "    is_dup_name = df['NAME'].duplicated()\n",
    "    if is_dup_name.any():\n",
    "        logging.error('duplicated names {}'.format(\n",
    "                df.loc[is_dup_name, 'NAME'].to_list()))\n",
    "    \n",
    "    # TODO update template with underscore in email column\n",
    "    is_valid_email = df['EMAIL ADDRESS'].str.match('^[A-z0-9._%+-]+@[A-z0-9.-]+\\.[A-z]{2,}$')\n",
    "    if not is_valid_email.all():\n",
    "        logging.error('invalid email addresses {}'.format(\n",
    "                df.loc[~is_valid_email, 'EMAIL ADDRESS'].to_list()))\n",
    "    \n",
    "    is_confirmed = (df['CONFIRMATION'] == 'YES')\n",
    "    if not is_confirmed.any():\n",
    "        logging.error('confirmation lacking for any contributors')\n",
    "        \n",
    "    return df\n",
    "        \n",
    "contrib_df = validate_contributors(anospp_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
