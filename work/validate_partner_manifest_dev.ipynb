{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ete3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/g7/vnlptw7d5pb544zsj_bypk44000h93/T/ipykernel_97086/1006053900.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mete3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgeopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgeopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeocoders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNominatim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ete3'"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "import itertools\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import ete3\n",
    "import geopy\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)\n",
    "# logging.getLogger().setFormat('[%(levelname)s] %(message)s')\n",
    "\n",
    "def setup_logging(verbose=False):\n",
    "    try: \n",
    "        del logging.root.handlers[:]\n",
    "    except:\n",
    "        pass\n",
    "    if verbose:\n",
    "        logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n",
    "    else:\n",
    "        logging.basicConfig(level=logging.WARNING, format='[%(levelname)s] %(message)s')\n",
    "setup_logging(verbose=True)   \n",
    "logging.info('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'data/NE BIOSCAN_Manifest_V1.0_Yarner_2021.xlsx'\n",
    "template_fn = '../data/BIOSCAN_Manifest_V1.0_20211207.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and install taxonomy\n",
    "ncbi = ete3.NCBITaxa()\n",
    "# only run update if needed\n",
    "# ncbi.update_taxonomy_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(fn, sheet='TAB1 Specimen Metadata Entry'):\n",
    "\n",
    "    logging.info('reading data from {!r}'.format(fn))\n",
    "    \n",
    "    df = pd.read_excel(fn, dtype=str, index_col=0, keep_default_na=False,\n",
    "                       sheet_name=sheet)\n",
    "\n",
    "    if df.index.duplicated().any():\n",
    "        logging.error('duplicate SERIES: {}'.format(df.index[df.index.duplicated()].to_list()))\n",
    "        \n",
    "    # trailing spaces\n",
    "    \n",
    "        \n",
    "    return df\n",
    "df = get_data(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_trailing_spaces(df):\n",
    "    for col in df.columns:\n",
    "        trailing_spaces = (df[col].str.startswith(' ') | df[col].str.endswith(' '))\n",
    "        if trailing_spaces.any():\n",
    "            logging.warning('trailing spaces found in column {!r}, SERIES {}. Removing for validation'.format(col,\n",
    "                df.loc[trailing_spaces].index.to_list()))\n",
    "            df[col] = df[col].str.strip()\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_df = get_data(template_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_columns(df, template_df):\n",
    "    \n",
    "    logging.info('checking manifest columns against template')\n",
    "    \n",
    "    data_cols = set(df.columns)\n",
    "    template_cols = set(template_df.columns)\n",
    "        \n",
    "    if data_cols - template_cols != set():\n",
    "        logging.warning('extra columns in filled manifest compared to template: {}'.format(data_cols - template_cols))\n",
    "    if template_cols - data_cols != set():\n",
    "        logging.error('template columns missing from filled manifest: {}'.format(template_cols - data_cols))\n",
    "check_columns(df, template_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_dict(fn, validation_sheet='Data Validation - do not edit'):\n",
    "    \n",
    "    # pick up validation values from data validation sheet\n",
    "    logging.info('extracting value validation data from {!r}'.format(fn))\n",
    "    valid_df = pd.read_excel(fn, dtype=str, sheet_name=validation_sheet)\n",
    "    valid_dict = dict()\n",
    "    for col in valid_df.columns:\n",
    "        valid_dict[col] = valid_df[col].dropna().to_list()\n",
    "    \n",
    "    return valid_dict\n",
    "valid_dict = get_valid_dict(template_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_missing(series, na_values=[]):\n",
    "    \n",
    "    # valid missing data \n",
    "    no_data = (series.isin(na_values))\n",
    "    if no_data.sum() > 0:\n",
    "        logging.info('excluding {} {!r} samples without data in {!r}'.format(no_data.sum(), na_values, series.name))\n",
    "    return series[~no_data]\n",
    "    \n",
    "exclude_missing(df['TIME_OF_COLLECTION'], na_values=['NOT_COLLECTED',''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_series(df):\n",
    "    \n",
    "    # series should be 1,2, ..., nsamples\n",
    "    logging.info('validating SERIES')\n",
    "    \n",
    "    # exclude non-numeric SERIES\n",
    "    series_numeric = df.index.astype(str).str.isnumeric()\n",
    "    if not series_numeric.all():\n",
    "        logging.error(f'Found and excluded non-numeric SERIES: {df.index[~series_numeric].to_list()}')\n",
    "        df = df.loc[series_numeric]\n",
    "        \n",
    "    # check the remaining SERIES are continuous\n",
    "    expected_series = set([str(i) for i in range(1, df.shape[0] + 1)])\n",
    "    observed_series = set(df.index.astype(str))\n",
    "    if expected_series != observed_series:\n",
    "        logging.error(f'In SERIES, {sorted(list(expected_series - observed_series))} are missing, '\n",
    "                      f'{sorted(list(observed_series - expected_series))} are unexpected')\n",
    "        \n",
    "    return df\n",
    "        \n",
    "df = validate_series(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_plates_wells(df, plate_col, well_col):\n",
    "    \n",
    "    # expect only complete 96-well plates\n",
    "    logging.info(f'validating {plate_col} and {well_col}')\n",
    "    \n",
    "    empty_rows = (df[plate_col] == '') | (df[well_col] == '')\n",
    "    \n",
    "    if empty_rows.any():\n",
    "        logging.error(f'Found and excluded {empty_rows.sum()} empty rows based on {plate_col} and {well_col}')\n",
    "        df = df.loc[~empty_rows]\n",
    "    \n",
    "    logging.info(f'found {df.shape[0]} samples across {df[plate_col].nunique()} plates')\n",
    "    \n",
    "    # add 96-well plate well IDs to validation\n",
    "    row_id = list('ABCDEFGH')\n",
    "    col_id = range(1,13)\n",
    "    expected_wells = set([r + str(c) for (r,c) in itertools.product(row_id, col_id)])\n",
    "    \n",
    "    for plate, pdf in df.groupby(plate_col):\n",
    "        dup_wells =  pdf[well_col].duplicated()\n",
    "        if dup_wells.any():\n",
    "            logging.error(f'duplicate {well_col} for plate {plate}: {pdf.loc[dup_wells, well_col].unique()}')\n",
    "        observed_wells = set(pdf[well_col])\n",
    "        if observed_wells != expected_wells:\n",
    "            logging.error(f'in {well_col} for plate {plate}, wells {expected_wells - observed_wells} '\n",
    "                          f'are missing, wells {observed_wells - expected_wells} are excessive')\n",
    "    \n",
    "    return df\n",
    "        \n",
    "df = validate_plates_wells(df, 'RACK_OR_PLATE_ID', 'TUBE_OR_WELL_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SCIENTIFIC_NAME'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO - which columns require NA, do not remove blanks to be able to get taxids for all\n",
    "def check_blanks(df):\n",
    "    \n",
    "    logging.info('Checking and excluding blank samples')    \n",
    "    \n",
    "    # blank criterion\n",
    "    is_blank = (df['ORGANISM_PART'] == 'NOT_APPLICABLE')\n",
    "    \n",
    "    blank_df = df[is_blank]\n",
    "    \n",
    "    non_blank_df = df[~is_blank]\n",
    "    \n",
    "    # last well of plate expected to be blank\n",
    "    non_blank_last_well_df = non_blank_df[non_blank_df['TUBE_OR_WELL_ID'] == 'H12']\n",
    "    if non_blank_last_well_df.shape[0] > 0:\n",
    "        logging.error('last well H12 is not blank at SERIES {}: in ORGANISM_PART, '\n",
    "                      'expected \"NOT_APPLICABLE\", found {}. '\n",
    "                      'These samples will be included in further analysis.'.format(\n",
    "                        non_blank_last_well_df.index.to_list(),\n",
    "                        non_blank_last_well_df.ORGANISM_PART.to_list()\n",
    "        ))\n",
    "    \n",
    "    # exclude blanks from downstream analysis    \n",
    "    logging.info('Blanks removed: {} samples of {} left for downstream analysis'.format(\n",
    "        non_blank_df.shape[0], df.shape[0]))\n",
    "    \n",
    "    \n",
    "    return non_blank_df\n",
    "print(df.shape)\n",
    "df = check_blanks(df)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_values(col, df, valid_dict, sep=None, na_values=[], level='e'):\n",
    "    \n",
    "    logging.info('validating values in column {!r}'.format(col))\n",
    "    \n",
    "    if col not in df.columns:\n",
    "        logging.error('{!r} column not found in manifest'.format(col))\n",
    "        return\n",
    "    if col not in valid_dict.keys():\n",
    "        logging.error('{!r} column not found in validation sheet'.format(col))\n",
    "        return\n",
    "    assert level in ('i','w','e'), '{!r} invalid logging level for validate_values'.format(level)\n",
    "    \n",
    "    series = df[col]\n",
    "    series = exclude_missing(series, na_values)\n",
    "    \n",
    "    col_values = set(series.unique())\n",
    "    # use separator to split values\n",
    "    if sep:\n",
    "        sep_col_values = list()\n",
    "        for v in col_values:\n",
    "            sep_col_values.extend([x.strip() for x in v.split(sep)])\n",
    "        col_values = set(sep_col_values)\n",
    "    valid_values = set(valid_dict[col])\n",
    "    invalid_values = col_values - valid_values\n",
    "    if len(invalid_values) > 0:\n",
    "        msg = 'invalid values in {!r}: {}'.format(col, invalid_values)\n",
    "        if level == 'i':\n",
    "            logging.info(msg)\n",
    "        elif level == 'w':\n",
    "            logging.warning(msg)\n",
    "        elif level == 'e':\n",
    "            logging.error(msg)\n",
    "#     else:\n",
    "#         logging.info('all values valid in {!r}'.format(col))\n",
    "            \n",
    "validate_values('ORGANISM_PART', df, valid_dict, sep=\" | \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_date(col, df, na_values=[]):\n",
    "    \n",
    "    logging.info('validating date column {!r}'.format(col))\n",
    "\n",
    "    if col not in df.columns:\n",
    "        logging.error('{!r} column not found in manifest'.format(col))\n",
    "        return\n",
    "    series = df[col]\n",
    "    series = exclude_missing(series, na_values)\n",
    "    \n",
    "    # invalid date formats\n",
    "    # empty string converted to NaT\n",
    "    date_series = pd.to_datetime(series, format='%Y-%m-%d', errors='coerce')\n",
    "    if date_series.isna().any():\n",
    "        logging.error('invalid dates in {!r}: {}'.format(col, \n",
    "                                                         series[date_series.isna()].unique()))\n",
    "    valid_date_series = date_series[~date_series.isna()]\n",
    "    \n",
    "    # dates in future\n",
    "    future_dates = (valid_date_series > datetime.datetime.today())\n",
    "    if future_dates.any():\n",
    "        logging.error('future dates in {!r}: {}'.format(col,\n",
    "            valid_date_series[future_dates].to_list()))\n",
    "        \n",
    "    # dates too old\n",
    "    old_dates = (valid_date_series < datetime.datetime.strptime('1900-01-01', '%Y-%m-%d'))\n",
    "    if old_dates.any():\n",
    "        logging.error(\"pre-1900 dates in {!r}: {}\".format(col,\n",
    "            valid_date_series[old_dates].to_list())) \n",
    "    \n",
    "    return valid_date_series\n",
    "df.loc[1,'DATE_OF_COLLECTION'] = 'NOT_COLLECTED'\n",
    "validate_date('DATE_OF_COLLECTION', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_time(col, df, na_values=[]):\n",
    "    \n",
    "    logging.info('validating time column {!r}'.format(col))\n",
    "    \n",
    "    if col not in df.columns:\n",
    "        logging.error('{!r} column not found in manifest'.format(col))\n",
    "        return\n",
    "    series = df[col]\n",
    "    series = exclude_missing(series, na_values)\n",
    "        \n",
    "    # invalid time formats\n",
    "    # NB empty string converted to NaT\n",
    "    time_series = pd.to_datetime(series, format='%H:%M:%S', errors='coerce')\n",
    "    if time_series.isna().any():\n",
    "        logging.error('invalid times in {!r}: {}'.format(col, \n",
    "                                                         series[time_series.isna()].unique()))\n",
    "    valid_time_series = time_series[~time_series.isna()]\n",
    "    \n",
    "    return valid_time_series\n",
    "# df.loc[1,'TIME_OF_COLLECTION'] = '23'\n",
    "validate_time('TIME_OF_COLLECTION', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_time_period(col, df, na_values=[]):\n",
    "    \n",
    "    logging.info('validating time period column {!r}'.format(col))\n",
    "    \n",
    "    if col not in df.columns:\n",
    "        logging.error('{!r} column not found in manifest'.format(col))\n",
    "        return\n",
    "    series = df[col]\n",
    "    series = exclude_missing(series, na_values)\n",
    "\n",
    "    # conversion with modifications for proper parsing \n",
    "    # by pd.Timedelta (does not accept missing data, e.g. 'PT1H')\n",
    "    # note - will not work for weeks and months\n",
    "    def convert_iso_duration(s):\n",
    "        if s == np.nan:\n",
    "            return np.nan\n",
    "        if not s.startswith('P') or 'T' not in s:\n",
    "            return np.nan\n",
    "        # add days\n",
    "        if s.startswith('PT'):\n",
    "            s = s.replace('PT','P0DT')\n",
    "        # add trailing minutes and seconds\n",
    "        if s.endswith('H'):\n",
    "            s += '0M0S'\n",
    "        elif s.endswith('M'):\n",
    "            s += '0S'\n",
    "        try:\n",
    "            return pd.Timedelta(s)\n",
    "        except:\n",
    "            return np.nan\n",
    "    time_period_series = series.apply(convert_iso_duration)\n",
    "    if time_period_series.isna().any():\n",
    "        logging.error('invalid times in {!r}: {}'.format(col, \n",
    "            series[time_period_series.isna()].unique()))\n",
    "    valid_time_period_series = time_period_series[~time_period_series.isna()]\n",
    "    return valid_time_period_series\n",
    "\n",
    "# df.loc[1,'DURATION_OF_COLLECTION'] = 'PVT1H'\n",
    "validate_time_period('DURATION_OF_COLLECTION', df);\n",
    "# df['DURATION_OF_COLLECTION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be replaced/supported by w3w check\n",
    "def check_location(df, fn, na_values=[]):\n",
    "    \n",
    "    logging.info('validating country with coordinates')\n",
    "    \n",
    "    loc_col, lat_col, lon_col = 'COLLECTION_LOCATION', 'DECIMAL_LATITUDE', 'DECIMAL_LONGITUDE'\n",
    "\n",
    "    try:\n",
    "        loc_df_complete = df[[loc_col, lat_col, lon_col]].copy()\n",
    "    except:\n",
    "        logging.error('One of {!r} {!r} {!r} columns not found in manifest'.format(loc_col, lat_col, lon_col))\n",
    "        return\n",
    "    loc_df_isna = (loc_df_complete.isin(na_values)).all(axis=1)\n",
    "    if loc_df_isna.any():\n",
    "        logging.info('removing {} {!r} samples with missing data from coordinate analysis'.format(\n",
    "                loc_df_isna.sum(), na_values))\n",
    "    loc_df_complete = loc_df_complete[~loc_df_isna].copy()\n",
    "    \n",
    "    # coordinates in geopy format\n",
    "    loc_df_complete['coord'] = loc_df_complete.apply(lambda x: '{}, {}'.format(\n",
    "            x[lat_col], x[lon_col]), axis=1)\n",
    "    \n",
    "    # get location data for coordinates\n",
    "    # use local copy of web query results for re-runs\n",
    "    # this \n",
    "    loc_fn = fn+'_loc.pkl'\n",
    "    if os.path.isfile(loc_fn):\n",
    "        locations = pickle.load(open(loc_fn, \"rb\"))\n",
    "    else:\n",
    "        # web map server - openstreetmaps\n",
    "        logging.info('querying coordinates')\n",
    "        locator = Nominatim(user_agent='myGeocoder')\n",
    "        rgeocode = RateLimiter(locator.reverse, min_delay_seconds=1)\n",
    "\n",
    "        locations = dict()\n",
    "        for c in loc_df_complete.coord.unique():\n",
    "            # pre-fill with unknown country\n",
    "            locations[c] = {'address':{'country':'UNKNOWN'}}\n",
    "            # check coordniate correctness\n",
    "            try:\n",
    "                lat, lon = c.split(', ')\n",
    "                lat, lon = float(lat), float(lon)\n",
    "            except:\n",
    "                logging.error('problem parsing coordinates {!r}'.format(c))\n",
    "                continue\n",
    "            if abs(lat) > 90:\n",
    "                logging.error('invalid latitude {}, should be in [-90,90]'.format(lat))\n",
    "                continue\n",
    "            if abs(lon) > 180:\n",
    "                logging.error('invalid longitude {}, should be in [-180,180]'.format(lon))\n",
    "                continue\n",
    "            # web query\n",
    "            location = rgeocode(c, language='en-gb')\n",
    "            # rgeocode returns empty location outside of counries and in some other situations\n",
    "            if location is not None:\n",
    "                locations[c] = location.raw\n",
    "\n",
    "        # save locations to file\n",
    "        pickle.dump(locations, open(loc_fn, \"wb\"))\n",
    "        \n",
    "    # parse country from partner input\n",
    "    loc_df_complete['partner_country'] = loc_df_complete[loc_col].apply(lambda x: x.split('|')[0].strip().upper())\n",
    "    \n",
    "    # extract countries from location data\n",
    "    loc_countries = dict()\n",
    "    for coord in locations.keys():\n",
    "        coord_country = locations[coord]['address']['country'].upper()\n",
    "        loc_countries[coord] = coord_country\n",
    "        \n",
    "        partner_countries = loc_df_complete.loc[loc_df_complete.coord == coord, 'partner_country']\n",
    "        if partner_countries.nunique() > 1:\n",
    "            logging.error('multiple partner countries for coordinates {!r}: {}'\n",
    "                          'skipping coordinate validation'.format(\n",
    "                                coord, partner_countries.unique()))\n",
    "            continue\n",
    "        if partner_countries.shape[0] == 0:\n",
    "            logging.error('no partner location found for coordinates {!r}'.format(coord))\n",
    "            continue\n",
    "        partner_country = partner_countries.iloc[0]\n",
    "        if coord_country == 'UNKNOWN':\n",
    "            logging.warning('could not locate country for coordinates {!r}, partner country {!r}'.format(\n",
    "                    coord, partner_country))\n",
    "        elif partner_country != coord_country:\n",
    "            logging.error('country mismatch for coordinates {!r}, partner country {!r}, '\n",
    "                          'coordinate country {!r}'.format(coord, partner_country, coord_country))\n",
    "    \n",
    "    # countries based on coordinates\n",
    "    loc_df_complete['coord_country'] = loc_df_complete['coord'].replace(loc_countries)\n",
    "    country_mismatch = (loc_df_complete.coord_country != loc_df_complete.partner_country)\n",
    "\n",
    "#     if country_mismatch.any():\n",
    "#         logging.error('coordinates do not match country for SERIES: {}'.format(\n",
    "#                 country_mismatch[country_mismatch].index.to_list()))\n",
    "    \n",
    "    # location data can be re-used, e.g. as an additional field\n",
    "    return loc_df_complete\n",
    "# df.loc[2,'DECIMAL_LATITUDE'] = '65'\n",
    "loc_test = check_location(df, fn)\n",
    "loc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_ncbi_taxonomy_anospp(df, ncbi, na_values = []):\n",
    "    \n",
    "    logging.info('validating species taxonomy against NCBI')\n",
    "\n",
    "    tax_names = list(df['PREDICTED_SCIENTIFIC_NAME'].unique())\n",
    "    \n",
    "    for na_value in na_values:\n",
    "        try:\n",
    "            tax_names.remove(na_value)\n",
    "        except:\n",
    "            pass \n",
    "        \n",
    "    for i, tax_name in enumerate(tax_names):\n",
    "        if len(tax_name) == 0:\n",
    "            continue\n",
    "        corr_tax_name = tax_name[0].upper() + tax_name[1:].lower()\n",
    "        if corr_tax_name != tax_name and tax_name != 'blank sample':\n",
    "            logging.error(f'{tax_level}: unexpected case for \"{tax_name}\", '\n",
    "                          f'changing to \"{corr_tax_name}\" for validation')\n",
    "        tax_names[i] = corr_tax_name\n",
    "\n",
    "    tax_info = dict()\n",
    "    tax_level = 'SPECIES'\n",
    "    \n",
    "    tax_info[tax_level] = ncbi.get_name_translator(tax_names) \n",
    "\n",
    "    unmatched_names = set(tax_names) - set(tax_info[tax_level].keys())\n",
    "    if len(unmatched_names) > 0:\n",
    "        logging.error(f'{tax_level}: {unmatched_names} not found in NCBI Taxonomy')\n",
    "\n",
    "    expected_rank = 'species' if (tax_level == 'SCIENTIFIC_NAME') else tax_level.lower()\n",
    "\n",
    "    for tname, tids in tax_info[tax_level].items():\n",
    "\n",
    "        ranks = ncbi.get_rank(tids)\n",
    "\n",
    "        upd_tid = tids[0]\n",
    "\n",
    "        if len(tids) == 1:\n",
    "            if ranks[upd_tid] != expected_rank: \n",
    "                # TODO warning->info for ORDER\n",
    "                logging.warning(f'{tax_level}: found unexpected rank for {tname} (taxid {upd_tid}): {ranks[upd_tid]}')\n",
    "        if len(tids) > 1:            \n",
    "            for tid, r in ranks.items():\n",
    "                if r == expected_rank and len(tids) > 1:\n",
    "                    logging.info(f'{tax_level}: using only first matching rank for {tname} (taxid {tid}): {r}')\n",
    "                    upd_tid = tid\n",
    "                    break\n",
    "            else:\n",
    "                logging.warning(f'{tax_level}: could not find matching rank for {tname}, '\n",
    "                                f'using (taxid {upd_tid}): {ranks[upd_tid]}')\n",
    "\n",
    "        tax_info[tax_level][tname] = upd_tid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_ncbi_taxonomy(df, ncbi, na_values = []):\n",
    "    \n",
    "    logging.info('validating taxonomy against NCBI')\n",
    "    \n",
    "    tax_columns = [\n",
    "        'ORDER',\n",
    "        'FAMILY',\n",
    "        'GENUS',\n",
    "        'SCIENTIFIC_NAME'\n",
    "    ]        \n",
    "    \n",
    "    hierarchies = df[tax_columns].drop_duplicates().copy()\n",
    "    \n",
    "    tax_info = dict()\n",
    "    \n",
    "    for tax_level in tax_columns:\n",
    "        \n",
    "        logging.info(f'validating {tax_level} against NCBI')\n",
    "        \n",
    "        if tax_level not in df.columns:\n",
    "                logging.error(f'{tax_level} column not found in manifest')\n",
    "                continue\n",
    "            \n",
    "        tax_names = list(hierarchies[tax_level].unique())\n",
    "        \n",
    "        for na_value in na_values:\n",
    "            try:\n",
    "                tax_names.remove(na_value)\n",
    "            except:\n",
    "                pass \n",
    "            \n",
    "        for i, tax_name in enumerate(tax_names):\n",
    "            if len(tax_name) == 0:\n",
    "                continue\n",
    "            corr_tax_name = tax_name[0].upper() + tax_name[1:].lower()\n",
    "            if corr_tax_name != tax_name and tax_name != 'blank sample':\n",
    "                logging.error(f'{tax_level}: unexpected case for \"{tax_name}\", '\n",
    "                              f'changing to \"{corr_tax_name}\" for validation')\n",
    "            tax_names[i] = corr_tax_name\n",
    "        \n",
    "        tax_info[tax_level] = ncbi.get_name_translator(tax_names) \n",
    "        \n",
    "        unmatched_names = set(tax_names) - set(tax_info[tax_level].keys())\n",
    "        if len(unmatched_names) > 0:\n",
    "            logging.error(f'{tax_level}: {unmatched_names} not found in NCBI Taxonomy')\n",
    "        \n",
    "        expected_rank = 'species' if (tax_level == 'SCIENTIFIC_NAME') else tax_level.lower()\n",
    "        \n",
    "        for tname, tids in tax_info[tax_level].items():\n",
    "            \n",
    "            ranks = ncbi.get_rank(tids)\n",
    "            \n",
    "            upd_tid = tids[0]\n",
    "            \n",
    "            if len(tids) == 1:\n",
    "                if ranks[upd_tid] != expected_rank: \n",
    "                    # TODO warning->info for ORDER\n",
    "                    logging.warning(f'{tax_level}: found unexpected rank for {tname} (taxid {upd_tid}): {ranks[upd_tid]}')\n",
    "            if len(tids) > 1:            \n",
    "                for tid, r in ranks.items():\n",
    "                    if r == expected_rank and len(tids) > 1:\n",
    "                        logging.info(f'{tax_level}: using only first matching rank for {tname} (taxid {tid}): {r}')\n",
    "                        upd_tid = tid\n",
    "                        break\n",
    "                else:\n",
    "                    logging.warning(f'{tax_level}: could not find matching rank for {tname}, '\n",
    "                                    f'using (taxid {upd_tid}): {ranks[upd_tid]}')\n",
    "                    \n",
    "            tax_info[tax_level][tname] = upd_tid\n",
    "        \n",
    "        #logging.info(f'{tax_level} {tax_info[tax_level]}')\n",
    "                    \n",
    "    # check correctness of taxonomy\n",
    "    for _, r in hierarchies.iterrows():\n",
    "        \n",
    "        if r.ORDER in na_values:\n",
    "            continue\n",
    "        try:\n",
    "            order_id = tax_info['ORDER'][r.ORDER]\n",
    "        except KeyError:\n",
    "            logging.info(f'cannot validate ORDER for \"{r.ORDER}\", skipping taxonomy consistency check')\n",
    "            continue\n",
    "            \n",
    "        if r.FAMILY in na_values:\n",
    "            continue\n",
    "        try:\n",
    "            family_id = tax_info['FAMILY'][r.FAMILY]\n",
    "            \n",
    "            family_lineage = ncbi.get_lineage(family_id)\n",
    "            \n",
    "            if order_id not in family_lineage:\n",
    "                logging.error(f'Family {r.FAMILY} (taxid {family_id}) does not belong to {r.ORDER} (taxid {order_id})')\n",
    "        except KeyError:\n",
    "            logging.info(f'cannot validate FAMILY for \"{r.FAMILY}\", skipping taxonomy consistency check')\n",
    "            continue\n",
    "            \n",
    "        if r.GENUS in na_values:\n",
    "            continue\n",
    "        try:\n",
    "            genus_id = tax_info['GENUS'][r.GENUS]\n",
    "            \n",
    "            genus_lineage = ncbi.get_lineage(genus_id)\n",
    "            \n",
    "            if order_id not in genus_lineage:\n",
    "                logging.error(f'Genus {r.GENUS} (taxid {genus_id}) does not belong to {r.ORDER} (taxid {order_id})')\n",
    "            if family_id not in genus_lineage:\n",
    "                logging.error(f'Genus {r.GENUS} (taxid {genus_id}) does not belong to {r.FAMILY} (taxid {family_id})')\n",
    "        except KeyError:\n",
    "            logging.info(f'cannot validate GENUS for \"{r.GENUS}\", skipping taxonomy consistency check')\n",
    "            continue\n",
    "            \n",
    "        if r.SCIENTIFIC_NAME in na_values:\n",
    "            continue\n",
    "        try:\n",
    "            species_id = tax_info['SCIENTIFIC_NAME'][r.SCIENTIFIC_NAME]\n",
    "            \n",
    "            species_lineage = ncbi.get_lineage(species_id)\n",
    "            \n",
    "            if order_id not in species_lineage:\n",
    "                logging.error(f'Species {r.SCIENTIFIC_NAME} (taxid {species_id}) does not belong to {r.ORDER} (taxid {order_id})')\n",
    "            if family_id not in species_lineage:\n",
    "                logging.error(f'Species {r.SCIENTIFIC_NAME} (taxid {species_id}) does not belong to {r.FAMILY} (taxid {family_id})')\n",
    "            if genus_id not in species_lineage:\n",
    "                logging.error(f'Species {r.SCIENTIFIC_NAME} (taxid {species_id}) does not belong to {r.GENUS} (taxid {genus_id})')\n",
    "        except KeyError:\n",
    "            logging.info(f'cannot validate SCIENTIFIC_NAME for \"{r.SCIENTIFIC_NAME}\", skipping taxonomy consistency check')\n",
    "            continue\n",
    "            \n",
    "    for tc in tax_columns:\n",
    "        df[f'{tc}_TAXID'] = df[tc].replace(tax_info[tc])\n",
    "            \n",
    "    return df\n",
    "        \n",
    "                \n",
    "validate_ncbi_taxonomy(df, ncbi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_int(col, df, na_values=[]):\n",
    "    \n",
    "    logging.info(f'validating int format in {col}')\n",
    "    \n",
    "    if col not in df.columns:\n",
    "        logging.error(f'{col} column not found in manifest')\n",
    "        return\n",
    "    series = df[col]\n",
    "    series = exclude_missing(series, na_values)\n",
    "    \n",
    "    for val in series.unique():\n",
    "        try:\n",
    "            int(val)\n",
    "        except:\n",
    "            logging.error(f'found non-integer value in {col}: \"{val}\"')\n",
    "validate_int('TIME_ELAPSED_FROM_COLLECTION_TO_PLATING', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd = validate_date('DATE_OF_COLLECTION', df)\n",
    "ad = validate_date('DATE_OF_PRESERVATION', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctdf = pd.concat([bd, ad], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctdf.iloc[:, 0] > ctdf.iloc[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_dates(before, after):\n",
    "        \n",
    "    logging.info(f'checking that {before.name} are earlier than {after.name}')\n",
    "\n",
    "    ctdf = pd.concat([before, after], axis=1)\n",
    "    date_conflict = ctdf[before.name] > ctdf[after.name]\n",
    "    \n",
    "    if date_conflict.any():\n",
    "        logging.error(f'{before.name} values are later than {after.name} for SERIES'\n",
    "                      f' {ctdf[date_conflict].index.to_list()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception('Done for now')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_bioscan(fn, template_fn, verbose=False, version='1.0'):\n",
    "    '''\n",
    "    Validation follows the order of columns order in data entry sheet\n",
    "    '''\n",
    "\n",
    "    setup_logging(verbose=verbose)\n",
    "\n",
    "    logging.info(f'# started validate_partner_manifest_v.{version}')\n",
    "    logging.warning(f'# manifest {fn}')\n",
    "\n",
    "    # read data\n",
    "    df = get_data(fn)\n",
    "    \n",
    "    # read taxonomy\n",
    "    ncbi = ete3.NCBITaxa()\n",
    "    \n",
    "    # prepare for validation\n",
    "    template_df = get_data(template_fn)\n",
    "    check_columns(df, template_df)\n",
    "    valid_dict = get_valid_dict(template_fn)\n",
    "\n",
    "    # orange cols\n",
    "    # exclude empty series\n",
    "    df = validate_series(df)\n",
    "    df = validate_plates_wells(df, 'RACK_OR_PLATE_ID', 'TUBE_OR_WELL_ID')\n",
    "    \n",
    "    # check and exclude blanks\n",
    "    df = check_blanks(df)\n",
    "    \n",
    "    validate_values('PRESERVATIVE_SOLUTION', df, valid_dict)\n",
    "    # CATCH_LOT not checked TODO do not allow missing\n",
    "    validate_values('BOTTLE_DIRECTION', df, valid_dict)\n",
    "    validate_values('ORGANISM_PART', df, valid_dict, sep='|')\n",
    "    validate_values('HAZARD_GROUP', df, valid_dict)\n",
    "    validate_values('REGULATORY_COMPLIANCE', df, valid_dict)\n",
    "    date_coll = validate_date('DATE_OF_COLLECTION', df, na_values=['NOT_COLLECTED'])\n",
    "    check_location(df, fn)\n",
    "    \n",
    "    # purple cols\n",
    "    # taxonomy validation adds a few columns\n",
    "    df = validate_ncbi_taxonomy(df, ncbi, na_values = ['NOT_COLLECTED'])\n",
    "    validate_values('SEX', df, valid_dict)\n",
    "    # HABITAT not checked\n",
    "    validate_time('TIME_OF_COLLECTION', df)\n",
    "    validate_time_period('DURATION_OF_COLLECTION', df, na_values=['NOT_COLLECTED'])\n",
    "    validate_values('COLLECTION_METHOD', df, valid_dict)\n",
    "    # DESCRIPTION_OF_COLLECTION_METHOD not checked\n",
    "    validate_int('TIME_ELAPSED_FROM_COLLECTION_TO_PLATING', df, na_values=[''])\n",
    "    # PHOTOGRAPH_* columns not checked\n",
    "    # VOUCHER_ID not checked\n",
    "    # PRESERVATION_APPROACH not checked - should match DATE_OF_PRESERVATION\n",
    "    date_pres = validate_date('DATE_OF_PRESERVATION', df, na_values=['']) # allow for empty values unlike DATE_OF_COLLECTION\n",
    "    compare_dates(before=date_coll, after=date_pres)\n",
    "    # COLLECTOR_SAMPLE_ID not checked\n",
    "    validate_int('ELEVATION', df, na_values=[''])\n",
    "    # OTHER_INFORMATION\tMISC_METADATA\tIDENTIFIED_BY\tIDENTIFIER_AFFILIATION\tIDENTIFIED_HOW not checked\n",
    "        \n",
    "    logging.info('# ended validate_partner_manifest_v.{}'.format(version))\n",
    "\n",
    "    return df\n",
    "\n",
    "# fn = '../../results/partner_manifests/IRD-Neandersquito_T222Amplicon_Manifest_V2.0.xlsx'\n",
    "df = validate(fn, template_fn, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Real manifests validation starts here - now also moved to separate notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = validate('../results/20220322/NBGW-[20210805]-manifest.xlsx', template_fn, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = validate('../results/20220322/NBGW-[20220105]-manifest.xlsx', template_fn, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test columns addition\n",
    "df = validate('../results/20220315/Sam_NHM-BIOSCAN-Manifest Jan 2022.xlsx', template_fn, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test columns addition\n",
    "df = validate('../results/20220303_add_tax/final-CARR-20210727-manifestV1.xlsx', template_fn, verbose=False)\n",
    "df.to_csv('../results/20220303_add_tax/final-CARR-20210727-manifestV1.taxids.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "fns = list(glob.glob('../results/20220304/*.xlsx'))\n",
    "fns.sort()\n",
    "fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = validate(fns[0], template_fn, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = validate(fns[1], template_fn, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = validate(fns[2], template_fn, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = validate(fns[3], template_fn, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = validate(fns[4], template_fn, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = validate(fns[5], template_fn, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = validate(fns[6], template_fn, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = validate('../results/20220301/CARR-20210727-manifest.xlsx', template_fn, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = validate('../results/20220301/NSCR-20220123-manifest (A3642498).xlsx', template_fn, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = validate('../results/20220301/YARN-20211205-manifest.xlsx', template_fn, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = validate('../results/August 2021 Manifest (Bill _ Fred).xlsx', template_fn, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = validate('../results/Mike Ashworth NE 2021-05-28 corrected BIOSCAN_Manifest_V1.0.xlsx', template_fn, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = validate('../results/Mike Ashworth NE 2021-06-24 BIOSCAN_Manifest_V1.0.xlsx', template_fn, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = validate('../results/NE BIOSCAN_Manifest_V1.0_Yarner_260621.xlsx', template_fn, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = validate('../results/NatureScot Working Copy_ of BIOSCAN_Manifest_V1.0 (A3484399).xlsx', template_fn, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = validate('../results/Shap 2021-05-28 corrected.xlsx', template_fn, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = validate('../results/Bioscan metadata_Jan_22_WYTHAM_WOODS.xlsx', template_fn, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = validate('../results/NE BIOSCAN_Manifest_V1.0_Yarner_2021.xlsx', template_fn, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = validate('../results/CARR-20210727-manifest.xlsx', template_fn, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
